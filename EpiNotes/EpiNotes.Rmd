---
title: "EpiNotes"
author: "Lewis Campbell"
date: "`r Sys.Date()`"
output:
  bookdown::tufte_html2: default
  bookdown::epub_book: default
  bookdown::tufte_book2:
    citation_package: natbib
    latex_engine: xelatex
    number_sections: yes
  bookdown::tufte_handout2:
    citation_package: natbib
    latex_engine: xelatex
bibliography: Epidemiology.bib
link-citations: yes
csl: biomed-central.csl
subtitle: For you
biblio-style: plain
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
# invalidate cache when the tufte version changes
#library(bookdown) + render_book("EpiNotes.html", "Epinotes.epub", output_format = epub_book(pandoc_args = "--mathml", epub_version="epub3"))
#calibre(input="EpiNotes.epub", output="EpiNotes.mobi", options = "--mobi-file-type new")
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{=tex}
\frontmatter
\tableofcontents
```
# Introduction {#intro}

This is the notes from my *MSc Epidemiology* from LSHTM, one of the two most respected Epidemiology institutions in the world. It's been a hard road and I wish I had done it sooner and spared more time for writing actually good stuff.

I have three big ideas at the moment.

## Response of the system to perturbation

This is broadly covered by my Self Controlled Case Series approach to measuring how an ICU reacts to prompts that are thought to influence fluid administration. It extends into the other methods of observing perturbation and I think it leads eventually into some sort of network analysis.

## Network analysis

This initially parasitises on Kenny Baillie's work but the idea that local density in a network can inform the force of effect is probably mine in the end.

## Marrying of networks and causal graphs, with response adaptation

I think this is mine. The possibility that a causal graph can be drawn to describe the mechanistic processes for an overall effect. Then, using probabilistic or perhaps even deterministic relationships, the force of effect down each edge is somehow elicited. Each edge is modified by incoming information from live biological information, as a sort of informative meta analysis. These can be used just to explain but I'm most interested in using them for randomisation probabilities to improve causal understanding in the middle of a trial which also informs about endpoints.

# Epistemology

The way in which we know things, or more generally the way in which we justify claims, is related to the truth and to the way the world is made, but only in the long run and there is no guarantee that an epistemologically rigorous statement, holding a strong basis for past and future inference, reflects ontological truth. Ontology is the generation of things, such as truths, and would allow epistemologically sound inference, but isn't always necessary to understand the world (see the geocentric model, and so on). The inner need to assert that an epistemological claim is in fact an ontological one is common, leads to people arguing that odds ratios are satan, and was called the mind projection fallacy by ET Jaynes.

Also, see later for iid values; de Finetti's theorem is that exchangeable values (\~= "can be reordered", and so "don't have an important correlation structure") can be approximated by mixtures of iid distributions. So '"iid" cannot be read literally' as McElreath says: we don't need to think that the generative process is iid at all, let alone the specified iid process, to hold that as an effective and safe epistemological assumption. Further, correlation may just alter small things about a distribution such as the ordering of importantly ordered small parts of it, leaving the overall distribution useably similar: just look at MCMC...

# Risk {#risk}

Summarise last: this is not the usual way but if we use the whole distribution we will make better inferences thatn if we summarise first (comparing means) we lose information, whereas if we summarise last (compare contrasts) we retain information and correctly quote uncertainty.

`r newthought('Risk is a conditional probability')`. This is the entry level, and is maybe a bit higher up than Epidemiology is usually sold. Conditional probabilities are talked about all over the place, but rarely in that language, which requires a retreat by many steps to get to the true underlying shape of the relationship. Relative risk and how it works, odds ratios, hazards and their ratios are fundamental but are maybe best understood via observing how they work in their more "advanced" utilities such as regression modelling.

The baseline odds, for example, is key to understanding the effect of a given odds ratio on the absolute risk difference. By extension it helps to understand how risk and odds differ, how invertible odds are so useful and how a little mathematical manipulation of the relative probabilities opens up a whole set of ways to figure out the true underlying effect. While the odds ratio is the best way to state a constant effect of an exposure on the likelihood of a categorical outcome, it needs to be applied to a baseline absolute odds (and hence risk) to display itself.

![Absolute benefit as a function of risk of the event in a control subject and the relative effect (odds ratio) of the risk factor. The odds ratios are given for each curve.](C:/Users/lewis/Documents/Studying/Epidemiology/EpiNotes/Risk_magnification.jpeg)

```{r fig.margin=TRUE, cache=TRUE, message=FALSE, echo=FALSE, fig.cap='Absolute benefit as a function of risk of the event in a control subject and the relative effect (odds ratio) of the risk factor. The odds ratios are given for each curve.'}
knitr::include_graphics('C:/Users/lewis/Documents/Studying/Epidemiology/EpiNotes/Risk_magnification.jpeg')
```

## Variables

All of this tends to assume one real value exists for each of these measures of effect. There is an underlying generative model for our hypotheses about these effects, whereby we may say "a causes b", or simply "for a of a certain value, we observe b of this predicable value". Because we can't observe every instance of every cause, or even the underlying *vis inertiae* of every random process, we have to use a statistical model to generate these estimates. These need variables.

A variable is just a symbol that can take on different values. Unobserved variables are usually called parameters. They can represent any part of any of the models, including the data, the effect, and so on.

## Measures of outcome

Case definitions need

-   Method for detection or definition

-   Case boundary

-   Unit of analysis (a diagnosis, an individual, a cluster...?)

Population at risk is the entire realised, actual population to which the case definitions are applied. Above that somewhere is a sort of superpopulation, an ideal version of the realised population. This is usually just called the population in what I think is not even shorthand but a lack of concern over how ephemeral it proves to be under analysis. This *population* is a sort of a holy quantity, those who could be cases: so case definition circularly influences the definition of a population, because the cases must be drawn from a population. The harder way to derive a case definition is to imagine the population first, and then create the definition of what would be important to call a case. In that case, the influences on the definition of the population are the more expected ones: geographic, demographic, perhaps genetic, and behavioural[@rose_sick_nodate].

The last part of the definition is wrapped up in both of the "methods of detection" and "boundaries of a case" points. The scale on which outcomes are measured is very important. If the important thing seems to be the value of a continuous measurement that is normally distributed in the population and stable over time in each individual, that is a very different thing from the value of a continuous measurement that has a multimodal distribution and changes rapidly with biological relevance behind each change. Then the questions start to flow:

-   is it the value under standard conditions?

-   is it the highest or lowest value, or a value over or under a thresold?

-   is it the area under the curve, the mean, is it the geometric mean, is it one of those weighted for the time spent at extremes?

-   is it the value conditional on some other measurement?

The one clear answer is that transformations should probably allow recovery of the original value so that information is not irretrievably lost, and so that other ideas can compete of how cause is best described. And that is before we start on categorical outcomes and all the gritty point of confusion from them.

# The contingency table {#contingency}

A table allows things to be seen, and calculations referred to. "Expected values" for the cells in the table can be calculated under various hypotheses using the marginal totals: these E don't have to be all possible at once, so "expected" for rows and columns will differ. The very first thing to say is that there must be sufficient items in each box, or as we'll call them later, each stratum, to say something about the expectation. You can't estimate the effect of a thing that is universal or absent in your sample, whether using this or most other methods.

```{r contingency-abcd, echo=FALSE, message=FALSE}
contingencyabcd <- tibble(" " = c("D", "$D^c$", ""),
            "+" = c("a", "c", "$N_{+}$"),
            "-" = c("b", "d", "$N_{-}$"),
            "total" = c("$N_D$", "$N_{D^c}$", "$N$")
            )
knitr::kable(contingencyabcd, booktabs = TRUE, caption= "Contingency table. This table is the familiar medical school one where sensitivity and specificity are shown to mean completely different things about a positive test, depending on the prevalence of the disease in the population.", escape=FALSE)


#\begin{tabular}{| c |c | c | c| }
#\hline
#& + & - & \\ \hline
#D & a & b & $N_{D}$ \\ \hline
#$D^c$ & c & d & $N_{D^c}$ \\ \hline
#& $N_{+}$ &  $N_{-}$  & N \\ \hline
#\end{tabular}
```

```{r contingency-dh01, echo=FALSE, message=FALSE, cache=FALSE}
contingencydh <- tibble(" " = c("Outcome", "No outcome", ""),
            "Exposed" = c("d1", "h1", "1"),
            "Unexposed" = c("d0", "h0", "0"),
            "total" = c("d", "h", "n")
            )
knitr::kable(contingencydh, booktabs = TRUE, caption= "Contingency table. This table is the same, but contains the d/h and 0/1 notation used later in some of the equations.", escape=FALSE)
```

```{r, echo = FALSE, message=FALSE}
Bayesmargins <- tibble::tibble(
    "measure" = c("sensitivity or recall",
                  "specificity",
                  "ppv or precision",
                  "npv",
                  "accuracy",
                  "variance"),
    "concept" = c("$P(+|D)$",  
                "$P(-|DC)$",
                "$P(D|+)$",
                "$P(DC|-)$",
                "$P(correct)$",
                "inhomogeneity in the table"),
    "value"= c("a/(a+b)",
               "d/(c+d)",
               "a/(a+c)",
               "d/(d+b))",
               "(a+d)/(a+b+c+d)",
               "$\\frac{(a+b).(a+c).(b+d).(c+d)}{n^2(n-1)}$")
)

knitr::kable(Bayesmargins, caption = "Marginal calculations. Marginal probabilities are also important later in Bayesian calculations, because they are the normalising constant, the P(B) in the classic form.", booktabs = TRUE, escape=FALSE)

#\begin{table}
#\caption{Marginal calculations}
#\begin{tabular}{l l l}
#\hline
#sensitivity & =		$P(+|D)$  	    & =	a/(a+b) \\
#specificity & = 	$P(-|DC)$	    & =	d/(c+d) \\
#ppv         & = 	$P(D|+)$	    & =	a/(a+c) \\
#npv         & = 	$P(DC|-)$	    & =	d/(d+b) \\
#accuracy    & =		P(correct)  & =	(a+d)/(a+b+c+d) \\
#variance    & =		$\frac{(a+b).(a+c).(b+d).(c+d)}{n^2(n-1)}$
#\\
#\end{tabular}
#\end{table}
```

The standard error for log odds ratio is

$$\sqrt{\frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}$$ and the 95% confidence interval for the log(OR) is +/- 1.96 log(SE).

The exponential version of this it the Error Factor $e^{z_{\alpha} \times \sqrt{\frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}$, whereby the 95% CI is OR/EF to OR\*EF or exp(log Estimate +/- 1.96 log SE)

## The rare disease assumption

An often misunderstood insight was Cornfield 1951 noting that the ratio of exposed in cases to exposed in controls, weighted for the size of the sample by using the odds ratio, was similar to the ratio of risk of disease in the exposed to the non exposed, as long as all these probabilities are small, such that they are not distorted by approaching 1. This is misinterpreted as the far more trivial "odds ratios approximate risk ratios under the rare disease assumption".

## Diagnostic likelihood ratio

[Likelihood](#likelihood) is covered later in plenty detail. For now, the diagnostic likelihood ratio for this limited situation of mutually exclusive states is revealed by the table, and is a measure of how inhomogenous are the numbers across the table. A certain hypothesis, such as "the disease is present" is more likely than its complement if the test is positive, by an amount equal to the ratio of positive results among the diseased compared to those without the disease.

```{r, echo = FALSE, message=FALSE}
DLRtable <- tibble::tibble(
    "DLR+" = c("$\frac{P(+|D)}{P(+|D^C)}$",
                  "sensitivity / (1-specificity)",
                  "Bayes Factor",
                  "Posterior Odds / Prior Odds",
                  "$\frac{\frac{a}{(a+b)}}{\frac{c}{(c+d)}}$",
                  "$\frac{(ac+ad)}{(ac+ab)}$"),
    "DLR-" = c("$\frac{P(- | D)}{P(- | D^c)}$",  
                "1 - sensitivity / specificity",
                "1/Bayes Factor",
                "etc",
                "",
                ""),
    "Likelihood"= c("DLR+/DLR-",
               "",
               "",
               "",
               "",
               "")
)

knitr::kable(DLRtable, caption = "The diagnostic likelihood ratio of a positive test is the DLR+, and following from this the negative diagnostic likelihood is DLR-, and from these two the entire Likelihood is DLR+/DLR-. If the Likelihood is far from 1, it means that a test result is very informative.", booktabs = TRUE, escape=FALSE)

#\begin{table}
#\caption{Marginal calculations}
#\begin{tabular}{l l l}
#\hline
#sensitivity & =		$P(+|D)$  	    & =	a/(a+b) \\
#specificity & = 	$P(-|DC)$	    & =	d/(c+d) \\
#ppv         & = 	$P(D|+)$	    & =	a/(a+c) \\
#npv         & = 	$P(DC|-)$	    & =	d/(d+b) \\
#accuracy    & =		P(correct)  & =	(a+d)/(a+b+c+d) \\
#variance    & =		$\frac{(a+b).(a+c).(b+d).(c+d)}{n^2(n-1)}$
#\\
#\end{tabular}
#\end{table}
```

## Measures of association

The likelihood ratio is a measure of association, and there are many others, with the preference guided as much by familiarity as by utility for the task. Cohen's kappa, the phi coefficient, the uncertainty coefficient, Regression coefficients, the F score combining precision and recall (identical to ppv and sensitivity), markedness and informedness are various measures [^1] .

[^1]: Cohen's kappa is 1 with perfect agreement, $\kappa = \frac{p_o - p_e}{1 - p_e}$ where $p_o$ is the observed agreement and is the same as accuracy, and $p_e = \frac{1}{N^2} \sum_{k,i} n_{ki}$ is the expected chance agreement, the mean pair agreement weighted for the number of times rater *i* predicted item *k* in total.

Relative risk is $r_e / r_u$. Attributable risk is the risk difference $r_e - r_u$ under the big assumption that the difference is causal, which might be teaching the fruit before the tree.

Attributable fraction AF is the proportion of the higher risk that is unique to those with the higher risk, as in the first equation here. Because relative risk $RR = \frac{r_e}{r_u}$, the second form is equivalent. $$AF = \frac{RR-1}{RR}$$ $$AF = \frac{r_e - r_u}{r_e}$$

Population attributable risk PAR is the extra population risk due to exposure, $PAR = r_t-r_u$, again assuming it's *all causal*. This is mathematically the same as attributable risk times the prevalence of exposure, and population attributable fraction is predictable from the definition as here, where p is the proportion of the population exposed, and p' is proportion of cases who are exposed: $$PAR = p.AR = p(r_e-r_u)$$ $$PAF = p' AF_E =  \frac{p'(RR -1)}{RR}$$

Assuming all sorts of things[^2] $$r_t = p.r_e + (1-p)r_u$$

[^2]: it's all causal, there are no effect modifiers, all other causes are orthogonal to both exposure and outcome, the stratum-specific risks are known for the exposed and unexposed), the risk in the total population is calculated for a single exposure.

So anyway PAF is like attributable fraction, and the whole calculation then goes a little something like this, which I suppose you have to either memorise or write down. $$\frac{r_{t}-r_{u}}{r_{t}} = \frac {PAR}{r_{t}} = \frac {p.(r_{e}-r{u})}{r_{t}}  = \frac {p.(RR-1)}{p.(RR-1)+1} $$

A confidence interval for PAF is given using 1-PAF[^3]: $$1- (1 - \textrm{PAF} ^{\times}_{\div} e^{1.96 \sqrt{}\frac{a}{b n_1} + \frac{c}{d n_0}})$$

[^3]: Honestly I'm not sure why but maybe it's emphasising that the observed value is usually the exposed and diseased, and so the uncertainty is in how much of the rest of the population there is.

## Inference using certain sampling designs

Call the parameter of interest theta; whether it's risk ratio, rate ratio, odds ratio (I know, sshh...). Then,

-   A cohort estimates theta directly but only estimates p and p' if it's a random sample, otherwise external estimates are used. Then the above equations are used.

-   In cross sectional studies prevalence is estimated, incidence can't be.

-   In case control, "theta is estimated by the odds ratio"; as $\theta = ad/bc$ and $PAF = p'(theta – 1)/theta$ and $p' = a/n$ so $PAF = \frac{1- (b/n_1)}{(d/n_0)}$, the ratio of cases unexposed to controls unexposed[^4].

[^4]: $AF_E$ is often used to assign blame by transposing the conditional: as it is the proportion of cases that are associated with the exposure it is called the probability of causation or the assigned share of causation. This is clearly an error unless rare conditions are met, and the effects of the error are anywhere from very mild to moderate depending on the baseline probabilities.

PAF for several levels of an exposure is given by $\sum \frac{p_k' (\theta_k -1)}{\theta_k}$ or the overall equation $\frac{p'(\theta_{tot}-1)}{\theta_{tot}}$, using as $\theta_{tot}$ a regression estimate with whatever covariates are desired to be included. The joint PAF for independent exposure is $$1-PAF_T = 1-PAF_1 + 1-PAF_2 ... 1-PAF_i$$

## Instability of relative risk

Relative risk is a tricky slippy thing, as shown here.

```{r, echo=FALSE, message=FALSE}
rel_risk <- tibble(" " = c("Exposure", "No Exposure", ""),
            "Outcome" = c(30, 10, 40),
            "No outcome" = c(20, 40, 60),
            "  " = c(50, 50, 100)
            )
knitr::kable(rel_risk, booktabs = TRUE, caption= "The proportions are initially as shown", align = 'c', escape=FALSE)

#\begin{tabular}{| c |c | c | c| }
#\hline
# & Outcome & No Outcome & \\ \hline
#Exposure & 30 & 20 & 50 \\ \hline
#No Exposure & 10 & 40 & 50 \\ \hline
#& 40 &  60 & 100 \\
#\hline
#\end{tabular}

```

```{r, echo=FALSE, message=FALSE}
rel_risk <- tibble(c("AR", "30/50 – 10/50", ".4"),
            c( "AF" , "((30/30+20) – (10/10+40))/(10/10+40)","2/3"),
            c(    "RR" , "(30/30+20) / (10/10+40)" , "3"),
            c("Odds of O+ if E+", "30/20" , "1.5"),
            c("Odds of O+ if E-" , "10/40" ,  ".25"),
            c("Odds ratio of E+ versus E-" ," (30*40)/(10*20)"," 6")
            )
knitr::kable(rel_risk, booktabs = TRUE, caption= "The risks are initially as shown", align = 'c', escape=FALSE)

#\begin{tabular}{| c |c | c | c| }
#\hline
# & Outcome & No Outcome & \\ \hline
#Exposure & 30 & 20 & 50 \\ \hline
#No Exposure & 10 & 40 & 50 \\ \hline
#& 40 &  60 & 100 \\
#\hline
#\end{tabular}

```

If the RR doubles to 6 by "clockwise rotation" of the table, as shown below, then the OR is not doubled but more than tripled, rising by a factor of 3.14 to a new odds ratio of 18.857

```{r, echo=FALSE, message=FALSE}
rel_risk_clockwise <- tibble(" " = c("Exposure", "No Exposure", ""),
            "Outcome" = c(36, 14, 40),
            "No outcome" = c(6, 44, 60),
            "  " = c(42, 58, 100)
            )
knitr::kable(rel_risk_clockwise, booktabs = TRUE, caption= "Clockwise", align = 'c', escape=FALSE)
```

Then if there is "right shift" in the table so that the relative risk is doubled to 6, with a reduced population prevalence, the odds ratio only rises by a factor of 1.77, to 10.615

```{r, echo=FALSE, message=FALSE}
rel_risk_right <- tibble(" " = c("Exposure", "No Exposure", ""),
            "Outcome" = c(24, 26, 50),
            "No outcome" = c(4, 46, 50),
            "  " = c(28, 72, 100)
            )
knitr::kable(rel_risk_right, booktabs = TRUE, caption= "Right-shifted", align = 'c', escape=FALSE)
```

Again, the "population attribution" is unstable.

```{r, echo=FALSE, message=FALSE}
rel_risk_PAR <- tibble(" " = c("PAR", "40/40+60 – 10/10+40", ".2" ),
            "  " = c("PAF using RR", "40/40+60 – 10/10+40 / 40/40+60", "0.5"),
            "   " = c("PAF using OR", "(0.5).(6-1) / (0.5).(6-1)+1", "0.71")
            )
knitr::kable(rel_risk_PAR, booktabs = TRUE, caption= "Notice the unstable population attribution even if assumptions of causality are correct.", align = 'c', escape=FALSE)
```

## Final caution

Remember, kids: the assumption that PAF describes a causal relationship **might not be correct**: it might describe a confounder, or have an effect modification. The PAF is a model, like any other. In this case it assumes a direct relationship between exposure and outcome, although not its size. In most other cases, we recognise uncertainty, and so we use statistical models to make inferences about the size of the effect.

# Probability and sampling {#probability}

Probability is always a *feature of populations*, not of data. The population is *connected to the data* by a probability using assumptions, so a *sample quantity is an estimator* and not the true probability. The population value, that mythical value again which needs Greek letters to deepen the mythos, is the estimand.

## Kolmogorov's axioms

```         
1. Probability of something happening is 1 $P(\Omega) = 1$

2. Probability of nothing happening is 0 $P(E)\geq 0, P(E) \in R$

3. Probability of a thing is 1-its opposite 

4. Probability of at least one of some mutually exclusive things is the sum of their probabilities 

5. If event A implies event B then probability of A < probability of B 

6. The joint probability of any 2 events is the sum minus their intersection 
```

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

Probabilities in the same probability space are independent if $$P(A) \perp P(B) \textrm{,  if: }  P(A \cap B) = P(A).P(B)$$

The conditional probability is obtained by dividing a joint probability by a marginal probability, *eg* $$P(x_3 | y_2) = \frac{p(x_3, y_2)}{p(y_2)}$$ \## Probability functions

The probability that a random variable such as $X$ will take on a particular value such as $x$ is kind of the whole point of the enterprise. The true value, and any uncertainty about that value, is all expressed in the probability of each of the values that $X$ can take on. The possible values of $X$ are different for each situation, or problem. They might be binary, constrained to a set of integers, or continuous. The **function** $f(x)$ that defines the probability for each of these values satisfies Kolmogorov's axioms, in particular it sums to one.

These are called [distributions]. For discrete values, it's a probability mass function which is positive for each value, and the probabilities at each value sum to 1.

For continuous probability, the probability density function PDF is a continuous function, and although the probability of a single specific number is 0 the area under which still integrates to 1; integrating the pdf yields to cumulative distribution function CDF. The area under a part of the pdf corresponds to the total probability for those values of that random variable. The Cumulative Distribution Function is therefore $CDF(x) = P(X \leqslant x)$.

Quantiles: the $\alpha^{th}$ quantile of the probability function f(x) is the point such that $F(x_{\alpha}) = \alpha$

Survivor function is S(x) = 1-F(x)

## Bayes rule

```         
$$f(\pi | x) = \frac{\textrm{prior} \cdot \textrm{Likelihood}}{\textrm{Normalising constant}}$$
$$f(\pi | x) = \frac{f(x) L(\pi | x)}{f(x)}$$
```

The normalising constant is the marginal probability of the observed value of x. This is usually the integral of the probability of that value under all the hypotheses under consideration, and is needed to scale the likelihoods to sum to 1 and to represent a genuine probability. It can be left out for many uses, as it's a constant for each $\pi$ and so the relative probability of each hypothesised value of $\pi$ is the same without it.

$$f(\pi | x) = \frac{f(x) L(\pi | x)}{\sum_{\textrm{all }\pi} f(\pi) \cdot L(\pi | x)}$$ $$\textrm{posterior} \propto \textrm{prior} \cdot \textrm{likelihood}$$ ... which is close enough for jazz. In particular, it means that we needn't calculate the normalising constant in order to identify the posterior model:

$$f(\pi | x) = \frac{f(x) L(\pi | x)}{f(x)} \propto f(\pi) L(\pi | x)$$

```{r}
#define constrained probabilities for wins, just to demonstrate the model 
chess <- data.frame(pi = c(0.2, 0.5, 0.8))
#define prior model, the relative probabilities we give to each of the win probabilities 
prior <- c(0.1, 0.25, 0.65)
#simulate 10000 values from the prior
set.seed(84735)
chess_sim <- dplyr::sample_n(chess,
                      size=10000, weight = prior, replace = TRUE)
#simulate match outcomes, depending each on the value of pi
chess_sim <- chess_sim %>% 
    mutate(x = rbinom(10000, size=6, prob=pi))
# plot
ggplot(chess_sim, aes(x = pi)) +
    geom_bar()
# tabulate with this potentially nice but oh I'm so tired of tables table generator in the janitor package
chess_sim %>% 
  janitor::tabyl(pi) %>% 
  janitor::adorn_totals("row")
#Plot x by pi using this kinky trick with count-to-proportions
ggplot(chess_sim, aes(x=x)) +
    stat_count(aes(y = ..prop..)) +
    facet_wrap(~ pi)
#narrow it down to those with only 1 win, then plot it. Piping it makes a ggplot object; take out the pipe to leave win_one as a data.frame 
win_one <- chess_sim %>% 
    filter(x==1)  
ggplot(win_one, aes(x=pi)) +
    geom_bar()
#summarise the posterior in a table
win_one %>% 
  janitor::tabyl(pi) %>% 
  janitor::adorn_totals("row")
```

## Bayes rule and diagnosis

Remember the diagnostic likelihood ratio of a positive test (DLR+) $$\frac{P(+|D)}{P(+|D^{c})} = \frac{sensitivity}{(1-specificity)}$$

and the complementary DLR-ve, which is $\frac{P(-|D)}{P(-|D^{c})} = \frac{(1-sensitivity)}{specificity}$.

Because $P(D) = (1-P(D^{c}))$ the complementary Bayes rules for positive predictive value is $$P(D|+) = \frac{P(+|D).P(D)}{P(+|D).P(D) + P(+|D^{c}).P(D^{c})}$$

And the Bayes rule for negative predictive value therefore is $P(D^{c}|+) = \frac {P(+|D^{c}).P(D^{c})}{P(+|D).P(D) + P(+|D^{c}).P(D^{c})}$, and dividing one into the other gives the posterior odds being equal to the diagnostic likelihood ratio times the prior odds:

$$\frac{P(D|+)}{P(D^{c}|+)} = \frac {P(+ | D)}{P(+ | D^{c})} \times \frac{P(D)}{P(D^{c})}$$ When a prior has the same model family, or distribution, as the posterior (with different "tuning" or parameterisation due to the Likelihood), it's called a [conjugate] prior.

## Presneill

$P(B_{k}|A) = \frac{P(A|B_{k})}{\sum_{i=1}^n[P(A|B_{i}).P(B_{i})]}.P(B)$

## Chebyshev's inequality

This helps to prove the law of large numbers. The probability that a variable takes the value of k standard deviations from the mean is a maximum of 1/k squared, for any distribution where the mean has a single value. This is conservative, but follows from axioms. The slight downfall is that of course there is still considerable uncertainty over $\sigma$ after observing a single sample standard deviation. The inequality is that the probability that the observed value is more than k sigma away from its mean is small and rapidly decreasing with k $$P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}$$

#Distributions

I don't know, maybe it's time to bring back some of these babies, just for fun.

## Bernoulli mass function

$p(X=x) = p^x.(1-p)^{1-x}$ $p(X=k) = \binom {n}{k} p^k(1-p)^{n-k}$ where $\binom {n}{k} = \frac {n!}{k!(n-k)!}$ (remembering that $\binom {n}{0} = \binom {n}{n} = 1$)

## Normal distribution

Any normal distribution is a multiple of sigma and an addend of mu from Z, the Standard Normal Distribution: $$Z = \frac{x - \mu}{\sigma},  N \sim (0,1)$$ So if X is a normally distributed variable with mean mu and sd sigma then $X = \mu + \sigma Z, N \sim (\mu,\sigma^2)$

65%, 95% and 99% of the SND lie within 1, 2 and 3 SD of $\mu$.

Another identical way to state the Gaussian distribution is with $\tau = 1/\sigma^2$. BUGS and JAGS like this. The resulting probability density function is $p(y | \mu, \tau) = \sqrt{\frac{\tau}{2\pi} e^{-\frac{1}{2}(y-\mu)^2}}$.

## Exponential

Useful when it must be strictly positive, such as a scale parameter. The average is the inverse of the rate.

## Multinomial distribution is

$$P(x_{1},\dots x_{n};k) = \frac{k!}{x_{1}!x_{2}!\dots {x_{n}!}}.p_{1}^{x_{1}} \dots p_{n}^{x_{n}}$$

of which a special case is the binomial distribution $P(x, k) = \frac{k!}{x!(k-x)!} \times p^{x}.q^{k-x}$

T distribution is descended from the standard normal, and assumed to be symmetric and centred on 0 so with only 1 parameter, the degrees of freedom. Skew can be exponentially transformed or another distribution used.

## Poisson

The Poisson distribution has both mean and variance equal to lambda, which is events over a given time, $\lambda = \frac{D}{t}$ and noting $1-x \approx e^{-x}$ so $(1 - x)^n \approx e^{-nx}$ its probability mass function for the unrealised variable X adopting a certain non negative, integer value x is: $$P(X=x;\lambda) = \frac{\lambda^x.e^{-\lambda}}{x!}$$

The relationshiip between the rate, time spent at that rate and the risk for a given proportion of the initial population to have experienced the event, or for any given member to have experienced it, is given in the way hinted by the above exponential relationship for survivor proportion:

$$\textrm{Risk} = 1 - e^{-\lambda t}$$

Standard Error of a rate is SE(number of events)/person-time at risk $SE_{events} = \sqrt{D}, SE_{risk} = \frac{\sqrt D}{t}$ and $\lambda = \frac{D}{t}$ so $SE = \sqrt{\frac{\lambda}{t}}$.

Standard Error of a log rate is $\frac{1}{\sqrt{D}}$, CI is $\lambda^{\times}_{\div} EF$, and EF=$e^{\frac{1.96}{\sqrt D}}$. A difference of log rates is the same as the log of the rate ratio on the original scale, = ${\lambda_0-\lambda_1}^{\times}_{\div} e^{1.96 \sqrt{\frac{1}{D_0} + \frac{1}{D_1}}}$

Used for example in modelling especially unbounded count data, $X \sim Poisson (\lambda t)$ where $\lambda = E[X/t]$ approximating the binomial for large n and small p; let lambda=np modelling contingency tables. In censoring survival models (as an "unbounded" problem). NHST is the Wald against H0: lambda0 = lambda1, $z = \frac{\textrm{log}(rate \> ratio)}{s.e. \textrm{log}(rate \> ratio)}$

## Beta binomial

First remember $\Gamma(z) = \int_0^\infty x^{z-1}e^{-x} dx$, and $\Gamma(z+1) = z\Gamma(z)$, and when *z* is a positive integer, this simplifies to $\Gamma(z) = (z-1)!$.

Then the pdf is: $$f(\pi) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \pi^{\alpha-1} (1- \pi)^{\beta-1} \textrm{ for }\pi \in [0,1]$$ $E(\pi) = \frac{\alpha}{\alpha + \beta}$ $Mode(\pi) = \frac{\alpha-1}{\alpha + \beta - 2}$ $Var(\pi) = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}$

# Beyond subjectivity and objectivity {#gelman}

Gelman and Hennig at the RSS 2017 presented on the false dichotomy that hides bad behaviour by both sides[@gelman_beyond_]. They propose that Objectivity be replaced by transparency, consensus, impartiality and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. Together with stability, these make up a collection of virtues that they think is helpful in discussions of statistical foundations and practice.

# Exposure measurement {#exposure-measurement}

The true exposure is like True Grail, only seen by indirect evidence of its presence by an Instrument: this is the Measured Exposure. Exposure measurement errors arises from the Instrument's design, its protocols, training or attention or malice or prejudice of those who use it; from the subjects in their memory, cyclic or random or circumstantial variability or recall; and from entering or coding or transforming data. Bias can be caused by differential measurement exposure error. If a parameter X is a proxy measure of the true parameter T, with a fixed bias b across any sample, and an additional random error E then $t_i = x_i + b + e_i$, where E(E)=0, E(X) = E(T)-b and so on. If the bias is not fixed, then the regression is not simple but multiple. This can be hard to detect if not suspected on the basis of mechanistic understanding, which couples with unpredictable "test coverage" of regression models to reflect the true situation. Non fixed bias is introduced by differential measurement error, among other things. Precision is given by the correlation of T with X, the validity coefficient. This all remains a bit theoretical unless the values of T can be known. $\rho^2_{TX} = 1 - \frac{\sigma^2_E}{\sigma^2_X} = \frac{\sigma^2_T}{\sigma^2_X}$ , and assuming linear relationships for Y with both T and X, if both $Y = \alpha_T + \beta_T T$ and $Y = \alpha_O + \beta_O T$ then $\beta_O = \rho^2_{TX} \beta_T$ but if X is a function of T rather than an addend, this doesn't hold.

This assumes a linear model, or rather two simultaneous models for T and X. Logistic models for log(OR) can also be built, so that the observed odds ratio depends on the correlation of T and X, tending to 1 as the correlation tends to 0. $$OR_O = OR_T^{\rho^2_{TX}}$$

The correlation can be expressed separately as sensitivity and specificity of X for T. The odds ratio or sign of the regression coefficient don't cross the null as long as (sens + spec) \>= 1, or as long as an exposed case is classified as exposed with greater probability than an unexposed control is classified as exposed.

Scatter plots of data allow clues about linearity or remote or influential points.

# Sampling {#sampling}

The study population is the population available for study. A [sampling frame] is the entire extant list of possible units which could be sampled.

"Equal probability" selections (of each final stage unit):

-   population value is estimated by sample values (self-weighting) eg Simple Random Sampling (and K&S alledges that Systematic Sampling is an EPS...?)
-   Probability proportional to size followed by numerical SRS (eg 4 at each stage 1)
-   SRS at stage 1 followed by proportional SRS (eg 20% of each stage 1)

Others need weighting at the analysis stage proportionate to their oversampling: - Stratified -- sample within strata and add together - Hierarchical -- Primary or First Stage (or Tier) or 1°, within which Second Stage (2°, etc) - "Probability Proportional to Size" then SRS with constant Tier 2 sample size

If different sized 1° samples, gives equal probability of sampling each 2° unit. SRS then SRS with 2° being a constant sampling fraction of 1° if equally sized 1° samples

Balance covariates by

-   stratification (usually logical)
-   blocking (see Fisher and Student's fight)
-   minimisation (re-read Senn 2004)
-   matching (by pairs, read Eldridge 2012).

## Matching

Matching attempts to make the groups comparable. The main effect is to reduce the discernible sources of variation in the final analysis, by ensuring that the variation between the members of a matched pair is minimised. This will increase statistical efficiency in most cases. In some cases it may reduce confounding, but that's not even as guaranteed as the gain in efficiency.

-   Frequency matching creates groups who are exposed, or unexposed, and who are the same in the frequency of whatever matching variables is desired. Group parameters are calculated and pooled, as in the MH method.

-   Individual matching takes cases and only selects controls who possess the same value of the matching characteristic.

Matched analysis is needed when any selection has been done by way of the matching criteria. The "summary" or "pooled" odds ratio across all strata is a weighted sum: $$OR_m = \frac{\sum D_{1j} D^c_{0j} / N_j}{\sum D_{0j} D^c_{1j} / N_j} , ^{\times}_\div e^{1.96 \sqrt{\frac{1}{N_{01}} + \frac{1}{N_{10}}}}$$ The accompanying test for a 2 by 2 table only uses information from pairs with discordant outcomes, and tests the hypothesis that the summary odds ratio is 1: $$\chi^2 = \frac{(n_{10} + n_{01})^2}{(n_{10} - n_{01})}, df = 1$$

There is selection involved in matching, which is not addressed in most places. The result of matching is that if the groups or individuals are perfectly matched, they are perfectly balanced for all characteristics other than the exposure of interest. Matching cases with controls for a variable most directly allows the estimation of effect modification by that variable. It also makes that variable ignorable if both groups really are identical except for the the exposure of interest. This reduces variance and therefore can increase precision or efficiency. It may also add an unknown amount of control for unacknowledged confounders if those are correlated with the matching variable.

Matching, *eg* by *"malaria rates last year"*, can cause problems. I intuit that if the matched variable is not the right one then no inferential efficiency is gained, eg if "rank of malaria incidence" is very different in the year of study then they're not matched by malaria incidence and random baseline variability may be considered to have been reduced when it actually hasn't. So that's a problem like HTE or undermatched confounders. As it happens they're not very correlated at all, r=0.2 or so. Also overmatching is logistically hard and increases concordant pairs.

Matching does lose a little power because only discordant pairs contribute to estimates, but it gains more power back by reducing variability. The problem that is hidden is if the matching and exposure variables are strongly correlated, when power is lost, probably without being recognised.\
If the matched variable is on the causal pathway it causes a bias, which will generally be to underestimate the effect of exposure on outcome.

Between these two effects there are "break-even values" for the correlation between matching and outcome variables for matching or stratification to be beneficial.

Matched Odds ratios are derived from MH summary odds ratios where only discordant pairs count: if individually matched then each pair is a stratum, if the tabulation is at pair level then two boxes contain zeroes for each concordant pair. Overall numbers of pairs are tabulated for each combination.

## Randomisation

Randomisation is distinct from random sampling - "In a properly randomised trial any difference between the groups outside that imparted by the intervention are due to chance." - "Any difference between the trial groups should be due to the outcome." - True randomisation depends on randomness of sequence and allocation concealment.

# Likelihood {#likelihood}

A probability conditioned for a model; or a probability of a model given some data; the likelihood is numerically (and can be computationally) equal to a probability of the data given the model; eg a binomial and cauchy likelihood exist for the same data and the same parameter space, and those two likelihoods are not always the same. The likelihood is calculated for each point in the parameter space and where it is maximal is the point best supported by the data, that is the likelihood which makes the data most likely under the model assumption: this is the maximum likelihood estimate MLE.

Because the whole curve is fitted, it has to depart from at least some of the observed values at least some of the time. This means that it rejects data, but it adds clarity and comparability between well designed tests.

## Quadratic approximation of the Likelihood

If the likelihood is normal, so log(likelihood) is quadratic and so easily solved using the pattern $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. If it's not standard normal then it's not easy to do and iteration is probably as good as algebra. The quadratic is chosen so that the curvature of the actual log likelihood and the quadratic fitted approximation are identical at the fit point, whether the MLE or the null, depending on whether the aim is to test or derive a confidence interval, or to determine the MLE. Wald tests fit at the MLE.

Other points are divided by the MLE to yield the likelihood ratio. They are standardised by comparing to the MLE, so that with all the approximations above the ratio at the MLE is 1 and the ratio at each 97.5% centile is 0.1465. A ratio is the exponential of a difference, and logs of likelihood ratios to the MLE are convenient because they are all negative.

The log(LR) at the 97.5% point is -0.192, useful for the Gaussian assumption underlying the quadratic approximation: $log(LR) = -\frac{1}{2}\frac{\bar{x} - \mu}{\sigma \sqrt{n}}$. Twice the log of the normal likelihood ratio is algebraically equivalent to the chi squared distribution, giving a chi squared statistic for every value of the parameter, given the data, under those assumptions. The fit of the curve is less good for any point other than the deliberately fitted one, so the choice is to fit at the MLE or at the null.

The quadratic is chosen to meet the log LR curve at the MLE and to have the same curvature: $$\textrm{log(LR)} = - \frac{1}{2} (\frac{M - \pi_0}{S})^2$$ as for D failures in N trials D/N = M

## Likelihood ratio statistic

The likelihood ratio statistic compares the likelihood at the chosen point on the line with another, such as the null. It can also be used to compare two likelihoods such as the MLE of two different models, giving an estimate of how relatively likely the data is under each of the models and so if they differ in only one parameter, what the probability is of the data .\
$$LRS = -2 log(LR), \sim \chi^2, df = 1$$

## Wald test

The Wald statistic describes the difference between two possible values for a coefficient of the model in units of standard errors, so $W = \frac{\hat{\beta} - \beta_0}{\hat{SE}(\hat\beta)} \sim N(0,1)$, or the square of all that which is $\chi^2$ distributed. If testing the null hypothesis that the coefficient is zero, which is the usual use, then $\beta_0$ disappears and the statistic is then the coefficient divided by its standard error. For logistic or poisson regression this is fixed at 1 because it's a conservative standard error in the first place and there's no single framework to describe it outside of that. For ordinary least squares regression it's estimated from the residuals of the fitted values using $\hat{SE}()\hat{\beta_j} = \sqrt{s^2 (X'X)_{jj}^{-1}}$, the product of those residuals and the design matrix. so for log or Poisson it's $LRS_{Wald} = (\frac{MLE}{S})^2$, and its square root if H0:log(LR)=0 is $z = \frac{MLE}{S}$, and for OLS it's t.

The Score Test requires an alternative curve. Usually this is the quadratic approximation to the normal likelihood which matches value, gradient and curvature at the null rather than at the MLE. The test itself is $\textrm{score}^2 / \textrm{score variance}$

This is realised as a chi squared test. If U=gradient and -V = curvature fitted at the null, then $-2\textrm{log(LR)}_{null_{quadfit}} = \frac{U^2}{V}$ and $$\chi^2_{MH} = \frac{U^2}{V}$$ is derived for the Mantel Haenszel test.

Fuckedly, London uses M not only for D/N but also for log(D/N-D). $S^2 = M(1-M)$, $M = log(\frac{D}{N-D})$ and $S =\sqrt{\frac{1}{D} + \frac{1}{N-D}} = \sqrt{\frac{M(1-M)}{n}}$. $\frac{1}{S^2}$ is called the *information* of the data: the larger the "information", the greater the curvature of log(LR), and the more precise the estimate and so on. A supported range of parameter values with their attendant likelihood over a certain cutoff likelihood can be calculated. As S is the standard error so the LR at the 95% CI limits is $e^{\frac{-1.96^2}{2}} = 0.1465$ if the likelihood curve is normal across the parameter space.

## Binomial likelihoods

are calculated so the most likely value of p is found for k successes in n trials Likelihood = $\binom{n}{k} p^k (1 - p)^{n-k}$, approximated by $-\frac{1}{2}\frac{D/N - \pi}{S}$ with S set as $\sqrt{\frac{p(1-p)}{N}}$ by calculus to match the curvature of the actual likelihood at its maximum, being the sample standard error; the plot of which against p is identical to its ratio, whose log is approximated by the normal assumption: $\textrm{log(LR)} = - \frac{1}{2} (\frac{M - \pi_0}{S})^2$, making the Wald test $-2\textrm{log}(LR) = (\frac{M - \pi_0}{S})^2 \approx \chi^2, \textrm{df = 1}$

## Poisson likelihoods

are calculated so for best guesses at baseline $\lambda0$ and rate ratio $\theta$ for d0 and d1 events over times T0 and T1 log Likelihood $L = (d_0 + d_1) log(\lambda_0) + d_1 log(\theta) - \lambda_0 T_0 - \theta \lambda_0 T_1 + \> constant$ which generates a surface maximal at $\lambda_0 = \frac{d_0 + d_1}{T_0 + T_1}$. Substituting this into the above, L is hence maximal at $\lambda0 = d_1 log (\frac{\theta T_1}{T_0}) - (d_0 + d_1) log (1 + \frac{\theta T_1}{T_0}) + \> constant$ which is the "profile log likelihood for theta"; its plot against theta is the same shape as its ratio-to-maximum against theta; its plot against log(theta) is approximated by: $\textrm{log(LR)} = - \frac{1}{2} (\frac{MLE - \theta}{S})^2$ where S is the SE of theta More complex likelihoods are fitted by iteration on the MLE, *eg* taking nulls (L=MLE=0) for all *n* parameters, and working out, calculating gradient and curvature at each value to sketch the best approximation n-surface with its n-dimensional maximum being the improved estimate of MLE and repeating to convergence which doesn't work if the data are insufficient to estimate the number of parameters or if the profile log likelihoods are non-quadratic when eg Poisson, Logistic and Cox regression uses log transformations, or the similar Score Test of form $-2 \textrm{log}(LR) = \frac{U^2}{V}$ where U is the gradient (*aka* the Score) and V the negative of the curvature of a quadratic approximation to the likelihood fitted at the null (aka the Score Variance), not at the MLE.

The plausible values of all but the likelihood ratio test depend on the units of the fitted quadratic. Also, the fitted values don't have a definite integral for anything above a quadratic equation and more complex likelihoods don't have such approximations.

In regression the LRS tests the joint null that all the variables equal their null values and tests any two models where one is a restricted form of the other.

# Robustness {#robust}

Robust objects (estimators, statistics, models...) are ones which are changed little by perturbations in the data; good robust objects retain efficiency while being robust. Efficient objects are ones which need few observations to attain a given performance. Performance is the ability of an object to describe or predict reality, such as to describe variation reliably, reject a false claim or detect a true difference.

# Clusters {#clusters}

Used when there's a direct effect plus indirect (ecological, herd, cooperative, etc) effect on participants, or when it's difficult to allocate individuals. Indirect include, for infections, that which reduces Quantity (herd, reduced carriage, vector death, epidemic periodicity) or Quality (less or more virulent strains, resistance, non-reproducing strains) of infection or the Immunity of victim (immune recency, cross reaction, polyvalency, multi-hit, immune mediated damage). "Direct" + "indirect" = "total"; if all in a cluster participate the effect is "total", otherwise the "overall" effect is the weighted average of direct + indirect on participants and indirect on non-participants. Individually allocated trials measure the Direct effect; cluster trials measure the Overall which depends on participation fraction and HTE combined with differential participation.

-   Second stage randomisation of individuals within clusters clarifies direct v indirect.

-   Spin the bottle: random direction to walk from centre to border of cluster

-   Used by the Expanded Programme on Immunisation, EPI of the WHO as 30 clusters of 7 randomly select units along the line of walk to be cluster centres

-   clusters are "next nearest until quota of 7 children reached", no callbacks

-   Not probability based ("not self weighting") and

-   In vaccine surveys mothers interviewed, Est is +- 10% at 95%CI assuming Design Effect 2

-   Probability of choosing a child is awfully roughly $P_i = m \times \frac{M_i}{M} \times \frac{n}{N_i}$

-   Where n is the very approximately equal number of children surveyed per cluster, Ni is total children in the ith cluster and others as below and $\hat Var (\hat R) = \frac{1}{m(m-1)} \sum_{i=1}^m{(y_i/n - \hat R)^2}$ so

-   CI is $\hat R \pm t_{1-\alpha, m-1} \sqrt{\hat V(\hat R)}$ and the bias is $\bar \rho - R = -\rho_{p, g}\sqrt{\frac{V(p)V(g)}{\bar g}}$ where

-   g is the proportional change in population since the census, p is the true vaccinated proportion, rho is the correlation between p and g, C is the number of clusters in the whole population and R is the true ratio $R= \sum_{i=1}^C{\frac{g_i p_i}{C \bar g}}$.

Design effect for clusters all of the same size is $1 + ((b-1) \times ICC)$ where b is the number per cluster and ICC is the correlation coefficient within clusters, as an average across all the clusters.

## Compact segment sampling;

Another cluster method.

Areas are initially chosen with PPS, eg from last census. The areas are divided into equal numbers of segments. Within each segment of a given area are an equal number of houses, sketched and recorded. A segment is randomly chosen from each area, and all houses in the segment are sampled.

The probability of choosing a house in the ith segment, and therefore a child, is very roughly $P_i = \frac{m}{S_i} \times \frac{M_i}{M_{tot}}$ where *m* is the number of selected clusters, Si is the number of segments in the ith cluster, Mtot the census population of all clusters in the sampling frame, Mi census population in the ith cluster. The outcome, *eg* vaccine coverage as a proportion, is estimated by a ratio of two random variables, number vaccinated and number chosen, each of which is weighted for the probability of being chosen, which therefore has to be known to estimate the true values. This is unbiased *IF* the population has changed uniformly since the informative census and segments are exactly chosen.

$\hat{Var} (\hat R) = (\frac{m}{(m-1)\hat N ^2}) \sum_{i=1}^m {\frac{(y_i - n_i \hat R)^2}{P_i^2}}$

$\hat R = \frac{\sum_{i=1}^m{y_i / P_i}}{\sum_{i=1}^m{n_i / P_i}}$

## Measures of clustering

IntraClass Correlation Coefficient (ICC or rho) is Between Cluster Variance/Total Variance $$\rho = \frac{\sigma_c^2}{\sigma_c^2 + \sigma_e^2}$$, also $\rho = \frac{\sigma_c^2}{\pi (1-\pi)}$

, or $\rho = \frac{\sigma^2_B}{\sigma^2_B + \sigma^2_W}$ where $\sigma c$ is the between cluster SD and $\sigma e$ is everything else (here, within-cluster variation).

ICC can be used for means, proportions and counts but is not defined for rates, and so another quantity is defined and the equations rearranged to use it. This is *k*, the Between-Cluster Coefficient of Variation, and is the standard deviation for cluster means divided by the overall mean = $\frac{\sigma_c}{\bar x} = \frac{\sigma_c}{\pi}$. The relationship between k and rho is normally $$\rho = \frac{k^2 \pi}{1-\pi}$$

## Design Factor

Design Factor is SE for design/SE under simple random, terminology analogous to Error Factor. In distinction to this is the Design Effect DEFF, which equals the Sample size using the proposed scheme divided by the sample size by simple random sampling. It almost always therefore is also equal to the Variance as proposed / Variance by SRS.

If the outcome measure of interest is a proportion, DEFF = Design Factor squared. This is then 1 + (Number in each cluster-1) \* rate of homogeneity = $1 + \rho(m - 1)$. DEFF on total N is $1+(\bar{m} - 1) \rho$ where m is number in a cluster. This is appropriate if clusters are fairly close in size but if cluster sizes vary widely will overweight the smaller clusters. In the case of highly variable cluster sizes, the coefficient of variation of cluster size $\frac{sd_m}{\bar m}$ is used and the eventual design effect is $$1+ \rho(\bar m(1 +(\frac{sd_m}{\bar m})^2) - 1)$$

Rate of homogeneity **roh** (here $\rho$) is the ICC for single stage clustering, and an equivalent ration of total variance for multi stage clustering.

k is estimated for power reasons from $\sigma c$ : see below or $m= \frac{N(1-\rho)}{(c - \rho N)}$ for cluster size given total N under simple random and number of clusters c. ICC can't be used when the outcome is a rate (events per time); so k is used here.

## Standard cluster sample size calculations

For two rates:\
$$n = \frac{(z_{1-\beta} + z_{1-\alpha/2})^2 . (\lambda_1 + \lambda_2)}{(\lambda_1 - \lambda_2)^2}$$\
As $E(s^2) = \lambda Av(1/y_j) +\sigma_c^2 = \lambda Av(1/y_j) + k^2\lambda^2$ so $\hat \sigma_c^2 = s^2 - r Av(1/y_j)$ and $\hat k = \frac{\hat \sigma_c}{r}$

For two means:\
$$n = \frac{4(z_{1 - \alpha/2} + z_{1-\beta})^2}{d^2}$$ , where $d = \frac{\delta}{\sigma}$, or $\frac{(u+v)^2(\sigma_1^2 + \sigma_2^2)}{\delta^2}$

As $E(s^2) = \sigma^2 Av(1/n_{j}) + \sigma_c^2$, so $\hat \sigma_c^2 = s^2 - \hat \sigma^2 Av(1/n_j)$ across each jth cluster and $\hat k=\frac{\hat \sigma_c}{\bar x}$ where x-bar is the mean of all observations across all clusters combined.

For two proportions, where $q = \frac{\pi_1 + \pi_2}{2}$, then

$$n = \frac{2[z_{1 - \alpha/2}\sqrt{2q(1-q)} + z_{1 - \beta}\sqrt{\pi_1(1-\pi_1) + \pi_2(1 - \pi_2)}]^2}{(\pi_1 - \pi_2)^2}$$

or $$n = \frac{[u\sqrt{\pi_1(1- \pi_1) + \pi_2(1 - \pi_2)} + v \sqrt {2\bar{\pi} (1-\bar{\pi})}]^2} {(\pi_1 - \pi_2)^2}$$

As $E(s^2) = \pi (1 - \pi) Av(1/n_j) + \sigma_c^2$ so $\hat \sigma_c^2 = s^2 - p(1-p) Av(1/n_j)$ and $\hat k = \frac{\hat \sigma_c}{p}$ where p is the combined proportion across all clusters combined.

These look simpler but sacrifice transparency if written for the number of clusters c for rates $c=1 + f\frac{[\frac{\lambda_0 + \lambda_1}{y} + k^2(\lambda_0^2 + \lambda_1^2)]}{(\lambda_0 - \lambda_1)^2}$ or for proportions

$$c=1 + f \frac{\left( \frac{\pi_0(1-\pi_0) + \pi_1(1-\pi_1)}{m} + k^2(\pi_0^2 + \pi_1^2 ) \right)}{(\pi_0 - \pi_1)^2}$$

where f is a factor combining z-alpha and z-beta, eg f=7.84 for power 0.8 and alpha 0.05, 10.5 for power 0.9 and alpha 0.05 and k is the assumed true coefficient of variation between clusters; y is the number of person-time follow up per cluster and m is the number of people per cluster, both of whcih are assumed identical.

If SRS but unequal randomisation ratios with size in each group n, then if the smaller group is n1, then alter the size of each group by $k= \frac{n_2}{n_1}$ and $n_1 = \frac{n(k+1)}{2k}$, which converges on $n_1 = \frac{n}{2}$ as the ratio k increases, at the cost of imprecision in estimates of the effect in group 1, and less information on rare events.

1 Was there an effect of treatment in this trial? 2 What was the average effect of treatment in this trial? 3 Was the treatment effect identical for all patients in the trial? 4 What was the effect of treatment for different subgroups of patients? 5 What will be the effect of treatment when used more generally (outside of the trial)?

Because few clusters tend to be randomised and the power calculations nastily fudged, baseline imbalance is common, with instability of the effect size estimates and hence increased false claims in both directions. Balancing of baseline covariates is often thought to limit this.

Balance is achieved by [matching] (add 2 instead of 1 to the RHS of the simplifed equation, k is the average coefficient of variation only between matched pairs), stratifiction or restriction / constraint (reject all random allocations until one is generated with eg 10% difference in any covariate per arm). Among these only matching is random, depends on the matching variables being well chosen and doesn't permit analysis of the main effects of any matching variable.

The estimates are either calculated over the whole study, or combined from cluster-specific estimates. This is not straighforward : if equal weighting of cluster estimates is desired and the clusters were not randomly selected from a well defined target population then cluster estimates are better if they were randomly selected with PPS then the overall estimate is a consistent estimator of the true population value, and is of course easier but it doesn't allow simple t tests.

To derive the standard error the variance of the risk or, interchangeably, the rate ratio, for cluster specific estimates is estimated roughly by working in the log RR scale $Var(\textrm{log} RR ) = Var(\textrm{log} R_1) + Var(\textrm{log} R_0) \approx \frac{Var(R_1)}{R_1^2} + \frac{Var(R_0)}{R_0^2}$, and $Var(R_1) \approx \frac{s_1^2}{c_1}$, where s1 is the observed SD of the cluster specific estimates across the number c of clusters in arm 1 (the intervention, for example). So the estimate accounting for clustering is $\textrm{log RR} \pm 1.96 \sqrt{Var(\textrm{log RR})}$. A simple T or Wilcoxon rank sum test can be used for the summary estimates using these summary values.

## Cluster level analysis

Adjusted estimates of the effect of treatment allocation rely on the differences (residuals) between a model that accounts for all important-seeming covariates (that is, all except for cluster effects and the effect of treatment allocation). These residuals will be randomly distributed across clusters under the null hypothesis, which is then tested by analysing them in place of the cluster level raw estimates. For example, a poisson regression provides estimates; residuals are calculated as the ratios of rate in each cluster to the predicted rate for those individuals under the model; the mean and SD of residuals are calculated for each allocation group; these provide a T test (or equivalent) and the ratio of mean residuals is the estimate of effect size, with Var(RR) calculated analogously to the above, this time for the ratio of residuals.

## Individual level analysis

With linear models, assume the data are all iid but that there is a cluster effect (data are still independent within a cluster but are all drawn from the same independent distribution). Use poisson for rates and a gamma error, giving a negative binomial likelihood. Normal for risk and a normal error, giving a normal likelihood. Binomial for logit and normal error, giving a non analytic likelihood which is converted to a log which is approximated by a log normal using the minimum of a quadratic equation to pass through a certain number of points representing the data. This last might be unstable on changing the number of points to fit, if the approximation likelihood is not a good reflection of the whole data. This is revealed by a quadrature check with \>1% difference in likelihood on changing the number of quadrature points; this is actually probably not necessary now with adaptive quadrature being used at least in Stata. If that's the case:

## Generalised estimating equations

GEE assume that any two points from separate clusters are uncorrelated, but within a cluster all points are identically correlated with each other, represented by an exchangeable correlation matrix derived from the data. This estimate of rho is the primary output; then the regression coefficients and their standard errrors are estimated from it in turn. The regression coefficient is a population average odds ratio, assuming no cluster level random effect but only correlation... I thiink. Overall the buzz words are exchangeable correlation matrices and robust standard errors, with a simple Wald test $z = \frac{\beta}{\textrm{SE}(\beta)}$ once all the hard sums have been written around the robust errors.

## Cluster level t test

Cluster level analysis with a t test is robust and has good coverage even with low cluster numbers (15 or fewer) but can't adjust well. Linear models are efficient, GEE robust. With binary data they differ: random effects models estimate cluster specific - averaged odds ratios and GEE estimate population averaged odds ratios. With 15 clusters in total you can estimate continuous or rate coefficients, with 30 their standard error; or 30 and 50 respectively for binary outcomes.

GEE inflate Type 1 error if few clusters; and vague hand waving about linear modelling's distributional assumptions not being easy to support if there are few clusters.

Moving field: Bayesian, restricted maximum likelihood.

# Variance and error terms {#variance}

For categorical data, variance is p(1-p) and sd therefore $\sqrt{p(1-p)}$ and standard error $\sqrt{\frac{p(1-p)}{n}}$ standard error for the difference in two proportions is $\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$

s.e.m., eg for the representativeness of a sample mean of its normally distributed population, is $$\frac{s}{\sqrt{n}} = \sqrt{\frac{s^2}{n}}$$

and for 2 independent samples is $$\textrm{sem} = \sqrt{\frac{s_{1}^2}{n_{1}} + \frac{s_{0}^2}{n_{0}}}$$ If it's reasonable to assume alike variance, such as for example randomised samples, $$\textrm{sem} = s_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{0}}}$$ for which the sample size-weighted common estimate of sigma assuming alike variance is $$s_{p}= \sqrt{\frac{(n_{1}-1).s_{1}^2 + (n_{0}-1)s_{0}^2}{(n_{1} + n_{0} - 2)}}$$

paired samples are modified if they are alike by rho $$s = \sqrt{(s_{1}^2 + s_{2}^2 - 2 \rho . s_{1} . s_{2} )}$$ which simplifies to $\sqrt{(s_{1}^2+s_{2}^2)}$ if the "pairs" are nothing of the kind, and not correlated at all so rho = 0 $$t = \frac{\bar X_{1} - \bar X_{0}}{sem}$$

and if variances are unequal the distribution of the difference of means is *NOT t*! It's something close to t that is well approximated by changing the degrees of freedom by $df = \frac{(\frac{S_{x}^2}{n_{x}} + \frac{S_{y}^2}{n_{y}})^2} {(\frac{(\frac{S_{x}^2}{n_{x}})^2}{(n_{x}-1)}) + (\frac{(\frac{S_{y}^2}{n_{y}})^2}{(n_{y}-1)})}$ which may be non integer

This altered t distribution is used to create the quantiles for confidence intervals etc. $eg \bar Y - \bar X \pm t_{df} \times \sqrt{\frac{s_{x}^2}{n_{x}} + \frac{s_{y}^2}{n_{y}}}$

and for proportions

standard error for a confidence interval: $\sqrt{\frac{p_{1}.(p_{1}-1)}{n_{1}} + \frac{p_{2}.(p_{2}-1)}{n_{2}}}$ standard error for a hypothesis test assumes both are from the same true population probability $\pi$ so uses the common proportion (all +ve / all n) before repeating the above formula: $\sqrt{p(1-p).(\frac{1}{n_{1}} + \frac{1}{n_{2}})}$

## Poisson regression

This is very similarly shaped to other regressions: rate = constant x timeband x exposure. Under a poisson distribution the lambda is equal across strata; if it's not and especially if there's a linear trend, there is overdispersion. If overdispersion is suspected by qualitative examination of rates, or by the ratio of LRS:df being greater than 1, then ~~an adult must be asked for help~~ a value (constant or distributed) can be added to the regression model to account for the departure from poisson. The value is called the frailty and the resulting model is a negative binomial model.

## Cox regression

limits each timeband to that containing one event (call it a timeclick). So rate = Changing baseline \* exposure: the model assumes the exposure effect is constant (the proportional hazard assumption). Test this graphically or by taking logs of the cumulative hazard so that the "Nelson-Aalen plot" of $\sum \lambda_{ti} = \sum \lambda_{t0} \times \theta_i$ is parallel against time, and test by fitting interaction terms of time (by arbitrary epochs or as a superimposed linear trend) and using LRT or something.

## Multivariate distributions

MV norm has mean as a vector, sigma is the variance / covariance matrix

These have parameters of location, scale and skew. Normal location is the mean, t location is the non centrality parameter which are a scalar for univariate and an n-element vector for n-dimensional distributions (eg an n-variate distribution of densities). Scale for a univariate is the variance for univariate normal and t distributions, but is a dispersion matrix sigma of n by n elements for n-variate distributions (it's the variance / covariance matrix). The skew parameter is a vector of length n for n-variate distributions.

MV t has degrees of freedom usually presumed identical across all variates, a parameter for centrality delta, and a variance / covariance matrix sigma

density by `dmvt(x, delta=c(i,j)`, sigma= `as.matrix(r, t, y, u), df=df, log=T)`

Bivariate has delta=0 if standard and sigma=diag(2), ie rbind(c(2, 0, 0, 2))

#Asymptotics {#asymptotics}

Really very useful topic: the behaviour of functions as the size nears infinity, in this case the behaviour of estimators as the sample size approaches infinity; eg \>Strong Law of Large Numbers:

sample averages converge on the population average as n increases.

> Central Limit Theorem:

"the distribution of averages of i.i.d variables (properly normalised) becomes that of a standard normal as the sample size increases"

So the average of samples, and their variance, converge on population values: a sample is Consistent if it does this, with its estimate converging on the estimated value.

For consistent samples, the mean, sd and the like are all consistent. - The distribution also becomes more normal-like as n rises. - 95% of 95% confidence intervals contain mu; except see below...

The CLT doesn't guarantee that a given n is large enough - sample proportions don't converge very fast on the population proportion, so - 95% confidence intervals don't always contain the true value 95% of the time, but - somewhat less.

"Exact" estimators eg binom.test in R, don't use the central limit theorem and are computationally intensive and while they guarantee 95% "coverage" of the true value are a little conservative. Poisson also don't converge well especially for low p: again, poisson.test is exact.

Sample size increases precision rather than the probability that its coverage includes the true value.

The estimator is $\hat{p} \pm z_{(1- \frac{\alpha}{2})} \sqrt{\frac{p(1-p)}{n}}$ Wald or Agresti-Coull estimators (preferred) work best at p=0.5

-   Wald : $95\% CI = \hat{p} \pm \frac{1}{\sqrt{n}}$, more generally an estimate $\pm$ its standard error: $$\hat p \pm z_{\alpha / 2} . \sqrt{{\hat p \hat q} {n}}$$

-   Agresti-Coull adjusts n by z^2^ so if X successes in n is the proportion p, and simplifying z of 1.96 as 2, adds 2 successes and 2 failures: so $\tilde{n} = n + z^2, \> \tilde{p} = \frac{1}{\tilde{n}}(X + \frac{1}{2} z^2)$ and CI for z = $\tilde{p} \pm z_{(1- \frac{\alpha}{2})} \sqrt{\frac{\tilde{p}(1-\tilde{p})}{\tilde{n}}}$

# Correlation and distances {#correlation}

Correlation is the amount by which a value is more like another given value than it is like all the remaining values in a dataset. It's more usually defined as any statistical relationship between two variables, most usually meaning a linear relationship.

## Categorical

Categorical variance is displayed by tabulation, eg mean pair agreement or Observed Agreement OA. The agreement due to chance is calculated for a table with given marginal totals and is distributed as chi squared:

The expected value in each cell is $E[x] = \frac{\textrm{Row total} \times \textrm{Column total}}{\textrm{Grand total}}$, and if as in table 1 the cells with agreement are a and d, then the maximum chance agreement is $\frac {E[a]+E[d]}{a+b+c+d}$.

Having calculated that, the Kappa statistic for excess agreement over max chance agreement is $\kappa = \frac{OA-CA}{1-CA}$,

## Continuous

Continuous variation is partitioned into systematic and random error, *eg* by ANOVA. The ratio of these errors is the Intraclass Correlation Coefficient, aka reliability coefficient *eg* by Pearson's correlation of product-moment. This is all a little dense and needs some explanation. Pearson's correlation coefficient is the "standardised covariance" between two values, which is the covariance divided by the product of their standard deviations. Covariance itself is the product of the variances of those values, where high positive values mean that high values in one variable are commonly found with high values of the other, and negative values mean that small values of one variable are associated with large values in the other.

$$\rho (X, Y) = \frac{cov (X, Y)}{\sigma_{X}.\sigma_{Y}}$$ , where $cov (X, Y) = E[(X-\mu_{X})(Y-\mu_{Y})]$ and using the identities $\sigma_{X}^2 = E[(X- E[X])^2] = E[X]^2 - E[X^2]$ and

```         
$E[(X-\mu_{X}).(Y-\mu_{Y})] = E[(X-E[X]).(Y-E[Y])]$, so
$\rho_{(X, Y)} = \frac{E[XY]-E[X]E[Y]} {\sqrt{E[X]^2 - E[X^2]} \sqrt{E[Y]^2-E[Y^2]} }$
                
```

then the sample correlation r, assuming

-   a normal distribution of y for each xi

-   a uniform variance for y across all x

-   y is monotonic on x

    -   is the value that describes the closeness of all points to a linear relationship
    -   is useful for a description of the scatter about the least squares linear relationship
    -   is the number of standard deviations that y changes for a 1 SD change in x.

Given by Pearson's Correlation Coefficient:

$$r = r_{xy} = \frac{\sum_{i=1}^n (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{\sum_{i=1}^n (x_{i} - \bar{x})^2}   \sqrt{\sum_{i=1}^n (y_{i} - \bar{y})^2}}$$ , and

can also be quoted as the mean of the standard scores of x and y: $r = r_{xy} = \frac{1}{n-1} \sum_{i=1}^n (\frac{x_{i} - \bar{x}}{s_{x}})(\frac{y_{i} - \bar{y}}{s_{y}})$ , and $\frac{\sum x_i y_i - n \bar x \bar y}{ns_x s_y}$\
= $\frac{n \sum x_i y_i - \sum x_i \sum y_i}{\sqrt{n \sum x_i^2 - (\sum x_i)^2} \sqrt{n \sum y_i^2 - (\sum y_i)^2}}$, or\
$r_{xy} = \frac{\sum x_i y_i - \frac{1}{n} \sum x_i \sum y_i}{\sqrt{\sum x_i^2 - \frac{1}{n}(\sum x_i)^2} \sqrt{\sum y_i^2 - \frac{1}{n}(\sum y_i)^2}}$

r squared explains the proportion of variance that is explained by the straight line, For that straight line the regression coefficient for the straight line $y = \beta_0 x + \beta_1 x +\gamma x + \epsilon$ is $\beta_1 = \frac{\sum(x-\bar x)(y-\bar y)}{\sum(x- \bar x)^2}$ or $\hat \beta_1 = Cor(Y, X) \frac{sd_y}{sd_x}$

And Spearman's is Pearson's rho of the *ranks* of the values, rather than the values themselves.

## Distances

In many ways Pearson and Spearman are ways to quote distances between two vectors. Other metrics are available, such as

-   Cook's distance, in regression roughly the effect of deleting the outlier $D_i = \frac{\sum_{j=1}^n (\hat y_j - \hat y_{j (i)})^2}{(k + 1)s^2}$
-   Mahalanobis distance of a point P from the distribution D along the principal component axes scaled for unit variance to make the Mahalanobis distance the standard Euclidean distance in the transformed space. The distance of each observation *i* from the centre of an $i = 1 \dots n$ by $p$ sample can be calculated by Wilk's method $MD_i = \sqrt{(x_i - t)^T C^{-1}()x_i - t}$, if t is the multivariable location, C is the estimated (usually the sample) covariance matrix. To detect a single outlier in this n by p multivariable sample, the squared Mahalanobis distances are compared with an F distribution with p and 1-p degrees of freedom
-   $\chi^2$ distances, distances are more appropriate for multivariable situations

## Locally weighted scatterplot smoothing {#loess}

LOESS are local polynomial regression smoothers. The running mean smoother is the most simple nearest neighbour method, but isn't actually very smooth, and performs poorly at the edges of the data as there's little to change the average from the extreme values. Instead of the simple mean, LOESS uses a locally weighted regression within each sliding, symmetric, nearest-neighbour window. At each point x, the weight of neighbour points $x_i$ on the fitted value at x is reduced by a "tri-cube" function of their distance from x along the x axis within the sliding window.

Regression smoothing simply seeks to minimise the penalised sum of squares. The first half of the equation below is the sum of squared deviations between s(x), the function smoothing x, and the observed value. The second half is the penalty for wiggliness, using the smoothing parameter $\lambda$ to control the amount of penalty for wiggliness, integrating across the second derivatives of the curve and so producing a zero result for a straight line and ever higher results for more wiggles. The penalised sum of squares is minimised by a natural cubic spline with knots at every datapoint, which is not practical for large datasets, and is wasteful as the effective degrees of freedom are smaller than the number of knots.

$$\sum_{i=1}^n (y_i - s(x_i))^2 + \lambda \int(s''(x))^2 dx$$ \## B-splines

Regression splines are a bit more practical. They are a linear combination of basis functions. Basis functions are defined by what they are components of: they are all members of a function space which is in turn made up of linear combinations of the basis functions. Piecewise polynomials are usually used. For example, a spline with a different set of cubic relationships ($y = \beta_0 + \beta_1 x_i^2 + \beta_2 x_i^2$, etc) with covariates above and below a cut point might have a coefficient times x to the power of 0, 1, 2, 3 for each window, hence 8 degrees of freedom to draw the overall relationship, and hence the ability to draw most conceivable shapes.

$$y = \Bigg\{{\beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31} x_i^3 \textrm{ if } x_i < c;\\
\beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32} x_i^3 \textrm{ if } x_i \geq c}$$

More simply and generally, the equation can be written as $s(x) = \sum_{l=1}^K B_{l,q}(x)\beta_l = B'\beta$ where B is the model matrix of the basis functions in this particular function space. The number of basis functions depends on the number of inner knots *m* and on the order of the spline, fewer functions giving a smoother spline and residual sum of squares minimised by having $K = p + 1 + m$ basis functions.

Penalised regression splines directly penalise wiggliness. Instead of the linear models from above, this leads to penalised sums of squares of the shape below, in which $\beta'P\beta$ is the penalty, and can be many things.

$$\underset{\beta}{\mathrm{min}} = \{ ||y - B'\beta ||^2 + \beta' P \beta \}$$ For a P-spline, or penalised spline, the penalty might penalise the difference between adjacent coefficients, and look like $\beta' P \beta = \sum_{l=1}^{K-1}(\beta_{l+1} - \beta_l)^2$.

## Conditional association 

This can happen in different ways. If a set of variables are all predictive of the outcome, and yet all correlated with each other, then a multiple regression will minimise the individual predictive value of each of them. The association, conditional on the other variables, is zero. This is called [#multicollinearity]. 

This among other things is why you don't compare coefficients: the total model will work fine, even while the contributions of the individual coefficients are wildly unstable. The correlations don't matter, nor the univariate association, it's the conditional association. 

# Hypothesis Testing {#nhst}

The confidence interval is the set of observations for which H0 is rejected, that is an inversion of the alpha level under H0 (eg for the Binomial rather than use an asymptotic approximation to the CLT for 0.95 as the Wald interval (too narrow), this can be directly calculated as an exact interval, the Clopper-Pearson interval (too wide) or another procedure the Wilson Score method). Often overlooked is that evidence for a hypothesis is always relative to that for another hypothesis; often it's the same evidence.

Correlation of parameters within a subgroup implies that, when viewed at the level of the population, there is some component of variation that is attributable to membership of that subgroup. A neater way to say this is that within cluster correlation measures the same phenomenon as between cluster variation. The information is therefore reduced in comparison to i.i.d observations. Robust standard errors, GEE and multilevel modelling are ways to address this.

## Robust standard errors

Robust variance is proportional to the sum of the squared [residuals] (residuals being the difference between the observed values and those predicted by the model). They can be calculated individually if the data are *i.i.d* or calculated for each cluster then summed, where there is correlation. This still assumes independence between clusters.

Robust standard errors do not affect the calculation of the maximum likelihood and so a LRS and test are not valid and the MLE is not altered. A quadratic approximation is not used so the standard errors are not excessively narrow; the cluster level residuals are used to generate the variance instead. "Robust SE are correct provided the model is correct" and there are \>30 clusters.

## Generalised Estimating Equations

GEE take account of correlation in the calculation of the effect estimate as well as the standard error. The correlation matrix structure is assumed: independent (iid), exchangeable (observations within a cluster are equally correlated and are not correlated with those outside the cluster) or autocorrelated (the same observation at different times, for example).

Exchangeable correlation is the usual GEE setting, where the effect estimate is a weighted combination of the effect estimates in each cluster and the standard error is derived from the residuals outside the model. A population average effect is estimated: the average odds of an outcome among those exposed, divided by the average odds among those not exposed.

# Multilevel Modelling and Random Effects models {#mlm}

The effect of cluster, whether innate, unmeasured confounder or a true mechanistic effect, is represented in the regression model by a separate term for each cluster: this is rendered tractable by assuming a stochastic distribution of the size of these terms rather than attempting to measure or specify the cluster effect for each cluster (hence "random effects"):

The log odds of the jth person in the ith cluster is then given by, where for example $u_i \sim N(0, \sigma_u^2)$ and $\sigma_u^2$ is estimated as part of the model building. The same information is imparted by estimating rho, the within cluster (also called "intra-class") correlation coefficient. A cluster specific effect is estimated: the odds of an individual having the outcome if exposed, divided by the odds if not exposed.

The intention is to derive a model that provides a fully specified likelihood: assuming that it has done so the LRT is appropriate. That likelihood becomes complicated easily: a bivariate normal distribution for the likelihood is produced for a normal error plus normally distributed cluster effect, which has an algebraic solution, as does the combination of poisson error and gamma cluster effects to produce a negative binomial; but the combination of binomial error and normal cluster effects, or poisson error and normal cluster effects, produce mixture distributions without roots. The reliability of estimates needs to be checked.

Power - is 1-beta and depends on the assumptions of the population structure - is a function that depends on the value of mu0 - the distributions of the observation under H0 versus Ha hypotheses are calculated - then a line is drawn across the null for alpha; - at that line the extreme tail under Ha is 1-beta = P

If testing Ha: mu \> mu0, $1 - \beta = P (\bar X > \mu_{0} +z_{1-\alpha} \frac{\sigma}{\sqrt{n}}; \mu = \mu_{a})$ , where $\bar X \sim N(\mu_a, \sigma^2/n)$ and mu(0), alpha are known. If 3 of the 4 unknowns are specified the last can be calculated

If assuming only the noncentral t distribution then power is $P = (\frac {\bar X - \mu_{0}} {s / \sqrt{n}} > t_{1-\alpha, n-1}; \mu = \mu_{a})$ ; note $P=(\alpha | \mu_a=\mu0)$ ; On the other hand power depends only on the difference in means divided by S.E.M $\frac {\sqrt{n}(\mu_{a} - \mu_{0})}{\sigma}$, and the effect size is dimensionless and can be used across contexts a bit.

# Survival and logrank test {#survival}

Kaplan Meier survival probability is the cumulative probability of an entity that is initially at risk surviving subsequent epochs until the one in question, each epoch being "data-defined" and usually as containing one event: as $$P_{event} = \frac{n_{events}}{n_{at-risk}} – (0.5n_{censored})$$ if the censoring time is not defined for the interval P(Si) = (1-Pevent1).(1-Pevent2)...(1-Peventi) $P(S_x) = \prod_{i=1}^x (1-\frac{\textrm{events}}{\textrm{at risk} - 0.5 \times \textrm{censored}})_i$ , or $\widehat {S}(t)=\prod \limits_{i:\>t_{i}\leq t}\left(1-{\frac {d_{i}}{n_{i}}}\right),$ A good KM curve has ticks on the line for censoring occurrences and nat-risk below the line Log Rank test is a Chi Squared, with Observed totals and Expected E = (row total x column total)/(grand total). Summed across all intervals for each group, and added, giving the statistic: $\chi^2_{logrank} = \frac{(O_1 - E_1)^2}{E_1} + \frac{(O_2 - E_2)^2}{E_2}$ , df=1 or $Z=\frac {\sum _{{j=1}}^{J}(O_{1j}-E_{1j})}{\sqrt{\sum _{j=1}^J V_j}}$ Not sure whether you could substitute P(Sx) times number at risk at t=0 for E.

The hazard function h(t) is the failure rate summed over ever smaller time intervals, which may be more than 1, hence is not a probability. This depends on a failure function, F(t), which is a probability, being a cumulative distribution function. The P(Si) above is equivalent to the reliability function R(t), also called the survival function S(t), the probability of "no failure" as a function of time. R(t) = 1 -- F(t). Cumulative failure as a function of time, F(t) is the integral over time of the probability density of failure as a function of time, f(t).

$\tilde{H} (t) = \sum_{{t_{i}\leq t}}\frac{d_{i}}{n_{i}}$

h(t) = f(t) / 1-F(t) = f(t) / R(t). $h(t) = \lim _{\Delta t\to 0}{\frac {R(t)-R(t+\Delta t)}{\Delta t\cdot R(t)}}$, which does not have to be parametric as long as there is a definite cumulative distribution.

If the failure density is modelled as exponential (ie Poisson(1), that is the time until first failure is Poisson distributed and other failures are presumed to be independent in every case including those already failed) then the familiar result is that $f(t) = \lambda^1 e^{-\lambda t}$ so $F(t) = \int_0^t \lambda e^{-\lambda t} dt$ then the hazard function h(t) is $h(t) = \frac{f(t)}{R(t)} = \frac{\lambda e^{- \lambda t}}{e^{- \lambda t}} = \lambda$, which is time-blind / memoryless / constant with respect to time.

Assuming, then, that the hazard function is constant and the cumulative hazard rises linearly:

-   Risk = events/number at risk;

-   hazard = (events/time at risk in the limit as t goes to 0) = lambda;

-   survival as a function of time S(t) = product of 1-risk across times;

-   cumulative hazard = sum of risks for all times (Nelson-Aalen estimate of the cumulative hazard).

The log of the hazard ratio for two exposures is log(HR1) -- log(HR2) = log(constant) H(t) = -log(S(T)) or S(t) = e\^(-H(t)) S(t) = e\^(-lambda t) H(t) = lambda t Risk up to time t = 1- e\^(-lambda t) Average survival time = 1/lambda

# Time Series {#time-series}

A regression model $Y_i = \beta X_i + \epsilon_i$ assumes the errors are independent, normal and homoskedastic with a common variance (the definition of white noise). If the value of X depends on the last value of X, then the value is said to be regressed on its previous value. This is autoregression, but by separating out the autoregressive part the remaining errors can still be conceptualised, and so written and analysed, as white noise. $X_t = \phi X_{t-1} + \epsilon_t$ and $\epsilon_t$ is still allowed to be white noise,

## Autocorrelation

Autocorrelation can be estimated from the correlation coefficient between the time series and itself. For a yearly oscillating process the correlation is positive at a 1-month lag and negative at a 6-month lag, for example.

## Stationarity

Stationarity is an important concept: if it's entirely stationary then the mean is constant and can be estimated by the sample average. If the correlation structure is constant then it can be estimated by all pairs of observations that are a certain time lag apart. For a series $x_1, \dots x_i$ the lag 1 correlation is estimated from a simple average of the correlation of all pairs that are 1 time unit apart; the lag 2 correlation of all pairs that are 2 time units apart and so on.

A time series that is stationary for mean and correlation structure is [**white noise**].

A **Random Walk trend** is not stationary, as it can wander up and down a long way. The differenced data (the value at t minus the value at t-1, $X_t - X_{t-1}$) is stationary, however.

**Trend stationarity** is displayed by random behaviour around a simple trend, such as an inflation-prone price in stable economic times. Differenced data still shows a stationary process, demonstrating that noise. This is often generated as $X_t = (1 + p_t)X_{t-1}$, being a way to separate the percentage change or [growth rate] $p_t$ from the value at time *t-1* and the result of each of those, the value at time *t*. The growth rate is approximated by $Y_t = \mathrm{log} X_t - \mathrm{log} X_{t-1} \approx p_t$.

If there is trend and heteroskedasticity, whereby neither mean nor correlation can be estimated from simple averages of the raw data, try logging and then differencing: logging positive data stabilises variance and differencing it removes the trend (*"detrends"* it).

## ARMA

If the value of a variable at time t is either static or at least predicted mainly by its value at time t-1, then it's a time series. Some function of time $\phi X_t$, added to an error term, gives the above formula of Autoregression: $X_t = \phi X_{t-1} + \epsilon_t$.

These assume errors are white noise; but usually errors are correlated with the previous error, just as the underlying true value is autocorrelated. So now the model needs to use a moving average. This describes an autoregressive process with autocorrelated errors, where the remaining component $W_t$ is still white noise. Moving average: $\epsilon_t = W_t + \theta W_{t-1}$

So ARMA is autocorrelated with autocorrelated errors, an autoregressive moving average. [ARMA]: $$X_t = \phi X_{t-1} + W_t + \theta W_{t-1}$$

After Wold, any stationary time series can be represented as a linear combination of white noise, as can any ARMA model. This [Wold decomposition] is shown below. The moving average with lag (or "order") *q* is a Wold decomposition where constants are zero after the *q*th term: $$X_t = W_t + a_1W_{t-1} + a_2 W_{t-2} \dots$$ After Gauss and Newton, the AR and MA order can be estimated in a similar way to [least squares estimation]. [^5]

[^5]: Because there is simultaneously an error in the observation generating process, there are always uncertainties in ascribing the output to a particular process. The autocorrelation function [ACF] of a pure AR process tails off slowly but the partial autocorrelation function [PACF] cuts off at lag *p*. A pure MA process has a PACF which tails off slowly and ACF cuts off at lag *q*. ARMA tails off in both ACF and PACF.

## ARIMA

A time series is integrated ARMA or [ARIMA] if the differenced data are ARMA. Easy. The number of times that differencing is required is given by *d*, which finishes the parameters needed for the usual model: p, d, q.

## Regression for time series

The models are fit to the appropriately detrended data using a sort of trial and error starting from simple and progressing to more complex. Model fit is assessed by BIC and AIC, generally accepting the more simple model.

An important criterion in time series is [residual analysis]: if the residuals are not white noise then the maximum explanation of the data generating process has not been found, and a qualitative examination of the ACF and PACF plots might suggest how best to tune the model, before comparing the fit using AIC and BIC.

## Seasonal ARIMA

Seasonal models can be added on top of the non-seasonal effect, making quite complex predictions overall and using arbitrarily many algebraic expressions to autoregress on a previous value. This can be built in Base R or rely on premade packages for the usual case of a small number of lags at seasonal and nonseasonal levels. The seasonal dependence is split into AR and MA components, as is the nonseasonal, and for example using monthly data it might be seen that a pure AR at a lag of 12 $X_t = \Phi X_{t-12} + W_t$ exists. The seasonal components are denoted by capitals where the nonseasonal are denoted by lower case, so a seasonal model at lag S may be SARIMA(p, d, q, P, D, Q, S).

The ACF tails off with a pure AR model, cuts off at lag QS for a pure MA, vice versa for PACF, and both tail off for a SARMA model.

As this is plausible in a facile sort of a way but hard to remember, here is a table:

```{=tex}
\begin{tabular}{| l | l | l | l | }
     & SAR(P)s         & SMA (Q)s        & SARMA(P,Q)s \\ 
ACF  & Tails off       & Cuts off lag QS & Tails off \\ 
PACF & Cuts off lag PS & Tails off       & Tails off \\ 
\end{tabular}
```
# Multiple testing {#multiplicity}

```{=tex}
\begin{tabular}{| l | l | l | l | }
\hline
& $\beta=0$ & $\beta \neq 0$ & Hypotheses \\ \hline
Claim $\beta = 0$ & U & T & m-R \\ \hline
Claim $\beta \neq 0$ & V & S & R \\ \hline
Claims & $m_{0}$ &  $m-m_{0}$ & m \\
\hline
\end{tabular}
```
Control of false positives involves an error measure and a correction for it. There are many such systems, their use depending on the needs of the end user of the data.

False positive rate $E[\frac{V}{m_{0}}]$ is the rate at which false results are called positive. Family wise error rate $P(V \geq 1)$ which converges to 1 for multiple tests even as alpha is fixed, controlled by - Bonferroni: $\alpha_{fwer} = \frac{\alpha} {m}$ (very conservative) or calculate p-values, order them as P1 ... Pm with their null hypotheses H1 ... Hm and either - (Holm, stable) let *R* be the smallest *k* such that $P_{(k)} > \frac{\alpha}{m + 1 - k}$, reject H1 to HR-1 or - (Hochberg, powerful, assumes positive dependence eg in reusing data) let *R* be the largest *k* such that $P_{(k)} \leq \frac{\alpha}{m + 1 - k}$, reject H1 to HR

False discovery rate is the rate at which claims of significance are false. Controlled so that $\textrm{E}(\frac{V}{R})$ is below a chosen threshold q: Benjamini-Hochberg order P1 ... Pm with their null hypotheses H1 ... Hm and for the largest *k* such that $P_{k} \leq \alpha. \frac{k}{m}$, reject Hi where i= 1 ... k which is useful because $E[Q] \leq \frac{m_0}{m} \alpha \leq \alpha$ Or you could report "adjusted p-values" which are no longer p-values eg to control FWER for P1 ... Pm take $P_{i}^{fwer} = \max{m \times P_{i}, 1}$ and call each Pi\<alpha significant

Expected values - are properties of distributions, as are variances of those distributions - the population value is estimated by the relevant sample value

E(x) is linear, so E(cx) = E.c(x) and E(aX+bY) = a.E(X)+b.E(Y) The centre of mass from a probability mass function of discrete data gives that expected value $E[X] = \sum_{x} {x.p(x)}$, extended to a sample mean where $\bar{X} = \sum_{i=0}^{n} x_{i}.p(x_{i})$, where p is independent and identically distributed and is equal to $p = \frac{1}{n}$ for every x.

For continuous variables it's an area under the function t.f(t) where f(t) is the variable's PDF $E[X] = \int t.f(t)$

Given a normally distributed continuous variable X the population variance is $Var[X] = E([X- \mu]^2) = E[X^2] - E[X]^2$

And because binary categorical random variables have expected value $E[\bar{X}] = (1-p).x^{c} + p.x$ then if x is coded as 0 if a variable is not present, eg tails, and 1 if present, eg heads, so $E[\bar{X}] = (1-p) \times 0 + p \times 1 = p$ and $E[X^2] = (1-p) \times 0^2 + p \times 1^2 = p$ then $E[Var(X)] = p - p^2 = p(1-p)$ and also from the definition of Var[X] $Var[aX] = a^2.Var[X]$

The sample variance is sort of analogous as $s^2 = \frac{ \sum_{i=1} (X_{i} - \bar{X})^2}{n-1}$ and $\frac {\sum x_i^2 - \frac{(\sum x_i)^2}{n}}{n-1}$

`r newthought('THE SAMPLE VARIANCE ALSO HAS A DISTRIBUTION')` relating to the population of sample variances from which it is drawn: it is an unbiased estimator, meaning the mean of sample variances is an estimate of the population variance; and the variance of sample means is an estimate of the population variance, scaled for the sample size. I think this is incredibly important. The variance of the sample mean is the population variance divided by n $Var[\bar{X}] = \frac{\sigma^2}{n}$, so s.e.m. = $\sigma\sqrt{\frac{1}{n}}$, because $Var[\bar X]= Var[\frac{1}{n}.\sum X_{i}] = (\frac{1}{n^2}).Var[\sum{X_{i}}] = (\frac{1}{n^2}).\sum \sigma^2 = \frac{\sigma^2}{n}$

The SE for the mean of a log transformed variable, because $log(X) \simeq log(\mu) + (X - \mu)(log'(\mu))$ and $log'(\mu) = \frac{1}{\mu}$, is $SE(log(X)) \simeq SE(X) log'(\mu) = \frac{SE(X)}{\mu}$

For log proportions, using $\frac{SE(X)}{\mu}$, $SE(log (p)) \simeq \frac{\sqrt{\frac{(p(1-p)}{n}})}{d/n} = \sqrt{\frac{1}{d} - \frac{1}{n}}$ so for log risk ratio, $RR = \frac{p_1}{p_2}$ so $\textrm{log(RR) = log}(p_1) - \textrm{log}(p_2)$ and $\textrm{SE(log(RR))} = \sqrt{Var(\textrm{log}(p_1)) + Var(\textrm{log}(p_2))} = \sqrt{\frac{1}{d_1} - \frac{1}{n_1} + \frac{1}{d_2} - \frac{1}{n_2}}$

For log rates $\textrm{SE(log}(\lambda)) = \frac{1}{\sqrt{D}}$ - The log of a product is the sum of the logs - The sum of the logs is the log of the products - The log of a quotient is the difference of the logs - The difference of the logs is the log of the quotient - The exponent on the argument is the coefficient of the log - The coefficient of the log is the exponent on the argument

# Bootstraps {#bootstrap}

Efron and Tibshinari. Permits confidence intervals without very complex maths. Approximate the sampling distribution of a statistic using the distribution implied by the data. Use the data you have and sample within it, with replacement, B times. Calculate the statistic you need to estimate for each new sample you have made. The resampling distribution non-parametrically approximates the population distribution, so the standard deviation of it implies the standard error of the median, confidence intervals can be generated etc. Bias Corrected and Accelerated (BCA) interval performs MUCH better than the raw CI; use bootstrap package to construct it.

## Permutation tests

Powerful and manifold: eg Rank sum test permutes ranks for the rank sum, Fisher's Exact test permutes binary groups for a hypergeometric probability, an ordinary permutation test permutes the raw data. Randomisation tests eg if matched data have differences with signs randomly reassigned the signed rank test is the result; or a regressor of interest might be permuted for regression testing. When grouped / stratified data are compared the labels are irrelevant for forming H0. So take grouped data, calculate some comparative statistic. Reassort the group labels randomly across the data; calculate a statistic for each of these random bins. Repeat many times; this results in a distribution of statistics Observe number of times these differences are more extreme than the initially observed data.

# Paired (matched) data {#paired}

If method A and method B are used on the same sample the discordance (r, s) is informative:

```{=tex}
\begin{tabular}{| c |c | c | c| }
\hline
& $+_{B}$ & $-_{B}$ & \\ \hline
$+_{A}$ &k & r & k+r \\ \hline
$-_{A}$ & s & m & s+m \\ \hline
& &  & N \\
\hline
\end{tabular}
```
This is also used in matched rather than paired data, where one can imagine rho being quite important.

Odds ratio is r/s, the ratio of discordance between the tests, and error factor is $e^{z_{0.05} \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$ so OR with 95% CI is $\frac{r}{s} \times e^{1.96 \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$ to $\frac{r}{s} / e^{1.96 \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$

The discordant proportion is $\frac{r+s}{n}$ and its standard error is $\frac{\sqrt{r+s}}{n}$

So 95% CI for an estimate of the proportional discordance is $\frac{r+s}{n} \pm 1.96 \frac{\sqrt{r+s}}{n}$

Paired categorical data from the 2x2 agreement table tested by McNemar's test: $$\chi^2_{pair} = \frac{(r-s)^2}{r+s}$$ or with the continuity correction $\frac{(|r-s|-1)^2}{r+s}, \nu = 1$, or by MH chi squared with pairs as strata or an exact binomial test if discordant cells \<20.

## Rate data

The rate, call it lambda, is estimated by $\hat \lambda$, usually Events / Total time at risk $\hat \lambda = \frac{D}{t}$ S.E. for the estimate is $\sqrt{\frac{\hat \lambda}{t}}$ so 95% CI is $\lambda = \hat \lambda \pm 1.96 \sqrt{\frac{\hat \lambda}{t}}$ Rate differences are $(\lambda_1 - \lambda_2) \pm 1.96 \sqrt{\frac{\lambda_1}{t_1} - \frac{\lambda_2}{t_2}}$ Expected number of events e1 is $E(D_1) = \frac{D t_1}{t} = \lambda t$

Variance for e is $V(e) = \frac{Dt t_{2}}{t^2}$

And chi squared is $\chi ^2 = \frac{(|D_1 - E(D_1)|-0.5)^2}{V(D_1)} \textrm{, df=1}$, $H_0 = \lambda_1 - \lambda_2 = 0$ or equivalently that $\frac{\lambda_1}{\lambda_2} = 1$

Chi squared for trend, where x is the group score or level, is $$\frac{(\sum D_i x_i - \lambda \sum t_i x_i)^2}{\lambda^2 (t \sum t_i x_i^2 - (\sum t_i x_i)^2)}$$

# Non parametric hypothesis tests {#nhst-non-parametric}

These make very few assumptions about the distribution of a parameter. But not none: they don't assume the shape or skew or kurtosis, and they test the central tendency, but they mostly assume something about them, such as the most common assumption that they are independent and identically distributed (i.i.d), often that they are symmetrical, or that two distributions for comparison are identical to each other. Hence some common tests are of the hypothesis that the sample "differs in degree" from a measured or assumed value.

## Wilcoxon Signed Rank

The signed rank test assumes symmetrical distribution of the parameter under consideration, which can be a set of matched samples, or a sample to compare to an assumed null. It is very similar to the rank sum test, and both the procedure and the name refer to differences from the median of all values.

First, calculate the differences (of each value from the median, or across each pair): N is total nonzero differences. Assign ranks to the absolute values of differences. Sum the ranks of positive differences, sum the ranks of negative differences. The smaller of these sums is T.

The null hypothesis is that T=sum of all ranks divided by 2. Now look up the observed T on a table of probabilities of that value or more extreme for given N. There are $\frac{n(n + 1)}{2}$ possible pairs of differences and the median of all these differences is the Hodges-Lehmann estimator of the central location[^6].

[^6]: And there is an even simpler test, the sign test, which compares the number of pairs with a positive and a negative difference. This is inefficient as it ignores the magnitude of differences, but it gives insight into the reasoning behind the signed rank test.

The real values, D, of these averages are ordered. This forms a series $D_1 \dots D_{T_5} \dots D_i$ where $T_5$ is the T with probability at least 0.025 (the 2-sided p value of 0.05 on the tables). $D_{T_5}$ to $D_i - D_{T_5}$ are the outer bounds of the 95% confidence interval for the difference, again assuming a symmetrical distribution. Clearly for small N these T are very conservative indeed.

## Wilcoxon Rank Sum

Also called the Wilcoxon 2-sample test / Mann Whitney U / Wilcoxon-Mann-Whitney. For the difference of medians in two samples. Assumes data are i.i.d. and makes the strong assumption that the distributions of the two samples "differ only in location", that is that they are identical, for continuous data. The test is still useable for ordinal data with a less strong assumption that P(X\>Y) = P(Y\>X).

Take two samples of size N1 and N2, with a total size N. Order all of the N, give them ranks in that order, equal values take the mean of the ranks they span[^7]. Sum the ranks across N1 as R1, and the ranks across N2 as R2. T is the rank sum in the smaller group.

[^7]: So 3rd to 6th inclusive being equal would mean each on is given 3+4+5+6/4 = 4.5

The total rank sum is $R_1 + R_2 = \frac{N.(N+1)}{2}$. Calculate the number of possible choices of size 1 ... T from groups of size N1 and N2. Sum the possible choices and invert for the probability or look it up for tables of N1, N2. An exact p value can be computed from the sums of signed ranks for all observations i, $SR_i$: $z = \frac{\sum SR_i}{\sqrt{\sum SR^2_i}}$.

More broadly, and this escape me a little for now, is that across strata Ui is $U_i = R_i - \frac{N_1 . (N_1 + 1)}{2}$, and the total possible is $U_1 + U_2 = N_1 . N_2$. $AUC_{ROC_1} = \frac{U_1}{N_1.N_2}$ for ROC curves. 95% CI by bootstrap or [Hodges-Lehman] estimates of location shift[^8].

[^8]: Hodges-Lehman, or more properly H-L-Sen estimators, are interesting. They're not a median or a mean but they have some properties that place them somewhere between. They can be used for marking the relative position along a distribution with easily explained performance characteristics, and so allow decisions about observed values and future values. The H-L estimator for location of a univariate distribution is the median of all possible differences between a measurement from group 1 and a measurement from group 2.

## Spearman Rank Correlation Coefficient

Ranks for each observation are calculated (ties given average rank). Pearson's correlation coefficient calculated for the ranks as if they were x, y coordinates.

# Data types and features {#data-types}

This is partly covered by the [sampling](#sampling) section, but it's integral to demonstrating epidemiological understanding.

## Questionnaires

Questionnaires are able to ask about

```         
attitudes
beliefs
behaviours
experiences
attributes
```

Methods of applying the question are by

```         
Interview
Questionnaire
Diary
Observation
Routine Record Reaching
Measurements
Environmental Measurements
```

Standard introductions are important. If appropriate to use a questionnaire, develop one that samples the minimum amount of an individual's total experience that will provide sufficient information concerning the problem under study (including exposure and covariates). It must:

-   be easy to answer, process and analyse,

-   minimises measurement error,

-   preserves dignity and privacy of respondent, and

-   limits burden on respondents.

Burden is added by

-   length

-   old information

-   low salience (and high sensitivity)

-   frequent events

-   greater depth of detail,

-   being a proxy respondent

Too much burden causes low response, failure to complete, low quality or random responses, alienation from questionnaires.

Responses are maximised by

-   low burden

-   trained interviewers

-   personalisation

-   storyboarded not scripted acknowledgment and introductions

-   facilitation (SSAEs for return,face-to-face, diligent attempts to contact, incentives)

Allow responses by softening or appearing to understand sensitive questions, giving an option to give a socially acceptable answer before reporting true exposure. Start with a relevant question, leave demographics to the end. Go by concept group, go from general to particular, go from bland to sensitive. Give an introduction, instructions (including the logic of the questions), link parts with language and conclude with thanks, ask for comments and tell what comes next. Make it nice to look at: clear, not cluttered or congested. If offering a closed list, cover all possible options once. End with a nice thank you and a request for comments or suggestions.

Pre-Test by expert initial review, interview, interviews or cognitive testing around different versions of a complex question (think-aloud, paraphrasis of the question, explanation for why a choice or answer was made).

Then pilot with a debrief of respondents, observation while responding, permutation of questions, item response distribution analysis (proportion missing / don't know / valid), validation (not always possible). At least expert review and 20 pilot runs.

Are all the words understood, does the logic of order always work, are there questions that makes no sense for some people, do the questions mean the same to everyone? Give aids to memory if possible: eg lead-in questions, possible answers to choose, a life calendar leading up to the time. If given by interview give space for times for starting, finishing and any notes.

Electronic data capture (questionnaires straight into electronic format) have advantages in filling or calculating routine information, checks on format or consistency, easy skipping or contingency questioning, easy alteration or updating, complex identification and linkage, and the preservation of the data.

Disadvantages are in the up front work, need for networking in some implementations, desirability of the hardware, and some points that are only important if you don't have IT support.

Translation

-   Preliminary translation (I would put Simple Clarity or Translatable before that one)

-   Evaluation of the first translation (eg by blind back translation)

-   cross-language equivalence eg by giving perfectly bilingual people both versions (?!)

-   validity assessment

# Tidy Data {#tidy-data}

-   Each observation is a row

-   Each variable is a column

-   Each "kind" of variable is a table

-   A column allows linkage between tables

Also have a row with variable names, which are human readable, and save in one file per table. A fully **coherent data set** is the raw data, tidy data, a codebook and a recipe to go from one to the other. The **codebook** is the information about the variable which is not contained in the tidy dataset, including information about units, the choices you've made in selecting or presenting data and about the design of the research. Best as a text file with sections "codebook", "study design". The script or set of instructions to process raw -\> tidy data should not need parameters: having parameters introduces analyst degrees of freedom.

# Wrong conclusions and anatomy of an error {#error}

All models are wrong, but some are useful. Knowing their moving parts might alert us to where they will be importantly wrong, or wrong in a way that breaks their usefulness.

If the effect of an identical exposure is not the same for all individuals, it may be because it is true, or because of random error, measurement bias, confounders, selection bias, or differential treatment effect. In other words, maybe the exposure is not in fact identical at the mechanistic level or the measurement error or variability is high, or just doesn't measure the effect itself, or there are other causes of the outcome acting at the same time but not altering the effect, or because there are genuine differences in effect either across a single population or due to categorical differences within identifiable subgroups. See [risk magnification](#risk) for how, even with identical effect, the scale of measure may give the appearance of heterogenous treatment effect.

Measurement error: Validity and Reliability....! For continuous variables: Correlation coefficients. Mathematical coupling. Agreement versus correlation. For categorical: Classification f agreement. See also [Cronbach alpha] for a composite rating X as a sum of K items $Y_1 + Y_2 + Y_i \dots Y_K$

$\alpha = \frac{K}{K-1}\left(1-\frac{\sum^K_{i=1}\sigma^2_{Y_i}}{\sigma^2_X}\right)$

## Misclassification

Misclassification may be the original sin or due to some random error or bias in measurement which then results in misclassification. This has variable effects but mostly:

-   Differential misclassification may be decisively erroneous.

-   Non-differential exposure misclassification biases estimates to null

-   Non-differential outcome misclassification underestimates alter ratios by less, and bias differences to 0

-   Non-differential outcome overestimates bias ratios and differences to null

## Latent error

When an inference has been drawn, and is apparently correct or useful for the decision at hand, but later causes errors or even damage, then a latent error has been discovered. Racial bias in AI, edge case glitches, zero-day exploits an the like are latent errors.

## The play of chance

Also dealt with in the section [Error], this is the simplest and the one against which

## Effect modification

"The effect of an exposure varies by the level of a third factor". This is easy to explain generally but the construction of the question is important in interpreting the answer.

If an effect is different between stable identifiable categories of individual (those with, or without, arms would have different effects on examination success from a scribing app), that is a [differential treatment effect].

If the effect varies continuously across a continuous variable that is [heterogeneity of treatment effect]: this is classically by baseline risk of the outcome, so that the effect is absent at lower risk and more pronounced at higher risk, independently of the scale of outcome measurement. It's potentially been demonstrated by admission risk of death among ICU populations exposed to a harmful intervention but is generally quite rare.

Overall, where the effect itself varies by the magnitude of another factor, this is called interaction or effect modification. It's often described as a departure from a combined linear effect of two independent exposures, such that the combined effect is greater or lesser than would have been predicted by simply combining the isolated effects of the exposures. This definition allows a statistical test, such as the chi squared test for heterogeneity across ordered categories of exposure, or a test of the null hypothesis that the effect is additive in a general linear model.

## Definitions of confounding

Confounders in Classical Epidemiology are

1 associated with the exposure in the source population

2 associated with the outcome in the nonexposed

3 not a causal intermediate from exposure to outcome

Confounding is defined quite divergently. As a working definition, it is a nuisance which distorts the estimate of the independent effect of a certain exposure. The confounder will have not change the true magnitude of the effect being estimated, but it will alter the estimate by an independent and possibly unmeasured mechanism. It is identified as the unequal distribution of a further influential risk factor between those exposed and unexposed to the exposure of interest. Confounding is distinguished from bias because it depends on a measurable quantity. The quantity can then potentially be controlled for, such as by comparing strata with and without it, or including it in a model which explains the observed outcome. A variable might be a confounder, but not alter the estimate by very much. In that case it may be safer to ignore it rather than risk introducing bias by conditioning on it. The approach to this depends very much on the amount by which the effect estimate alters with the confounder, and the causal model for generating the data under which the analysis is carried out.

When things are not confounded they are conditionally independent (eg Y is independent of X, given knowledge of A, or $Y \perp \!\!\! \perp X|A$). Fortunately that conditional dependence structure can be deduced from causal graphs, in the next section. Unfortunately, there may be very many causal pathways that give the same conditional independence structures (these are called a Markov Equivalent Set of DAGs).

## Directed Acyclic graphs {#DAGs}

DAGs are basically all the causal model that most people will ever use. Although the information they contain can be written in any other format of causal model, such as structural equations and potential outcomes frameworks or structural causal models, they have the advantage of being understandable in more than one way, because they are pictures, yet analysable as text because they are a list of relationships. Part of the attraction is that they can be used in the do-calculus.

Acyclic means they don't contain loops. Thus the rules apply more clearly, without the loss of definite solutions induced by cyclic graphs. They are made of:

-   Nodes, Edges, Directed or not; Parents, Children, Ancestors, Descendants.

-   a front door path is all forward arrows eg AC or AEDF,

-   a backdoor path is one starting with a "block" or reversed arrow eg EACD

Markov Equivalence is when two structurally different DAGs have the same conditional independence structure. The entire Markov Equivalence Set can be generated algorithmically, and the silliest ones then rejectedfor greater confidence.

## The elemental confounds

The four elemental structures of confounding could be

-   The Fork, where X and Y share a common cause Z and so are correlated, that is they are not independent $Y \not\!\perp \!\!\! \perp X$. If stratified by Z though, we can say that in each stratum of Z or "conditional on Z", X and Y re independent $Y \perp \!\!\! \perp X | Z$ :

```{r confounding-fork}
coord_dag_fork <- list(
    y=c(x=0,z=0,y=0),
    x=c(x=1, z=2, y=3)
)
dag_fork <- ggdag::dagify(x ~ z,
              y ~ z,
              coords = coord_dag_fork)
ggdag::ggdag(dag_fork) + theme_void()
```

-   The Pipe, where again X and Y are no independent $Y \not\!\perp \!\!\! \perp X$, but X is a cause of Z which is in turn a cause of Y. Z can be said to mediate the effect of X on Y. For any value of Z, X and Y are independent $Y \perp \!\!\! \perp X | Z$. In this case though, everything that monad Y knows about monad X is already known by monad Z. Sometimes that direct effect is included in the scientific question, sometimes it's not. Conditioning on Z can radically alter the estimate:

```{r confounding-pipe}
coord_dag_pipe <- list(
    y=c(x=0,z=0,y=0),
    x=c(x=1, z=2, y=3)
)
dag_pipe <- ggdag::dagify(z ~ x,
              y ~ z,
              coords = coord_dag_pipe)
ggdag::ggdag(dag_pipe) + theme_void()
```

-   The Collider, where X and Y share no common cause, don't cause each other, but do share a result Z. Here, $Y \perp \!\!\! \perp X$, until stratifying by Z. $Y \not\!\perp \!\!\! \perp X |Z$, which is unlikely to be the quantity you're looking for p(Y\|do(X)). You can easily create a statistical phantom, a non-causal association, by conditioning on a collider:

```{r confounding-collider}
coord_dag_collider <- list(
    y=c(x=0,z=0,y=0),
    x=c(x=1, z=2, y=3)
)
dag_collider <- ggdag::dagify(z ~ x,
              z ~ y,
              coords = coord_dag_collider)
ggdag::ggdag(dag_collider) + theme_void()
```

-   The Descendant, may be a visible thing that is a descendant of an unobserved important thing. Its effect depends on which type of thing it's a descendant of but in each case it holds information about its ancestor. If its ancestor is a mediator in the pipe structure as in the picture below, then adjusting for the descendant acts a little as adjusting for the ancestor. So in this particular example $Y \not\!\perp \!\!\! \perp X$ because of the causal chain through Z. Just as conditioning for Z would remove truly causal information, A will remove the proportion of causal information that it holds about Z and might even completely remove the causal effect of Z so that X and Y appear artefactually independent $Y \perp \!\!\! \perp X|A$:

```{r confounding-descendant}
coord_dag_descendant <- list(
    y=c(x=1,z=1,y=1, a=-1),
    x=c(x=1, z=2, y=3, a=2)
)
dag_fork <- ggdag::dagify(z ~ x,
              y ~ z,
              a ~ z,
              coords = coord_dag_descendant)
ggdag::ggdag(dag_fork) + theme_void()
```

## Control of threats to inference:

Discarding information based on potential comparators having parameters that fall outside acceptable ranges: - Restriction is absolute and simple but reduces potential comparisons, hence inferences - Matching - by individual (search for control with Value = case Value +/- acceptable range) - by group aka frequency (distribution in case group matched by that in control group) - re-note Case Definitions: clear method, clear boundaries, defined source population

Or not discarding but instead randomising. - Simple random "needs a whole population list" although of course there are limits. - Stratified random, multiple stage random, single randomisation of the fixed point of a sample system, called "systematic random". Eg start at a random place in a pseudorandom ordered list and take regular steps through to sample.

# Bias

Bias is any systematic error in design or conduct of a study leading to incorrect conclusions. Any of the other influences in this section will count as a bias, if it is not detected. If it is detected and cannot be controlled for, or controlling for it introduces another influence, then it is also a bias[^9].

[^9]: This reading may be wrong, because very few commentators are willing to go outside the lines when describing fundamental concepts. A note from London divides it into descriptive studies, where bias arises if the sample is not representative of the population, and analytic studies, where bias arises if the groups to be compared are not comparable. I think they're talking about the parameter estimate, which in "descriptive" is an estimate of an unobserved population parameter and in "analytic" is an estimate of an unobserved generalisable parameter of the difference between groups.

Fortunately there are various types of bias that have been identified. In modern epidemiology all of these are in essence either a selection or an information bias (the latter also called a misclassification bias in some quarters).

## Selection bias

Selection bias is present when there is a systematic difference in the exposure or outcome between those who are observed and those who are not, or more broadly where one or both of these groups in the sample fail to reflect the target population. The probability of being observed is therefore not the same between the groups or strata to be considered, and so they are non-comparable. That is:

-   The sampling frame may not be representative of the target population

-   The groups may not be comparable (their sampling frames may differ)

Response bias is a type of selection bias, the odds ratio among those who respond being biased unpredictably by the ratio of probabilities of selecting those responders at random from a contingency table composed of the entire sampling frame. Knowing this, a non-response of 0.8 could bias the odds ratio by a factor of 0.8 to 1.4. $$OR_R = \frac{P_a P_d}{P_b P_c}OR$$ Other selection biases include:

-   Compliance bias, which is the basis for preferring Intention to Treat for most analyses of exogeneously allocated interventions[^10].

-   Surveillance bias, where there is greater intensity of observation on particular individuals who are then more likely to be identified as having further characteristics, irrespective of the mechanistic relationship of those characteristics to either exposure or outcome

-   Differential Follow-up has been treated as one of the very many [missing data] problems. This one particularly in situations where individuals either leave or are ignored for the duration in which they may develop outcomes or even further exposures, confounders, and so on[^11].

-   Healthy worker effect

-   The bias induced by conditioning on non-confounding variables, turning them into confounders, sits a little apart from all of these and has been called Collider Stratification Bias[^12].

[^10]: If those who (fully) comply are systematically different from those who do not in respect of predictors of outcome, the estimate of effect will be incorrect.

[^11]: This time, or I suppose hazard element subtly differentiates it from the bias in which these may exist but are not looked for in most individuals, which I've called surveillance bias.

[^12]: This can be conceptualised in a few different ways, such as opening up a backdoor path in the causal diagram where none existed before conditioning, or less formally as a selection bias. An example is taking emphysema patients as the controls for lung cancer cases in a study of the association between smoking and lung cancer.

## Selection-Distortion bias 

Also called Berkson's Paradox, this is bias induced by conditioning. An association is created by selection for at least one of two unrelated features. The selection induces the association precisely _because_ the features are unrelated until the moment of selection. 

Adding predictors to a linear regression induces this effect, called here collider bias (not like colliders in DAGs though, which is unhelpful). 

## Information bias

Information bias is present when there is a difference in the accuracy of information collected for exposure or outcome.

-   Reporting bias is due to subjects reporting either exposure or outcome with an accuracy which depends on their status in the other[^13].
    -   Self report bias is universal, and the only question is how to interpret across studies.
    -   Recall bias occurs when the time distance or type of information determines the accuracy of its reporting[^14].
    -   Interviewer bias is introduced when an interviewer guides the interviewee in a way that minimises the possibility of certain responses and rewards others.
-   Observer bias is due to measurements giving different answers on one depending on the status of the other[^15]. Observer biases include:
    -   Recording
    -   Measurement
    -   Classification
    -   Ascertainment
-   Reporting bias is a serious effect but outside of the individual study, as where the public perception is influenced by selective reporting of certain results and the suppression of others. Systematic reviews may display this.

[^13]: For example reporting exposure differently if they have the outcome, or reporting outcomes differently if they were exposed. Greater awareness of the research question, or reasons to hide or lie such as desirability or acceptability of particular answers, desire to please or frustrate and so on, are similar in effect and also can be called reporting bias.

[^14]: Such as when recent or unusual information is more likely to be recalled or at least reported.

[^15]: For example, the outcome is reported with greater deviation or variation in the exposed, or vice versa

## To avoid bias

-   Blinding[^16]

-   Objective measures

-   Randomisation

-   Complete follow up

-   Timely measurements

-   Calibration under the desired circumstances ( *eg* values in true controls by *eg* questionnaire or BP cuff calibration)

[^16]: Why does this work, exactly? It eliminates some forms of observer bias, and it reduces several forms of selection bias. It only reduces those that need for their effect some sort of knowledge of exposure or outcome.

(Cochrane list)

# Causal reasoning and the do-calculus {#causality}

DAGs are a subclass of the entity "causal model". Structural Causal Models, for example, encode the information as a Tuple M = (U, V, F, P(U)) where U is endogenous variables and F a function describing the distribution of the endogenous variables U, V is exogenous variables, and P(U) a probability distribution over the domain of U. Then F is replaced by $F_x$ when modelling the result of intervention.

Backdoor paths using DAGs were dealt with [above](#DAGs).

## Bradford Hill's viewpoints

Having had these taught as criteria, it's reassuring to hear them as simply things to consider in a pondering, while-running sort of way. ABH himself called them viewpoints and there it stood until DAGs. The list is:

-   "An active agent or a static condition" Temporality
-   Strength (size)
-   Consistency (across types of information)
-   Dose:Response (Biological gradient)
-   Specificity
-   Biological plausibility
-   Coherence (sort of biological plausibility but probably intended to mean that borrowing from other situations supports the idea)
-   "Experiment" (protection by removal, in the examples)
-   Analogy (the example is HepB / HIV)
-   Predictive performance (no alternative theories after rigorous testing)

X and Y are associated in the population if X causes Y, Y causes X, or both X and Y are caused by another variable, call it Z. In this case X and Y are not associated in strata defined by Z X and Y may not be associated in the population, but are associated within strata of Z, if Z is caused by both X and Y Z-conditioning can therefore create or remove X:Y associations depending on the causal structure.

## Axioms of the do-Calculus {#do-calculus}

A graphical algorithm for back door adjustment is a good entry to this: back door because all causation goes through the front door, in the direction of the causal arrows from the exposure of interest. The back door is all those paths from exposure to outcome which start by going backwards up an arrow that points to the exposure.

Remove all front door arrows from the exposure, and if there are unblocked paths there is confounding. Draw dotted lines connecting any nodes that share a child that is either in S or descends to S. A set S of candidate confounders will be sufficient to control confounding if

-   none is a descendant of the exposure

-   there is no open path from the exposure to the outcome that avoids a member of S,

-   including the new, dotted lines that denote 1st-gen sharing of S ancestry S can induce open paths by causing the dotted lines: this demonstrates the creation of conditional association by stratum creation.

The do-Calculus simulates intervention by deleting certain functions from the model, replacing random parameters X with a constant x. This allows $P(Y|do(X))$ to be estimated in terms of the joint probability of the observed distribution. There are three rules to this do-Calculus, derived from rules of probability and straightforward reading of the DAG:

1.  We can ignore any node that doesn't sit on a path from exposure to outcome. This is called a node that's d-separated. The graph removing these paths is called $G_{\bar x}$.

$$P(y|z, do(x). w) = P(y|do(x),w) \textrm{   if   }(Y \perp Z | W, X)G_{\bar X} $$

2.  If a set of variables Z blocks all back door paths from exposure X to outcome Y, then simply observing them is sufficient to estimate their causal effect. The observations can be treated as intervention, $P(Y | do(X), Z) = P(Y| X , Z)$. More generally, in the DAG below with a cause X and an observed control variable U, the probability of Y consequent to intervening on X is the distribution of Y, stratified by X and by the control variable, averaged over the distributin of the control variables $P(Y| do(X)) = \sum_U P(Y|X,U) P(U) = E_U P(Y|X,U)$. Marginalising (or averaging) over the control variables changes the causal effect of X on Y and is important, except perhaps in the simplest clearest models. So, again, we don't use the coefficient relating X to Y after controlling for U, but samples and a description from the entire posterior predictive distribution.

```{r confounding-do-calc}
coord_dag_fork_do <- list(
    y=c(x=0,u=1,y=0),
    x=c(x=1, u=2, y=3)
)
dag_fork_do <- ggdag::dagify(
    x ~ u,
    y ~ u,
    y ~ x,
              coords = coord_dag_fork_do)
ggdag::ggdag(dag_fork_do) + theme_void()
```

3.  If there are no causal paths from X to Y through Z, then $P(Y|do(X)) = P(Y)$. This can also be stated as ignoring the variables Z if $(Y \perp Z|W,X)$ in the graph with arrows removed that go into X and into all W that are descendant of Z, called: $G_{\bar X \bar{W(Z)}}$.

4.  Use [<https://daggity.net>] to check it! Use \@(ref:greenland_causal_1999)

# Reliability {#reliability}

Reliability is almost the same as Repeatability. Reliability is sort of the long run and idealised version. If there is then both intra and interobserver reliability of a comparable population, then the result is repeatable.

Variability is an informal sort of word describing the difference between repetitions. "Sources of variation" is a slightly more formal concept, and if the measure that varies is a continuous measure then it is partitioned into systematic and random error. Variance is a formally defined measure of variability, as the mean squared deviation from the mean value.

Partitioning can be assigned for example by ANOVA; the ratio of systematic to random error is the Intraclass Correlation Coefficient, *aka* reliability coefficient. The portion of variability that is due to a systematic component can be described by some correlation coefficient such as Pearson's method.

Pearson's correlation of product-moment is defined for a population $$\rho (X, Y) = \frac{cov (X, Y)}{\sigma_{X}.\sigma_{Y}}$$

In turn, the covariance of X with Y is the expectation of the the product of deviation of each from its mean $cov (X, Y) = E[(X-\mu_{X})(Y-\mu_{Y})]$. This allows the correlation coefficient to be defined purely in terms of the expected values of each parameter using the identities that define variance $\sigma_{X}^2 = E[(X- E[\bar{X}]^2)] = E[X] - E[X^2]$ and the expectation of each parameter independently $E[(X-\mu_{X}).(Y-\mu_{Y})] = E[(X-E[X]).(Y-E[Y])]$, so that

$$\rho (X, Y) = \frac{E[XY]-E[X]E[Y]}{\sqrt{E[X^2] - E[X]^2} \sqrt{E[Y^2]-E[Y]^2}}$$

The sample correlation r is then of the same shape $$r = r_{xy} = \frac{\sum_{i=1}^n (x_{i} - \bar {x})(y_{i} - \bar {y})}{\sqrt{\sum_{i=1}^n (x_{i} - \bar {x})^2} \sqrt{\sum_{i=1}^n (y_{i} - \bar {y})^2}}$$

That sample correlation coefficient for two vectors can also be quoted as the mean of the standard scores of x and y, that is their deviation from their mean, divided by their root variance: $$r = r_{xy} = \frac{1}{n-1} \sum_{i=1}^n (\frac{x_{i} - \bar {x}}{s_{x}})(\frac{y_{i} - \bar {y}}{s_{y}})$$

Correlation between categorical variables starts by tabulation. A simple measure is mean pair agreement or Observed Agreement ($OA = \frac {a+d}{a+b+c+d}$). Having defined the observed agreement, the maximum chance agreement is defined as $CA = \frac {E[a]+E[d]}{a+b+c+d}$, using the expectation of each value and the assumption of consistency across the table $E[a] = \frac{\textrm{Row total } \times \textrm{ Column total}}{\textrm{Grand total}}$. As in the chi squared statistic, the Kappa statistic for excess agreement over max chance agreement is then $$\frac{OA-CA}{1-CA}$$

Regression is by $y = \beta_0 + \beta_1 x + \epsilon$ where epsilon is normal with SD estimated by s, below

Beta 1 from $\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar X)(y_i - \bar Y)}{\sum_{i=1}^n (x_i - \bar X)^2}$, then $\hat \beta_0 = \bar Y - \hat \beta_1 \bar X$ or $\hat \beta_1 = Cor (Y, X) \frac{SD_y}{SD_x}$

Confidence intervals for the slope are $\beta_1 \pm t_{2 \times 0.975, \nu} \times s.e.$, with t statistics used to test H0: coefficient = 0 Standard error differs for prediction of a line at x0 and for the probability of a new point at x0 $s.e._{line \beta_0} = \hat \sigma \sqrt{\frac{1}{n} + \frac {(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}$, $s.e._{point \beta_0} = \hat \sigma \sqrt{1 + {\frac{1}{n} + \frac {(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}}$

, $s.e._{\beta_1} = s{\frac{1}{\sqrt{\sum(x - \bar x)^2}}}$ $s = \sqrt{\frac{\sum(y - \bar y)^2 - \beta_1^2 \sum(x - \bar x)^2}{n – 2}} \textrm{, df = } n - n_{coefficients}$

So if the null hypothesis is that exposure is not predictive of outcome, and exposure of interest is at the head of columns, then the expected frequency of outcomes is (row total/table total) for the proportion of entries that exist in the row, times (column total) for the available number of exposed observations.

```{=tex}
\begin{tabular}{| c |c | c | c| }
\hline
& + & - & \\ \hline
D & a & b & $N_{D}$ \\ \hline
$D^c$ & c & d & $N_{D^c}$ \\ \hline
& $N_{+}$ &  $N_{-}$  & N \\ \hline
\end{tabular}
```
The distribution of the squared differences in observed and expected values generates a chi squared distribution scaled for the expected values, with degrees of freedom $\nu = (rows-1) \times (columns-1)$. That is, the chi squared statistic is given by $\sum {\frac{(O-E)^{2}}{E}}$, H0 = "the distribution of observations among the categories of one variable is independent of their distribution among the categories of the other" or "there is no association between row and column variables"

To repeat: $$E(a) = \frac{N_D N_+}{N}$$ $$Var (a) = \frac{N_D N_+ N_{D^c} N_-}{N^2(N-1)}$$

which is only from Smith et al *"Field Trials"* but the same shape as in the C-M-H test for trend below

Chi squared is distributed as $\frac{1}{{2^\frac{k}{2} \Gamma(\frac{k}{2})}}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}$, but that's not important right now.

In the special case of a 2x2 table the chi squared statistic is $\chi^2_{Yates} = \frac{N.(|ad-bc|-\frac{N}{2})^2}{(a+b).(c+d).(a+c).(b+d)}$

including Yates's continuity correction of O-E-0.5 for each cell because of a tail of the underlying binomial continuous distribution being better estimated by a point midway between that cell count and one fewer. This sums to N/2 for a 2x2 table with N=20-40 and no value \<5. More generally chi squared can be used in a 2xC table if no value \<1 and 80% values are \>=5

Otherwise Fisher's Exact: $$p_{table} = \frac{N_+ ! N_{+^c}! N_D! N_{D^c}!}{N! a! b! c! d!}$$ and either sum p_table across all the possible tables with lower probabilities or double the sum of probabilities of lower p_table (either acceptable, both differ!)

and for the test of a "trend" across scored levels of a variable which differs between groups Xi, hence having one degree of freedom as it's comparing two means-of-scores Xbar1 and Xbar2 is a t test squared, which is also the form of the Pearson Chi Squared: $$\chi^2_\rho = \frac{(X _{1} - X _{2})^2}
{s^{2}.(\frac{1}{n_{1}} + \frac{1}{n_{2}})}$$

or if assessing against H0: increase in log(OR) per ration of x==0, which is a trend in ordinally scored variables where xi is score of a group i, di is those with an outcome in that group; ni is the total in the exposure group; O and N are the totals with the outcome and grand totals.

$U = \sum d_i x_i - \frac{O}{N} \sum n_i x_i$ $V = \frac{O(N-O)}{N^2(N-1)} N \sum (n_i x_i^2) - (\sum n_i x_i)^2$ $S^2 = \frac{V}{QR}$ and error factor $EF = e^{1.96 S}$ $\chi^2_{trend}=\frac{U^2}{V} \textrm{, df=1}$ Increase in log odds for an increase in x is U/V; the variance of this estimate is $\sqrt{1/V}$. The difference between the standard Chi squared test for this table and the Chi squared test for trend is the Chi squared test for departure from linear trend with df = (number of exposure groups -- 2). Also $V = \sum V_i$ where $V_i = \frac{D_i H_i N_{0i} N_{1i}}{N_i^2 (N_1 -1)}$ which is called the Mantel-Haenszel Chi Squared in SM04 of EPM202.

# Regression {#glms}

Linear regression is almost the canonical version of this, with Logistic, Cox, Poisson and even Mantel_Haenszel methods being special or extended cases.

A regression line is the line to which values "regress" in repeated observation of the parameter values. The remaining variation in those sampled values is smaller than the variation in the whole dataset, because some of it is already modelled by the line. Errors are unobservable true errors from known coefficients, whereas residuals are the observable errors from the estimated coefficient: so in a sense residuals are estimates of the error.

Regression seeks to minimise the residuals which because they add to 0, are squared:

For $Y_i = \beta_0 + \beta_1 X_i$ the sum of squares = $\sum_{i=1}^n (Y_i - \beta_0 + \beta_1 X_i)^2$ is minimised when $\beta_1 = 0 , \hat \beta_0 = \bar Y$ and when $\beta_0 = 0 , \hat \beta_1 = \frac{\sum_{i=1}^n X_i Y_i}{\sum_{i=1}^n X_i^2}$

the estimator that minimises the sum of squares for a regression line is $\beta_1 = \frac{\sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X)}{\sum_{i=1}^n (X_i - \bar X)^2}$ , or $Cor(X,Y) \frac{sd(Y)}{sd(X)}$

Model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ where $\epsilon_i \sim N(0, \sigma^2)$. Observed outcome i is Yi at predictor Xi; Predicted outcome at predictor Xi is $\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i$ and residual is $e_i = Y_i - \hat Y_i$ Least squares minimises $\sum_{i=1}^n e_i^2$, ei being estimates of $\epsilon_i$ .

The errors are the product of all the unmodelled confounders or modifiers acting outside the model, on the observed data, so $E(\epsilon_i ) = 0$ ; if an intercept is included then their sum=0 and if a linear regressor is included then $\sum_{i=1}^n e_i X_i = 0$ so the residuals are very useful for investigating poor model fit such as by residual plots.

ML estimate of the variance $\sigma^2$ around the line is $\frac{1}{n} \sum_{i=1}^n e_i^2$, and $\hat \sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2$ is an unbiased estimate for the population sigma based on the independent residuals. The numerator for total variability in response variable is $\sum_{i=1}^n (Y_i - \bar Y)^2$ and is made up of the variability explained by the line and the residual variability. $\sum_{i=1}^n (Y_i - \bar Y)^2 = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n (\hat Y_i - \bar Y)^2$ and

the proportion of the total variability explained by the regression relationship is R squared:

$$R^2 = \frac {\sum_{i=1}^n (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}$$

Which is literally the sample correlation squared. Deleting data or adding terms to a regression model always increase R-squared.

Variance around the regression line is usually replaced by its estimate. $\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar X)^2}$

## Shrinkage

When assigning individuals a common distribution, and fitting that distribution to the sample, then each individual informs the value of each other. This is important in father-son height prediction and I think in the partial model shrinkage of an integer-substituted regression model.

## Mantel-Haenszel

Mantel-Haenszel in general uses an association measure in stratum i weighted for total exposure in the stratum. So for a cohort describing relative risk:

$RR_{MH} = \frac{\sum{\frac{d_{1i} \times T_{0i}}{T_i}}}{\sum{\frac{d_{0i} \times T_{1i}}{T_i}}} = \frac{Q}{R}$ or if $w_i = \frac{d_{0i}T_{1i}}{T_i}$ then $RR_{MH} = \frac{\sum(w_i \times RR_i)}{\sum w_i}$

$se_{MH} = \sqrt{\frac{V}{QR}}$ where $V = \sum V_i$ and $V_i = \frac{d_i T_{1i} T_{0i}}{T_i^2}$ and NHST is the MH chi squared

$\chi^2_{MH} = \frac{(\sum d_{1i} - \sum E_{1i})^2}{\sum V_i}$ or $\frac{(O-E)^2}{V}$, df = 1

where Exi is overall stratumi rate times observation time for exposure x, $E_{1i} = \frac{d_i \times T_{1i}}{T_i}$

To detect effect modification the Chi Square test for heterogeneity $H_0: RR_i = RR_{MH}$

$$
\chi^2_{het} = \sum \frac{(d_{1i} T_{0i} - RR_{MH} d_{01} T_{1i})^2}{RR_{MH} V_i T_i^2} \textrm{,  df = c-1} 
$$

Cohort E(D1) = Y1 \*(D/T) and U = D1 -- E(D1)

Var(U) = $D \frac{t_1}{t}(1 - \frac{t_1}{t})$ and test statistic $z= \frac{U}{SE(U)} = \frac{U}{\sqrt{Var(U)}} or Z^2 = \frac{U^2}{V} \textrm{, Chi square df=1}$

## Logistic regression

The quantity estimated from the model is the log(Odds), from the log(OR)(="coefficient") and log(Other Odds). SE for log odds is as usual $\sqrt{\frac{1}{x_1}+\dots}$ so SE for odds is $e^{\textrm{SE(log odds)}}$ log(odds) = a + b1A1 + b2A2 ... Hypothesis testing by Wald tests (with SE for the null hypothesis, so the SE chosen depends on th null hypothesis chosen) H0: log(Odds) = 0 for each level of a potential variable for the model, $z=\frac{\textrm{coefficient}}{\textrm{SE}}$ Or LRS, H0: the likelihood is not significantly increased by addition of the variable Adjustment for other variables uses a proportional odds assumption. Because it's built from a square table the absolute numbers of cases and non cases determine the baseline odds (="constant"): therefore the sample fraction ratio is added to the model. log(odds) = log(constant) + log(sample fraction ratio)+ log(exposure) sample fraction ratio = proportion of cases sampled / proportion of non cases sampled log(constant) + log(sample fraction ratio) = constant\*

Logistic regression for trend across levels of a variable: doesn't have to assume proportional odds (although this simplifies and gives more precision) this is parameter reduction eg from 4 parameters for different levels to 1 this is critically altered by the choice of grouping for levels of exposure eg choose the midpoint of arbitrary age bins to Hypothesis tests for trend are "test for linear trend": model is better with linear effect than without, then "test for departure from trend": for improved prediction with many versus one parameter both use LRS with df = difference in number of parameters.

Interaction in the model is referred to as eg param1.param2 and is a single new parameter tests for interaction are against H0: the interaction is not greater than chance, using LRS.

Interaction is not detected automatically and must be added to a model then tested, so there is no way to guarantee that all meaningful interactions are detected or that interactions detected are meaningful beyond the dataset.

The assumptions of a linear model imply that the residuals are normally distributed, with a constant variance across x and y. Plots of standardised residuals (rescaled to mean = 0, sd = 1) help: density of Std Res for normality, Std Res versus corresponding fitted value for deviation from linear.

## The MH odds ratio across strata

is the weighted mean sum of odds ratios across strata generally $OR_{MH} = \frac{\sum(w_i OR_i)}{w_i}$. The weights $w_i = \frac{d_{0i} h_{1i}}{n_i}$ are the fraction of "diseased unexposed times healthy exposed" in the grand total sample; equivalent to the denominator of OR divided by the total. In the same way $OR_{MH} = Q/R$ where $Q = \sum \frac{d_{1i} h_{0i}}{n_i}$ and $R =\sum \frac{d_{0i} h_{1i}}{n_i}$ then se(log(ORMH)) = $\sqrt{\frac{V}{QR}}$, V = $\sum V_i = \sum \frac{d_i h_i n_{0i} n_{1i}}{n_i^2 (n_i - 1)}$, V calculated from the marginal totals because each stratum has equal variance Chi squared is $\chi^2_{MH} = \frac{(\sum d_{1i} - \sum E_{1i})^2}{\sum V_i} = \frac{(O-E)^2}{V} = \frac{U^2}{V} \textrm{, df=1}$,

$E_{1i}= \frac{d_i n_{1i}}{n_iO} = \sum d_{1i}, \> E = \sum E_{1i} \textrm{, U = O-E}$ MH chi squared is (n-1)/n times the size of the non MH chi squared in the simple 2x2 table.

Rule of 5 to check validity: if all the expected values in the table are greater than 5, Chi squared is appropriate. Otherwise use an exact test such as Fisher's or an exact goodness of fit test. For heterogeneity tests H0: for all i, ORi = ORMH , so $$\chi^2_{het} = \sum \frac{(d_{1i} h_{0i} - OR_{MH} d_{0i} h_{1i})^2}{OR_{MH} V_i n^2_i}$$

## Multivariable regression

Extending the line to a plane, or later to an n-dimensional plane. Linear in the coefficients. Least squares used again, fixing at zero (or removing ) coefficients and calculating regression through the new origin for the remainder, repeatedly. For a model with sum of squares $\sum (Y_i - X_{1i} \beta_1 - X_{2i} \beta_2)^2$ to estimate B2 define Y centred, $\tilde Y_i = Y_i - X_{1i} \beta_1$ then sum of squares = $\sum (\tilde Y_i - X_{2i} \beta_2)^2$ and so $\beta_2 = \frac{\sum \tilde Y_i X_{2i}}{\sum X_{2i}^2}$, and so on for other coefficients. and if $e_{i, a | b} = a_i - \frac{\sum_{j=1}^n a_j b_j }{\sum_{i=1}^n b_j^2} b_i$ So we get the minimiser $\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2} X_1}$ or $\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e^2_{i, X_1 | X_2} }$

A multivariable regression coefficient is hence the expected change in response for a 1 unit change in the associated predictor, holding all the other values constant (at zero, say, as if regressing through the origin for each predictor).

The model is then $Y_i = \sum_{k=1}^p x_{ik} \beta_k + \epsilon_i$ with fitted responses $\hat Y_i = \sum_{k=1}^p x_{ik} \hat \beta_k$ residuals are $e_i = Y_i - \hat Y_i$ and variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i^2$ coefficients have standard errors $\hat \sigma_{\hat \beta_k}$ and H0:coeff=0 tested by the t-test $\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$ 

## Multicollinearity 

This is caused by conditional association; don't use individual association or correlation to diagnose it. The only importance is in conditional association. The overall problem is non-identifiability. 

## Regression diagnostics

**Outliers** can be real or spurious, conform to the regression line or not. Outlier detection is partly started from the tablesor scatterplots. Supervised detection is pretty much by eyeball, noting that unidimensional outliers are the obvious ones in a box and whisker, but might need scatterplots of the 2D raw data, or a reduction of higher dimensional data such as principal components in order to detect outliers that are within one or both of the orthogonal dimensions, but in combination identify outlier observations.

Algorithms can be useful to start this: - z scores $z = \frac{x_i - \bar x}{s}$ Tukey's method allocates possible for $1.5<z\leq3$, probable for $z>3$ - "modified z score" \>3.5, $M_i = \frac{0.6754(x-i - \bar x)}{MAD}$ where MAD is median of the absolute difference $|x_i - \bar x |$ - similar principle with Student distribution - similar with log normal residuals - Cook distance, roughly in this case the effect of deleting any given observation - Mahalanobis distance with the critical values determined using Bonferroni bounds

Leverage is distance along the regression, influence is orthogonal distance from the line. Influence measure diagnostics amount to leaving out a point and recalculating and all are a bit "in context". Poor model fit includes heteroskedasticity, missing model terms, temporal patterns (find out by residuals versus collection order). Residual Q-Q plots investigate normality.

Adding spurious variables inflates the variance: as the true sigma is unknown the variance inflation factor of a predictor is the inflation of variance due to correlation of it with other variables, over the ideal situation in which it's orthogonal to all the other variables. Missing helpful variables out introduces bias in the estimate.

## ANOVA {#ANOVA}

Related to linear regression. A table of n observations y, in k groups The sum of squares of deviations from the mean, $\sum (y - \bar y)^2$, is partitioned into between group $\sum n_i (\bar y_i - \bar y)^2$ with *k*-1 degrees of freedom within group $\sum (n_i - 1) s_i^2$ with *n-k* degrees of freedom for a line the total variability is from the 0-gradient line (null) at the mean of y, the y-distance from the null line to the regression line at each xi = SSregression, the y-distance from regression to point at xi = SSresidual and each SS is divided by its degrees of freedom giving the mean square, $MS = \frac{SS}{df}$ For a line, the residuals are $\sum (y - \hat{y})^2$ and the model SS is of deviations from the mean.

The F statistic is $\frac {\textrm{Between Group MS}} {\textrm{Within Group MS}} , df = k-1, n-k$ or $\frac{regression}{residual}, df= 1, n-2$ for a line. For the F test, under the null hypothesis the ratio of two variances follows the F distribution and for a line, F = t^2^ . This can also be used for regression diagnostics where $F = \frac{\textrm{Model MS}}{\textrm {Residual MS}}$; A partial F test excludes the contribution of one covariate after adjustment for others, as a LRT equivalent.

The $R^2 = \frac{\textrm{Regression SS}}{\textrm{Total SS}}$, giving a proportion of variance explained by the regression or grouping.

As $Var(X) = \frac{1}{n -1} \sum_{i=1}^n (X_i - \bar X)^2$, so $Cov(X, Y) = \frac{1}{n -1} \sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i Y_i - n \bar X \bar Y)$ and $Cor (X, Y) = \frac{Cov(X, Y)}{S_x S_y}$

Mantel-Haenszel estimate of the common Odds Ratio is weighted for the non-supporting information as a proportion of stratum size $\frac{\sum ({\frac{a_i d_i} {n_i}})}{\sum ({\frac{b_i c_i} {n_i}})}$, that is $\frac{(a_{1}.d_{1} / N_{1}) + ... (a_{j}.d_{j} / N_{j})}{(b_{1}.c_{1} / N_{1}) + ... (b_{j} c_{j} / N_{j}})$ or if $w_i = \frac{b_i c_i}{n_i}$ then $OR_{MH} = \frac{\sum w_i OR_i}{\sum w_i}$

and there's a corresponding Cochrane-Mantel-Haenszel Chi^2^ test of the same shape as the simple single table single stratum situation, $\chi^2_{MH} = \frac{[\sum a_{i} - \sum E (a_{i})]^2}{\sum Var[a_{i}]}$, equivalent to $\chi^2_{MH} = \frac {(|\sum{\frac{a-(a+b).(a+c)}{n}}|-0.5)^2} {\frac{\sum(a+b).(a+c).(b+d).(c+d)}{(n^3-n^2)}}$

$\chi^2_{trend} = \frac{[\sum^k_{i = 1}r_i v_i – R \mu]^2}{p(1-p)[\sum^k_{i = 1} n_iv_i^2 – N \mu^2]}$

$mu = \sum^k_{i = 1} \frac{n_i v_i}{N}$

where each of k groups of observations are denoted as ri successes out of ni total with score vi assigned. R is the sum of all ri, N is the sum of all ni and p = R/N.

## Conditional Logistic Regression

Conditional on the membership of some stratum, such as a matched pair, the effect of a covariate is estimated. The effect of the matching variable is not estimated (because the procedure would be different) nor is the constant term (because that would need the effect of the matching variable to be included in it). Instead of logit(p) = log(odds) = beta + stratum + constant, we have log(OR) = beta.

Confounding is assessed by excluding a covariate from the model to create nested models, which are then tested by the likelihood ratio test; interaction by stratifying and using LRT with or without strata; linear trend by a score test or by adding a linear predictor and LRT.

Variables can, confusingly, be fixed or changing; if changing they can have a deterministic or stochastic relationship with the determinant of that change, such as time. Rates that vary systematically over time are accounted by splitting into epochs and calculating rates conditional on the occurence of an event, ignoring non event containing epochs (Lexis expansion or "episode splitting"). This looks like a sort of limit case for Cox regression, allowing estimates of the rate, not only rate ratio, during each epoch as long as there are sufficient events.

# Generalised Additive models {#gams}

## Intro to GAMs with hand waving

GAMs describe the relationship between several predictors, and a response. As for linear models, they estimate the response based on levels of all the predictors.

One way to understand the point of all this is to start from scatterplot smoothing. Linear models $y = f(x) + \epsilon$ estimate the linear function that best minimises errors in predicting the response, for example by least squares.

It seems that a GAM extends linear models. In the base linear model described so far, $\sum \beta_j X_j$ simply multiplies each $X_j$, or a defined transformation of it, by a covariate which is already chosen. In a GAM, there is a similar underlying shape but the covariate is replaced by a function *s*, so that the form is very subtly different $\sum s_j(X_j)$. Instead of simply multiplying X by a constant, requiring the relationship of X to Y to be linear, the functions *s* can be any sort of function. For example, it may have polynomial transformations of the predictor, such as a $1, x, x^2$ sort of a Taylor expansion, or use a Fourier transformation with the *sine* and *cosine* of the predictor, or a wavelet expansion or a spline. If we think of these as bases, then we can cook up a fairly simple looking additive model that is analogous to a linear model. McElreath says polynomials are awful, splines and GAMs and Gaussian processes are less awful, absent some more mechanistic model.

"Polynomials have strange symmetries and explosive uncertainty at the edges".

GAMs aim only for a smooth function, which can be non parametric and so can be very general indeed, which is where the motivating example of a scatterplot smoother is useful. A spline might be something like "for all x less than the knot point v, f(x) = 0, for all x greater than or equal to v, f(x) = x-v", or a version which has more useful mathematical properties by using $1, x, x^2$ terms and avoids zero derivatives at the knots, and so on.

Then the functions can be estimated differently for exposed versus unexposed, assuming nothing about the type of effect that the exposure has. It could be u shaped, hill shaped, and so on, and there could be unlimited variety of modelled relationships such as several non linear and several linear predictors. Then $y = f(x) + Z\beta +\epsilon$ in the context of least squares analysis, or $g(E[y]) = f(x) + Z\beta$ for the generalised linear models.

So far so simple, just extending the idea of linear models. This has been extended even further, treating the parameters or even the bases as random effects with a distribution, say a normal distribution. This means that analytic choices matter much less (number of knots and so on), and mixed models become possible (allowing restricted maximum likelihood to be used to determine the local prediction rather than having to choose how smooth to make the model as a set of choices).

## Estimate smoothers

See the section on [smoothing](#loess) for more detail on how things are smoothed. Smoothing options are mainly down to local scoring (loess), smoothing splines or regression splines (such as B-spline, P-spline, or thin plate spline). Regression splines can be written as linear combinations of basis functions, which don't depend on the response variable.

Polynomials, such as $\mu_i = \alpha + \beta_1 x_i + \beta_2 x_i^2$ for a parabola, can fit all kinds of shapes and are technically linear because they are an additive function of the parameters, but they have "strange and often undesirable symmetries and have explosive uncertainty at the edges". They have no local smoothing, only global smoothing (so that any data point on any part of the x axis can change the curve arbitrarily far from itself).

Remember GAMs involve estimating the smoothers for each predictor simultaneously, along with any covariance between the smoothers.

The local scoring algorithm is a extension of the backfitting algorithm, which is an extension of the Gauss-Seidel procedure, to solve any messy linear system. First set each function to zero, then use it to estimate each observation, then construct a "pseudo-dependent variable" and smooth that variable using weights which are roughly the variance of each observation, iterating to convergence. This is computationally expensive because the weights change with each iteration.

Otherwise cast the GAM as a large GLM made of linear combinations of basis functions and corresponding coefficients.

The original invention of GAMs was with a local scoring algorithm to estimate them. The modern approach seems to be to solve as a large GLM with random effects, similar to the description of using PIRLS, penalised iterative least squares.

## Penalised likelihood

The object of the GAM is to maximise the penalised likelihood function. In general, this is $2l (\alpha, s_1(x_1), \dots , s_p(x_p)) - \mathrm{penalty}$ where *l* is the standard log likelihood function. So then for a GAM modelling binary outcomes, with logistic link function, the likelihood is $\sum_{i=1}^n y_i\textrm{ log } \hat p_i + (1-y_i) \textrm{ log} (1- \hat p_i)$. In this case as well, $\hat p_i = P(Y =1|x_1, \dots, x_p) = \bigg(1 + e^{-\hat\alpha - \sum_{j=1}^p s_j(x_{ij})} \bigg)^{-1}$.

And the penalty can be $\lambda \int(s''(x))^2 dx$ as in the [section on smoothing](#loess).

## Choose model smoothing parameter

There are various ways of deciding how much smoothing is required (what value for lambda, how many knots, and so on). General cross validation is one, or restricted maximum likelihood (REML) as the modern, parameters-as-distributions approach.

# Bayesian Models

This is based around Richard McElreath's Statistical Rethinking, which has the best setup for an open public course. Just hands down. As with all models, causal thinking is important to justify the shape of the model using information outside of the data. The causes of the data looking as it does, any assumptions about why future data might not look as the current data does, and how to estimate, predict or explain it, all have to come from outside of the data using generative models, and it's best if these generative models are causal in nature.

## Variables

Variables are things that are independent or estimated in the model. They can be parameters, and they can also be data. Parameters can be represented by a distribution, and distributions can be represented by a distribution. Mind. Blown. 

Variables can also be relevant only as things to modify those parameters: indicator or index variables, for example. 

## Categorical variables

This is a subtle and probably advanced realisation and so appears later in Rethinking, but conceptually it belongs here: there are implications in choosing how to specify a model, that aren't always explicit when using pre-made routines. Learning Bayesian analysis requires rethinking all of these routines and so it helps a little to find the way into the models. For example, categorical variables are often specified in models as a single parameter, with a 0 or 1 [indicator variable](#indicator-variable) so that the parameter value is zero, contributing nothing at all, for observations in the "baseline" category. You can extend this to many categories by using k-1 parameters each with a 1/0 indicator variable for k categories, when the uncertainty becomes more similar for each with respect to k-1 others s k increases.

But look: that not only implies normalness of the zero-holding category, but implies greater certainty about that category, as a whole degree of variability has been removed from it. If instead you model it as an [index variable](#index-variable), where the variable is used only to hold the place for different values of a parameter in that place, then you model the same degree of uncertainty for each parameter. Honesty and ontological relevance at the cost of false precision from fewer parameters to be estimated. You can extend this to many categories by using more parameters as before. The only other attraction is that multilevel models depend on this approach.

## Common transformations

These are best done as pre-processing, before iterating through many instances of the model, to save redoing the processing every time.

Standardisation such as by $\frac{variable-mean}{SD}$ is often useful for polynomial or various other models where squares of large numbers become unbalanced and can even challenge the maths of a computer.

Log transformation stops numbers from being smaller than the floating point arithmetic can handle, and exponentiation then retrieves the original units anyway.

## Parameters

The parameters are not independent of each other except under very rare conditions, and so the parameters are not often independently interpretable. Instead of interpreting parameters, instead push out posterior predictions. A good entry to Bayesian estimation is just by re-looking at linear regression. In this paradigm, each of the parameters is modelled by a distribution: there's no one correct value that is being estimated, but a range of values with their associated plausibility.

The generative model might be that weight is determined by height and some unknown things, or $W = f(H, U)$. A Bayesian statistical model can then accept that every parameter of that model is itself uncertain, and can be modelled by a distribution. This powerful because the distributions can be updated, using a prior and data, and producing posterior predictions. The statistical model might be that $W \sim \mathcal{N}(\mu_i, \sigma)$, where the mean is itself represented by a statistical model $\mu_i = \alpha + \beta H_i$, and each part of that model is also represented by a distribution, each starting with a prior: $\alpha \sim \mathcal{N}(0, 10) \\ \beta \sim \textrm{Uniform }(0,1) \\ \sigma \sim \textrm{Uniform }(0,10)$

## Bayesian model variables

Just a quick note about variables. Continuous variables are easy in this Bayesian style, because they are directly represented by a continuous distribution, transforming each realised value. Categorical variables can be represented by indicator variables (0,1 as in the usual regression), or by index variables (1,2,3,4 ...). Index variables extend from one to many categories with no change in code or concepts, are better for specifying priors, and extend easily to multilevel models (involving clusters, sites, etc). The parameter value is estimated using only those cases where the index matches the value of the desired population. This is similar to using indicator variables, but doesn't make an assumption that one value of the indicator is just null.

## Priors

Priors are posteriors, posteriors are priors, they are all distributions and there is nothing special about any of them. In linear prediction, the priors are quickly overwhelmed by data so they don't matter all that much, but they are a very good place to practice and to get the justification right for priors.

There is no correct prior, only justifiable ones, useful ones, agreeable ones, reasonable ones. The preceding section on GAMs shows that a linear model can represent anything, at all (viz von Neumann's statement about elephants).

## Likelihood in Bayesian models

The posterior is the prior times the likelihood; the likelihood is the number of ways an observation can be produced, with the span of parameters that are chosen, which then acts to convert the observed data into a probability.

## Contrasts 

For a model of height fit to a population of males and females, a linear model will produce parameter estimates; say the mean estimate for the intercept is different by 8. This is not the difference between populations, it's just the amount of variability that appears in the model. If one parameter has a large standard deviation, that uncertainty propagates into the contrast between the groups. The truly interesting value is the difference in posterior prediction at each point, given by the updated model using both prior and posterior; this is the contrast, and let's say it's 5 because of that uncertainty. 

The causal contrast is really what we want to compare between groups, or a group and a hypothesis. It is never legitimate to compare the degree of overlap of two distributions: this means that it's never legitimate to compare their confidence intervals or p values either, but rather their contrast,derived from the posterior distribution (such as is obtained by a Bayesian posterior, or, I suppose, by bootstrapping).

The difference in means, or the whole distribution of differences, is something that can be modelled as a posterior distribution, and so can be estimated in light of evidence.

These estimates are whole distributions, not points: points are not estimates, they are decisions.

## Centreing

Centreing, eg $\mu_i = \alpha + \beta(X_i - \bar{X})$, makes it easier to define a scientifically meaningful prior, as does standardising. This leans on the behaviour of a linear model as a predictor, rather than on the generative causal model underlying the data, but it works as long as it's possible to recover the natural scale predictions (which look like the untransformed data) from the transformed version used to build the model. This avoids having to build potentially complex, although causally convincing, prior distributions.

## Posterior predictive effect of a single parameter

The point, or linear single predictor, of a single parameter is never right unless in the most simple model that has perfectly represented all the variation. The correct estimate uses all of the posterior distribution, from which samples can be taken and used to explain what's going on. The question, what is the effect of X on Y given Z, is best represented as p(Y\|do(X)). This can be simply p(Y\|X,Z) in the simplest situation of Z as a common cause, but it helps to phrase the estimand in that causal language because it helps to consider the specific form of the estimator. If Z is a collider, for example, then the effect of X on Y is not estimated by p(Y\|X,Z) but better by p(Y\|X).

The posterior probability at each proposed parameter value can be constructed in a few ways:

-   grid approximation, directly calculating at each chosen proposal using the entire likelihood (or, plausibility), which gets comutationally intensive quickly
-   quadratic approximation, assuming that most curves can be reproduced by a quadratic formula
-   simulation such as MCMC, which assumes very little indeed

## Bayesian linear model

Just a Bayesian model, where outcome variable is a linear function of (has a constant and additive relationship with) the predictor variable, and of other variables. The model considers all possible values of this constant relationship, with the posterior distribution of plausibility for each combination of relationships being the model output. For example, to predict height from a prior of mean 178, sd 20:

$h_i \sim \textrm{Normal} (\mu_i, \sigma)$ $\mu_i = \alpha + \beta(x_i - \bar{x})$: Note this is not stochastic. Also that it's not special to make it linear. It's just easy to explain. If another relationship is easy to explain, and makes more sense, then use that. $\alpha \sim \textrm{Normal}(178,20)$ "what is the value of height when $x_i$ at the mean, $\bar{x}$? $\beta \sim \textrm{Normal}(0,10)$"what is the change in height when $\x_i$ changes by one unit?"; this prior allows negative relationships and maybe a log normal, forcing $\beta$ to be positive and allowing log($\beta$) to be normal and centred on zero, like a Gaussian and so quite easy. $\sigma \sim \textrm{Uniform}(0,50)$

Many possible lines (because, we asked for lines) are generated from the infinite number of lines that could be generated. The most plausible of them, meaning the most plausible state of the world which is consistent with the data, is chosen.

## Multivariable Bayesian linear model

Same as before: in the simple condition, a normal distribution is completely described by its mean and variance, and a multivariate normal is described by a vector of means, and a variance-covariance matrix. The mean is modelled as the sum of an intercept and a n additive combination of the products of parameters and predictors.

There are various ways to write this down, such as the summation $\mu_i = \alpha + \sum_{j=1}^n \beta_j x_{ji}$ or using matrix notation simply **m=Xb** where m is a vector of predicted means, b is a column vector of parameters, and X is the design matrix with as many rows as the data and as many columns as predictors plus one. That first column has to return the intercept so is filled with 1s, and the others are estimated in the process of building the model.

# Model fit

The fit is assessed internally to produce some kind of optimisation of the way the parameters are estimated. Mean squared error and its pals are common ways to assess that fit, or binary prediction error for binary models, and so on. Generally, though, it's not really the fit of the model to the sample that is important.

There's a good heuristic that a less complex model, having the same fit as a more complex one, is preferable. It's not clear what makes this so, apart from a philosophical preference. One thing is the predictive accuracy, or at least the fidelity to an underlying truth. Having fit the model to a training set, we can use a test set to judge its predictive accuracy, and we find that in fairly simple models there is a trade-off between in-sample accuracy, and out-of sample predictive accuracy.

Out of sample accuracy is as good a judge of value of a model as any.

## Plotting the model

Sometimes just looking at the plots really is helpful to think about what's happening.

-   Predictor residual plots show the outcome against residual predictors values, like showing the average prediction error when all of the other predictor variables have been used. The variation that's not expected by the model is then left over as a function of the other predictors.
-   Posterior prediction plots show model-based vs real data, to check fit. - Countrfactual plots are the implied predictions for imaginary experiments, a subtle difference but taking the final causal step.

## Features of a model

-   Fitting = calibration and is parameterisation by selecting models with a "good fit" to data
-   Validation is checking the model results by some standard (statistical or "face validity", see [@collins_sample_2016])
-   Discrimination is the ability of a prognostic model to differentiate between people with different outcomes, such that those without the outcome (*eg*, alive) have a lower predicted risk than those with the outcome (*eg*, dead). There are several ways to quote discrimination, such as *c*-indices (including Harrell's *c*-index, the probability that a subject experiencing an event earlier has a lower score in a survival predictor). The D index is the separation between two Cox regression curves, created by ordering all the Predictive Index values, transforming them using standard normal order statistics (then dividing by $\sqrt{8/\pi} \approxeq 1.596$) then fitting this as a single term Cox regression, whose coefficient and standard error are then D and its standard error.
-   Parametric sensitivity analysis is altering parameters in the model to check the change in results 

## Goodness of Fit

Goodness of fit metric is used to decide whether the model prediction fits the data. Given a set of observations and their accompanying expected values at positions i under a certain model that takes parameter x, a goodness of fit function describes how well the model fits the data for a given value of x. This function might be written g(E1, ... En, O1, On).

The Mean Squared Error is a good generic way to determine Goodness of Fit. Training MSE is almost irrelevant to the purpose of a model, and test MSE is not estimable.

A saturated model exactly fits all the data points. When an objective function describes how the goodness of fit function varies by the choice of x, it can be minimised for some value of x. Formally, we maximise f(x) subject to x $\in$ X, where $f(x) = g(E_1(x), \dots , E_n(x), O_1, \dots , O_n)$. 

## Bias-variance tradeoff 

All models essentially estimate a function *f* relating observations to an outcome. This estimated function $\hat{f}$ can be tested in various ways. Most of these ways must trade bias against variance: the larger the proportion of observations in the training set, the smaller the variance and so the better the precision, but the greater the bias and so the worse the accuracy. This [^22] is common in machine learning, for reasons of overuse of machine learning. [Cross validation](#cross-validation-1) is a powerful way, often used, for "testing the skill" of a machine learning model.

[^22]: bias-variance tradeoff

The unobserved test set MSE of any model is always reducible to

$$E(y_0 - \hat{f}(x_0))^2 = Var(\hat{f}) + [Bias(\hat{f(x_0)})]^2 + Var(\epsilon)$$

This means, if the error is a property of the population and not the model or the sample, that there is clearly a trade-off between test bias and test variance to minimise MSE[^23].

[^23]: The bias-variance trade-off is a condition of life for model builders.

## Multicollinearity 

Non-identifiability of a parameter is when the structure of a model and data do not enable estimation of the parameter's value. Imprecision in the posterior increases with the conditional association between predictor variables. 

## Information Criteria

Let's talk here about Information Criteria, because I'm lost in my own notes. Not a great start. Assuming we are worried about MSE[^24], the MSE goes down the more parameters we put in a model, up to the saturated model. To talk about how good a model is while favouring parsimony, a penalty for the MSE is applied, and we still seek to minimise the resulting penalised error: `penalised error = MSE + k(p+q)` The [Akaike information Criterion] (AIC) uses k = 2, and the [Bayesian Information Criterion] (BIC) uses k = log(n) and so imposes a stricter penalty on the number of parameters.

[^24]: mean squared error

Some options are - linear residual distance (may not fit at all, positive and negative cancel), - absolute linear residual distance (may not fit at all, odd behaviour at zero) - sum of squared residuals (SSR or SSQ) is the least squares method $g(x) = \sum_i w_i (E_i(x) - O_i)^2$

If the deviances from model are uncorrelated, have equal variance expectation (and so wi = 1) and are normally distributed then uncertainty is not a problem for inferences on whether a model is superior to another model. Under these preconditions the least squares estimate equals the maximum likelihood estimate. On the other hand, there's no absolute statement possible about "how good" a fit is using this method, only that it's better than another one. If variances are not equal they can be weighted, by the reciprocal of the variance at each observation if known, or by the reciprocal of the model value at each observation if not (the latter is Pearson's chi squared technique). Maximum likelihood is another option: choose a likelihood function that expresses the probability of a certain parameter conditional on the data observed and conditional on the likelihood function. For example, the likelihood that the probability of an event is p, given 5 observed successes out of 10, is given by the binomial function and is maximal at p=0.5.

The deviance allows model fit to be compared across models, and gives a sort of "absolute goodness" of fit by comparing the likelihood of a test model with the likelihood of the saturated model. D clearly gives a number less than one, tending to 1 as the model approaches perfection.

$D = -2 log \frac{L(\lambda)}{L(\hat \lambda)}$

Then the difference in deviances between to models is chi squared distributed with degrees of freedom equal to the difference in number of parameters between the models. This can be used to construct parametric confidence intervals around the best fitting parameters, form a test statistic and calculate the [AIC]. The parameters that lead to the best fit are chosen by a fitting algorithm. 

## Cross validation {#cross-validation-1}

Leave-one-out (LOO cross validation) is a common way to do this. Take one observation out of the model and re-estimate the parameters; now check how well the model fits all the points, including the one left out. Do that for all the points, 

Is a [@resampling] technique often used to check the "skill" of a model, intended to reflect its ability to classify an unseen dataset. [k-Fold cross validation] is a common introduction, giving a less-optimistic estimate of the "skill" than will a simple train-test split. A sample of observations is split into k groups, *after* which any data preparation is done on the sample: - one group is the test set and the remaining k-1 are the training set, the process for generating a model or its parameters is followed and the estimates for the test set are compared to its actual values - the process is repeated k times so each of the k groups is a test set once - the sample thus generated of model evaluation scores is used to estimate the model skill, reported as a mean with variance of the model skill scores - the choice of k is a complex one and integral to the bias-variance tradeoff; of just use k=10 [^25]

[^25]: The data preparation is done after creating the sample for k-fold cross validation, to minimise information leakage where information obtained in the preparation optimises the model subtly for the entire set rather than estimating its behaviour for an unseen set. Note also that each member of the sample serves as a test subject once and a training member k-1 times, to the extreme of leave-one-out cross validation where k=n.

## Parameterisation

Parameterisation is giving values to parameters in a model by whatever means. This results in a model which generates expectations for the parameters $\hat{E}(X)$ and so, given actual values for those parameters, generates expected values for the outcome. It is confusing that these expectations are not used if the intention is only to generate the expected parameter value, despite using the exact same assumptions.

In any case the predictive index is then the individual result of the predictor for an observation, $PI = \beta_1x_1 + ... \beta_k x_k$, and the D index is closely related to its standard deviation.

# Missing {#missing}

This theory section around missing data is culled heavily from Julie Josse User!2018. Data is missing for various reasons

MCAR -- P(missing) does not depend on either the missing value itself or any other value.

MAR -- P(missing) does not depend on the the value itself; but may depend on other variables (ie it's Missing At Random Within Strata)

Multiple correspondence analysis MCA graph on a "missingness matrix" shows neighbours in missingness; but doesn't imply a missingness definition (eg broken machine probably MCAR but if broken by a high value, then MNAR).

[EM algorithm], or supplementary Expectation-Minimisation algorithm, allows a model that uses the observed data, then updates with the last estimate and so on. Needs a new process for each assumed underlying model.

Imputation using the mean shrinks the correlation to zero; by the regression line shrinks to 1; can impute by the regression line plus noise (stochastic regression) and preserve both the joint and marginal distribution of the data, so estimation has no bias added to the missingness bias itself. It's an extension of this last to the multivariate case that results in the mice procedure.

The [impact of missingness] is related to the difficulty of imputation and for the same reasons: both proportion and structure of the missingness, and of the data itself, are important in determining these. Get signposts to this by cross validation, or PCA, or various other techniques.

Principal Components minimises the distance between an observation and its projection. Estimates the single best projection matrix that does so. With missing values there is no such single estimated matrix. Use instead one of the many algorithms, eg Iteraive PCA. Iterative PCA starts with values imputed somehow, eg the mean. Constructs a vector using the left and right singular value matrices (check) then updates the imputation; then repeats. Always converges but it's not a convex method so there are local maxima; so Josse in missMDA use a "regularized PCA". PCA uses the observed linear relationships between variables and so is most efficient and predictive of reality when the true relationships are linear. So you need subject knowledge, to interpret or even to reparameterise (eg by making interactions explicit). Nevertheless PCA can work when variables are collinear or n \< p, while linear-based methods (mice, Amelia, joint imputations) work less well. "You inherit from the techniques of your imputation". This is because it preserves both joint and conditional distributions, as above, and so all inference based on these has no additional bias.

Multiple correspondence analysis MCA is like PCA for categorical data, using an indicator matrix

Low rank structures imply that you can use clusters to infer with greatest efficiency.

# Analyse {#analyse}

## Stratification

Needs accurate identification of strata: if too broad will replicate the confounding, if misclassified could unpredictably increase or decrease the confounding: "residual confounding". If matched by strata, those strata are reported as if unmatched then results pooled somehow. If perfectly matched for the confounder then the groups can't study the effect of the confounder unless they're also perfectly matched for another cause (resulting in tiny fractured groups, for which a solution is Bayesian Borrowing). Stratum specific probabilities are either randomly distributed about the true total population value if there is partially controlled confounding, or are real, and display the effect modification.

## Standardising to a standard population

Standardisation applies weights to observations in categories according to a standard distribution of observations expected in those categories.

In eg direct standardisation for age, a standard demograph weights crude population mortality for age standardised mortality: the number "deaths per thousand person years of standard population" is the "deaths per thousand person years of actual population" (crude death rate) times comparative mortality figure (CMF, ratio of rates)[^17].

[^17]: I think the weight here is the area of a histogram where x=age range and y=observations, each of the steps of which are multiplied by a death rate to give a new histogram. The area under this new histogram is the population standardised mortality rate and the ratio of its area to the original is the CMF; clearly the standard matters, especially as they insist on probabilities not odds.

Indirect standardisation for age uses not size but death rates as the standard. It's the basis for the Standard Mortality Rates and hence ratio, SMR. [^18]

[^18]: "Direct standardisation is subject to less bias"; why? Because true rates are needed and so there's more finely grained data? Maybe, because "indirect may be the only possible" if rates aren't available at each stratum or numbers are small hence standard error high.

So

-   Direct uses observed rates if applied to a standard population, ("Directly standardised rates")

-   Indirect uses standard rates if applied to the observed population

If a confounder is misclassified its effect may remain: residual confounding

Or use a model to borrow from neighbouring areas, using both a structured and unstructured geographical component, eg:

$Y_i|P(x_i) = Binomial(N_i, P(x_i))$ or

$logit(P(x_i)) = z_i \beta + S(i) + u_i$

Where S(i) is according to some coordinate reference system CRS (UTS in Eastings + Northings; or lat/long in unstructured ellipsoid projection)

# Study designs {#designs}

## Case Control

The sampling is either one or more tier: "internal controls" are unexposed ones from the initial sample. If a rare exposure the initial sample must be very large; or "similar" unexposed can be selected from the same or a similar population in another sample as "external controls". The similarity is discussed in mechanistic terms needing experts, not advice on sufficient stochastic descriptions of the similarity. "If the groups are comparable they have internal validity"; "if the conclusions are applicable to other populations it has external validity" "In case studies the controls are only a subset of the NonCases in the population"

The OR is "Odds of exposure in those with outcome"; the logical flips assumes that the sampling probability of controls is the same whether exposed or unexposed.

Case definition is: - The method used to identify - The boundaries of cases -- eg cutoff on a continuous measure, ins and outs for categorical - The unit of analysis -- eg community, person, organ, events themselves

## Cohort

E(D1) = Y1 \*(D/Y)

-   Exposure status is not biased by knowledge of outcome
-   Exposure status is known for all (because it's the definition of who is in)
-   Time sequence and course of events can be described
-   More than one outcome or type of outcome can be reported

Consider Source of controls How many types of controls How many controls per case (power gains fall from 4 per case) Should they be matched? Loss to follow up reduces power and certainty Differential loss to follow up (by exposure or outcome) biases irretrievably Information / classification bias if outcomes are not equally accurate by exposure

## Ecological studies

These use population values rather than individual values so n is small Multi-group / Time-trend, descriptive / analytic Cheap, easy, quick, maybe all that's possible, allow population inference But don't allow individual inference and don't control confounders "Ecological Fallacy" is extending ecological inferences to individuals Population average exposures would also have to vary by individual to account for variability in outcomes per individual

Contextual effects of exposures: when grouped, may have effects contingent on others Outcomes can become exposures for other or the same outcomes (also a contextual effect) Summary / environmental / integral measures Confounding is easily hidden by complexity and size and there are no opportunities to adjust Associations, so confounders, may exist at population level that don't at individual level Details of the distribution of confounders is often not available Bias: Information bias is common, often undetectable and severe often due to the use of proxy, secondary or routine data at high risk of misclassification "non-differential misclassification within groups in ecological studies", unlike in individual level studies, "usually leads to bias away from the null" Misclassification is very common and may be severe. A sort of selection bias by examining exposure and (time dependent) outcome in the same slice, eg measuring candidate risk factors and the cancers at time t (not t+latent period).

Correlations and regressions are the usual numbers generated after exploratory analysis. Having the Idea from Crombie has a holy feeling about it, for all that it was written humbly by a sharp witted amateur. Review existing practice Challenge accepted ideas Look for conflicting ideas, ask around Investigate geographical variation or interindividual variation Identify Cinderella topics Let loose the imagination / be kind to ideas / reject little Dissect the questions Write Discuss with colleagues over pints Think in the bath Read around the subject until you have read enough Look upstream Choose the best question Will this method answer my question? Evidence there is a problem\
Is it feasible? Burden of disease Do I have the resources? Big possible changes to patient outcome Is method appropriate for the design? Cheap solution or expensive problem Question is a question other people can understand An answer would be done by people Urgency of good data / chance of bad practice to entrench Feasibility Refine the question with a sharp tool and a high threshold for acceptance Review as if newly seeing it Is it as important as it looked? Don't rush in: are you still sure that's the question? Review and redo the analysis, simulate, simulate, simulate Criticise mercilessly Where could it go wrong? Why might people not participate? What necessary data might not be feasible to collect? Do we have enough blood and treasure to spend? Can we simplify it and still get an answer / will it be even better? Be realistic Ask the best version of the best question as well as you can, if you can do it well Don't waste effort on getting incrementally closer to perfection Don't start what you will never finish Qualitative Seeks to interpret phenomena through the meanings people bring to them When the topic is not familiar, the context is needed, rich views or the social world in which others live is the goal. Common features are flexibility, triangulation, contextualisation, iteration and reflexivity. Techniques include interviews, focus groups, observation, participation.

# Utility {#utility}

This is something that needs to be expanded here. 

# Ethics {#ethics}

Beneficence, non-Maleficence, Autonomy 

Justice, Goal-, duty- or rights-based justifications 

Validity, welfare, dignity (where Dignity = Confidentiality, Consent, Coercion) 

Consequentialism, Deontology, Communitarian schemata

Reseach is not valueless and ethics are not universal: it's a process to ensure the dilemma is continuously resolved.

ICH GCP principles 
1. Do it with GCP 
2. Benefit \> Risk 
3. Rights of subjects \> Society 
4. Info supports the trial 
5. Protocol 
6. Approved by Ethics 
7. Supervised by Dr 
8. Qualified team 
9. Informed consent 
10. Data protected 
11. Privacy / confidentiality AND GMP manufactured products 
12. Quality control in trial 

NIH has the... The Eight Ethical Principles 
1. Collaborative Partnership 
2. Social Value 
3. Scientific Validity 
4. Fair subject selection 
5. Favorable risk-benefit ratio 
6. Independent review 
7. Informed Consent 
8. Respect for human subjects 

Protocols for the NIH contain
         
- Précis 
- Introduction 
- Objectives 
- Inclusion and Exclusion Criteria 
- Plan for Monitoring Subjects and Criteria for Withdrawal of Subjects from the Study 
- Analysis of the Study 
- Human Subject Protections 
        - Rationale for Subject Selection 
        - Recruitment Plan and Procedures 
        - Justify the Exclusion of Women, Minorities and Children (if applicable) 
        - Evaluation of Benefits and Risks/Discomforts of Participation 
        - Description of the Consent/Assent Process 
        - Plan for Maintaining Privacy and Confidentiality of Subject Records and Data 
- Data and Safety Monitoring Plan 
- Protocol Monitoring Plan 
- Data Management Plan 
- Plan for Research Use and Storage of Human Samples, Specimens or Data 
- Remuneration/Compensation 
- Scientific References 


## Community Engagement

From Smith Morrow Ross: "community engagement will be defined as the process of the trial team working collaboratively with the community on all aspects of the study which affect the community and its well-being. Overall, engagement should typically involve continuous mutual learning and communication between researchers and a range of community members before and during a trial and after a trial ends."[^19] Should never be prefabricated, always negotiated and always actually strengthen participants' ability to control important aspects of their own lives or "empower" them. Find out, acknowledge and reconcile complexities in community attitudes and internal power structures, empower the unempowered[^20]. Pilot or engagement activities may reveal that much more work needs done just to understand the views of the community, before touching the research design. Locoregional administrators or governments, or relevant NGOs in some places, are also necessary[^21] Local health providers are usually just completely necessary for the intervention to work or be measured, and are also advised to be properly engaged for advice and "synergy".

[^19]: I think there's a little too much of the White Man's Burden in this, where the inherent "good" of engagement is because of the researchers displaying cultural humility. I'm sure it's nice to see but the studied community would get along just as well with nobody coming in to satisfy themselves about their cultural humility. The conduct of a trial does change things and the focus should be on whether those changes are beneficial enough to justify doing the study there.

[^20]: This can surely cause problems? Breaking the wheel is not without collateral damage.

[^21]: ... and probably evil, I'd guess. Smith adds special interest and religious groups to this, explicitly recommending power games and assessing factions as if it's in fucking Helmand. It's largely motherhood shite otherwise.

Frameworks Methodologies Theories Participatory blaaahhhh

Apparently visual aids are Participatory.

## Communicating results

They recommend considered and mediated feedback of overall results, but they also recommend using simple tests as a sop to the simple folk while sending off all the other tests.

# Surveillance {#surveillance}

Surveillance is "the ongoing systematic collection, analysis and interpretation of health data essential to the planning, implementation, and evaluation of public health practice, closely integrated with the timely dissemination of these data to those who need to know. The final link in the surveillance chain is the application of these data to prevention and control. A surveillance system includes a functional capacity for data collection, analysis and dissemination linked to public health programmes."(CDC, 1986)

Surveillance (tracking a disease or risks) vs monitoring (tracking process and outcomes).

-   Practical, uniform, rapid; unlike epidemiological research which is one-off and specialised

-   Many data collectors with less interest or time

-   Passive (cheap, routine, patchy and dodgy data) or active (expensive, higher quality data)

-   Lab surveillance only works for some diseases and depends on referrals; or serology surveillance of samples sent for other purposes

-   Save some money and approach good performance by two stage systems or sentinel systems.

-   Establish the objectives of the surveillance system

1.  Develop a case definition
2.  Develop a data collection mechanism
3.  Field test methods
4.  Data analysis
5.  Interpretation
6.  Dissemination of information
7.  Evaluation of the system

# Infectious epidemiology

This is a mathematical modelling pursuit. As such it depends on things not measured for its calculation and hence not included in it. These assumptions are usually important and generate surprisingly little comment.

The first concept is probably effective contact. All human interaction can be boiled down into the frequency, duration and type, which is disappointing. A step further can abstract it to the rate of effective contact, which in a large enough population is perfectly adequate for modelling.

## Deterministic models

Abstractions are used to simplify the problem so that it represents the long run behaviour of a population with various states and flows between them.

One useful abstraction is to define the number of effective contacts *per person* per unit time: then by holding the definition constant, the probability of infection for each of those contacts is almost disease specific and is called $\beta$. With the current proportion infected (or the absolute numbers of infected I and total population N), the force of infection can then be defined as

$$\lambda = \frac{\beta c I}{N}$$ The initial rate of new infections, assuming homogenous mixing, is then dependent on the force of infection and the size of the population. Both the force of infection and the number of susceptible individuals changes as more are infected, and the rate changes depending on initial values. A system of differential equations can be set up to describe an arbitrarily complex model

"Generation time and serial interval are the same"

-   **Generation time** is *Latent* plus *Infectious* and
-   **Serial interval** is *Incubation* plus *Symptomatic* time, which adds up to the same without its components necessarily being the same. It's also, more formally, between identical clinical moments in successive cases in the same chain of transmission.

**Virulence** is proportion symptomatic / having

SAR (Secondary Attack Rate) is proportion Exposed and Infected / Exposed and usually assumes **identical duration** and **intensity** of a **unique** exposure among all who are considered, such as a class or village. SAR and transmission probability are conditional parameters and either require contact details or see below.

$R_0$, or the basic reproduction number, is the number of new infections from the introduction of an infectious individual into an entirely susceptible population. It's and odd sort of idealised value: it could have been observed pragmatically if perfect knowledge of all exposed individuals were available, but on the other hand it changes with the slightest perturbation in the mixing parameters, composition and structure of the population. For building models it's useful with contemporaneous data. Heuristically, it's most useful as a threshold: if $R_0 < 1$ in a population the disease can't expand.

-   Compartments in populations behave differently and most diseases are not "simple" so modelling derives the R0.\
-   Models rarely derive the same R0 if given the same data.

$R_0 = c\beta t$, that is $R_0$ = contacts \* duration of infectiousness \* probability of infection per contact. If x is the proportion susceptible and "herd immunity" is 1-x then when R is 1: $HI = 1-\frac{R}{R_0} = 1-\frac{1}{R_0} = \frac{R_0}{R_0}-\frac{1}{R_0}$, so HIT (herd immunity threshold) is $HIT = \frac{R_0-1}{R_0}$

**Vaccine efficacy** is a foolish concept and has resulted in much unwanted inference and communication. It is "usually" 1-relative risk reduction (1 - Risk in vaccinated/Risk in Unvaccinated). If due to reducing infectiousness it's $VE_i$, if susceptibility $VE_s$.

-   If $VE_s=0$, $VE_i=1$ and f is the proportion vaccinated then $R_f=(1-f)R_0$

-   If $f=1-(1/R_0)$ then herd immunity happens. For more general $VE_i$ and $VE_s$ this f is multiplied by $\frac{1}{VE_s + VE_i - VE_s VE_i}$

Estimation depends on being able to see direct and indirect effects; with some assumptions $VE_s$ and $VE_i$ can be jointly estimated from only the disease rate in vaccinated versus unvaccinated if there are different vaccinated proportions in strata, clusters or times. This joint estimation has a coverage estimate trading $VE_i$ against $VE_s$. If considering a 2nd stage cluster, infection probability $\beta$, $VE_s =0$, proportion infected at time t $I_t$, contact probability *c*, duration of contact *d* then in population of n individuals the hazard rate for infection if vaccinated is $\lambda_{0i}(t) = {c \beta I_{i}t}{n_i}$ and for unvaccinated is that times $1-VE_i$.

# Machine Learning {#ML}

```         
A Perceptron is the smallest form of neural network, with weighted inputs and a bias which feed an activation function to produce a single output.  The output is calibrated repeatedly against appropriate values and the weights altered to improve the closeness of prediction until either accuracy or number of repetition limits are crossed.
```

The ideal type of data depends on the intended output. eg Description: whole population Exploratory: many variables on a random sample, up to the whole population Inferential: The right variables in the right population, randomly sampled Prediction: split training and test sets from same population Causal: data from a randomised trial Mechanistic: rich data about all available responses and predictors


# Meta Analysis {#meta-analysis}

-   Provide a data display and objective review
-   Give a summary interpretation
-   Test an overall hypothesis
-   Estimate an average exposure effect
-   Assess whether data compatible with the exposure effect being the same in all studies

Fixed Effect assumes the effect really is the same and any difference is due to sampling variation: provides a summary using a weighted average of the individual study estimates. Weights are eg 1/variance of the log odds ratio, so the summary OR is $\psi_F = \frac{\sum_{i=1}^k w_i \psi_i}{\sum_{i=1}^k w_i} , \pm \frac{1}{\sum_{i=1}^k w_i}$, which is the variance regenerated from the averaged weights. A Forest Plot represents each study (box size = weight, bars for CI, diamond width = CI of summary estimate) and the heterogeneity between them uses a Chi Squared test $Q = \sum_{i=1}^k w_i (\psi_i - \psi_F)$. If heterogenous then use Random Effect.

Random Effect assumes the true effects vary around an average: the between study variance of the estimates (or alternatively, the residuals around the average of the estimates) is used to modify weights of each study. A summary OR is the same shape as the fixed model summary OR but with between-study-variance-adjusted weights, w\*i. These weights are far less dispersed than the Fixed Effect weights so smaller studies are weighted more heavily and the confidence intervals tend to be wider. So Random Effects models are more conservative.

# Git work {#git}

`pwd` print working directory `ls` `clear` `touch` newfilename for new file `cp` file filetocopyto -r for recursive(=copy all within) `mkdir` `cd` `cd ..` up one `cd` home `rm` thing to remove -r with caution! No undo! `mv` object dir , also changes name if with file name `echo` `date`

git init git clone git add . Adds all new files to the index git add -u updates git add -A does both git commit -m "message" to local repository git push to remote directory git checkout -b branchname another version of the same directory git branch see what branch you're on pull request is not a git feature but on github

# Syntax and reminders {#syntax}

## Base R

`download.file` `file.exists` `dir.create` `list.files` `dir` `ls` `read.table(sep, quote, na.strings, nrows, skip)` `read.csv` `read.csv2` `seq(from, to, by along )` `cut(data, breaks)` `factor()` `relevel(data, ref)` `intersect` `split-apply-combine` `sample`

split-apply-combine with(tapply(X, factor, function)) type of structure or with(by(X, factor, function)) or aggregate(data \~ factor, source or environment, function) by position by logical statements sort order eg X[order(X$var1, X$var3),] add columns by X\$newvar or by Y \<- cbind(X, newvar) sum colSums rowSums any all is.na %in% head tail table xtabs(var1 \~ var2 + ., data=DF) where . is all the variables ftable(

apply(data, dim, fun) tapply(apply along an index, a function to data) ie tapply(data, index, function) sapply(simplify the output of a function applied to data) eg perms \<- sapply(1:10000, function(i) testStat(DEcounts, sample(group))) lapply(apply a function across a list; unlist to simplify)

`Hmisc::cut2(data, g)` `reshape2::melt(data, id, measure.vars)` `dcast(melteddata, formula, Aggregation value)` `transformations` `plot` - `coplot` - `hist` `abs`, `sqrt`, `ceiling`, `floor`, `cos`, `sin`, `log`, `log2`, `log10`, `exp`, `round(data, digits=n)`, `signif(data, digits=n)` `p.adjust(method = c("bonferroni", "BH", "BY")`

## Rethinking

`quap()` produces a quadratic approximation of a model specified by an initially non-evaluated `alist` of formula terms. `dens` plots the density of each of the distributions to allow checking for sense and whether it's what you intended. `precis` gives a summary of the posterior distribution (made from the variance-covariance matrix). `extract.samples` can then make up samples from the relevant multidimensional Gaussian distribution. `link()` samples from the posterior distribution of the parameters calculated by quap(). Defaults to using the original dat so pass it a vector of predictor values over which to sample. Then use `mean` or `PI` to compute averages and bounds for each value of the predictor variable (rather than analytically). Then use `line` and `shade` to draw these if it seems like a good idea.\
`sim()` produces samples from the full posterior model (simulated values), including both uncertainty in parameter values and uncertainty in a sampling process. `splines::bs` can produce the basis functions.

## time series

`stats::arima.sim` is a terribly described function. `arima.sim(model = list(order = c(p, d, q))` n= number of observations to simulate). p is the AR order, q is the MA order (the highest lag) as in [ARMA]. Then separately the AR and MA parameters are provided, in positions corresponding to the individual lag and in number up to the order. So `model = list(order = c(0, 0, 0))` is white noise, `model = list(order = c(0, 0, 1))` is MA(1) and order = c(0, 0, 1), `model = list(order = c(0, 0, 1), ma = 0.9)` is a MA(1) with MA parameter 0.9.

This generates an xcf object whatever that is, with a useful generic method to plot it. The last order entry is the differential: a d=1 means that the first difference of the ARMA is stationary in which case this is an ARIMA model. `stats::acf` computes the autocorrelation function. For a pure AR process this function "tails off", not seeming to have any informative maximum. For a pure MA process this function becomes zero above the MA order. These can be used to iterate to the best value of the order, starting with (1, 1).

For an ARIMA the ACF tails off very slowly and the PACF is almost 1 at lag 1, whereas after differencing it once, if d=1 the ACF/PACF looks like that of an ARMA model. `stats::pacf` is the partial autocorrelation function. `astsa::acf2` plots the acf and pacf `sarima(data, p, d, q, P, D, Q)` from astsa estimates the parameters of the (specified) model, and provides some residual plots, including Q-Q, standardised residuals, Sample ACF of residuals and the Q-statistic p-values. `sarima.for(data, p, d, q, P, D, Q, n.ahead)` produces forecasts

## SQL

Install the software then start the service. This kind of doesn't work often. So check with `service postgresql status` which might say it's `down`.

So then use `sudo service postgresql restart` And then may need `sudo -u postgres psql` to log in as the user `postgres`, which is just the default non-root user.

`psql [database name]` starts up a prompt in psql. Which can be exited, as can its submenu `\help`, with `\q`. SQL doesn't have to be in capitals, or indented, or anything.

When finally in, there are some common queries.

`SELECT FROM` aggregate functions such as `MIN()`, `MAX()`, `AVG()`, `COUNT()` `WHERE` can't be used with aggregate functions so use `HAVING` eg `SELECT \* FROM table HAVING AVG(parameter)\>13`

`CREATE TABLE <tablename> (fieldname FIELDTYPE);` etc `INSERT INTO table VALUES (valueforrow1col1, valueforrow1col2);` etc `SEL`

## xlsx

`xlsx::read.xlsx(sheetIndex, rowIndex, colIndex)` `read.xlsx2` `write.xlsx`

## XML

HTML is a markup language. XML, Extended markup language, is a structured data store or formally, "a meta-language facility for defining a markup language", which is self-describing, extensible and yet with strict parsing rules. Apparently it also allows tool sharing across disciplines and I await enlightenment on that. Mainly works via tags, which can have attributes, and form elements. The `XML` package has short (good) and long (long) tutorials, and provides methods for XML and HTML which are moderately different protocols.

-   `<tag>` such as the tag surrounding this sentence `<\tag>`
-   attributes are added to the tag such as `<tag attribute> <\tag>`
-   the basic unit of XML is an element, also called a node or chunk; elements are the combination of tags and the content tagged by the tags
-   `t\<tag />` is an empty or collapsed tag, such as is often used for images

`xml::xmlTreeParse(fileURL, useInternal)` TreeParse returns a structured list which can be subset eg rootNode[[1]][[1]], or have components programmatically extracted `xmlRoot` `xmlName` `xmlSApply(doc, path fun, ...)`, *eg* `xmlSApply(rootNode, xmlValue)` is how to alter the content of the XML object There is yet another language to learn, called `xPAth` language with syntax such as - `/Node` a top level node - `//Node` a node at any level - `Node['@value'='thing']` a specified node; read the short intro at least. `httr::GET, PATCH, POST, HEAD, PUT, DELETE`

## HTML

<https://developer.mozilla.org/>

`tags` include - `<p></p>` block element for paragraphs `<span></span>` inline element for paragraphs `<img>` a self closing inline element for images

`<head>` comes above, `<body>` after.

## CSS

`style` can be found in many levels; the more specific cancels the less specific. Comment a block by /\* ... \*/ `selector{property: value;}` = who{what:how;} For example, select by html tag value.

`class` is made up on the spot in the html, and referenced with a preceding dot in the css. Many classes can exist, many elements can have the same `class`. May also be used to select. For example, `<img class = "x" src="">` then `.x{color:green;}` `id` which is set within the html tag can also be used to select, referenced by a hashtag. Only one element can have a given `id` on each page. For example `<h1 id = "tops">` then `#tops{color:green;}`

`display` can be - `block` (sequential lines) - `inline` (same line, can't have width altered) - `block-inline` (same line, can alter width), or - `none` (is not rendered). Some elements are `block` by default, some `inline`, but they can have display types assigned in CSS.

`visibility` can be set to `hidden`, when the element is rendered but not displayed

`position` can be - `static` (like default html-derived) - `relative` to how it would have been had it been static, but pushed in the named direction by the amount defined (so `position: relative; top: 50px;` places the object downward by 50 pixels). It can then overlap other elements, while leaving its static ghost behind to interfere with the rendering. - `absolute` which is, it turns out, relative to the parent and pushes the position within the parent away from the named edge. So the opposite of relative. so`position: absolute; top: 50px;` Moves the object 50px from the top of the parent. Can overlap, doesn't leave a ghost. - `fixed` stays where it is even while scrolling.

`margin` can be looked up but auto is useful for horizontal centreing, along with `text-align`.

## JSON: Javascript Object Notation

`jsonlite::fromJSON`, `toJSON` multiple nesting of data from the data frame resulting from from JSON

## data.table

`data.table` subsets by expression, not column; does not copy data tables on subsetting, use copy function; uses plyr-like operations, for example on .N by some value; `:=` adds a variable `N` is an integer length 1 containing the number of observations `key` data.tables have keys to allow joins. `setkey` and `merge` `fread` for fast reading (substitute for `read.table` for tab separated files)

## Tibble

`tibble` can make packages eg with list-columns so a data frame need not hold numbers or other low level entities in its cells but whole lists of interest such eg as a training resample from a dataset, a test resample, a model fitted to that test dataset and the resulting RMSE. Tibbles are lazy and surly, don't require a list of vectors as the underlying structure and give a little more information in printing than data.frames.

## dplyr

`arrange` reorders rows `filter` returns subset of rows based on a condition `select` returns subset of columns (use `starts_with`, `contains`, `matches`, `ends_with` to specify), in the order specified `mutate` transforms data and adds or replaces them in the data.frame `rename` rename without otherwise changing `summarise` summarises. Also spelled summarize `group_by` reorders a data.frame "by group" `merge` (x, y, by.x, by.y, all = T/F) where x is one dataset and y another, and "all" is whether to keep duplicate variables and add a subscript to the added variables to identify it . `plyr::join` is faster but only works by a common variable. `plyr::join_all` takes a list (so need to form a list) of data frames and joins all by a common ID variable. `%>%` chains operators in sequence `*_join`: `left_join` includes all observations in the left data frame; `right_join` analogous, `inner_join` only observations in both, `full_join` all observations no matter the match `dbi` package for using many other backends to dplyr

## tidyr

`unite` brings together values, like paste but with type specs.

`lm(outcome~predictor + predictor2 - 1)` With intercept removed by -1; `relevel()` to set intercept. Counts modelled straightforwardy and adjusted for lower bound or heteroskedasticity. Indicator variables add an optional intercept for binary factors.

Influence.measures . rstandard is residuals, hatvalues is for influential points (esp data entry errors). dffits is how much the line at that point is influenced by the absence of the data point. dfbeta is one for each point for each regressor; it's a matrix. cook's distance is a summary of the dfbeta by giving an overall change in them. resid returns ordinary residuals, hatvalues gives the predicted Y value with the point excluded vs included. A cross validated leave-one out residual, PRESS residuals are resid(model) / (1- hatvalues(model)).

## Hadley Wickham on the Tidyverse:

`Import-Tidy-{Transform Visualise Model} -- Communicate`

The eye is drawn to the verbs and that's important because good data science can't be done without programming and the verbs are activities that can be programmed. But for good workflow it's the interactions between each of those that's important. Modelling is ultimately a computational approach that scales well but, because it uses assumptions at some level, and can't question those assumptions, can therefore not really surprise. Visualisation on the other hand is a human activity and it's much harder to throw more brains at the problem than to throw more computers at the problem

Share data structures, compose simple pieces, embrace functional programming and write for humans.

## Python

'sake

`object` Objects have attributes, which have associated methods (functions specific to object types). Methods are often called with an object as prefix: object.method() .

`method` Methods work differently depending on the type on which they are called. Fucks again python.

`packages` Packages are collections of scripts (.py files); they are called modules in this context. Packages imported, then modules called from the packages by package.module (eg numpy.array and the irritating renaming of the package is where np.array comes from); or modules are imported from packages then called bare; in which case the "import as" makes more sense because you can quote it bare.

`index` Indexing starts at 0 and starts if descending at -1. Indexing in NumPy is by recursive [] calls or by R-style [dim1, dim2, ...] coordinates.

`dictionary` Key:value pairs initiated with {"key":"value}, accessed by [""]. Stored as hashes. Lists can't, therefore, be keys because they are mutable.

`Pandas` Produces DataFrame which is a cbind of "series" . All to make an array that can contain different data types. Accessed by [] for the series and [[]] for a new DataFrame containing the same information as the [] does. Extended by the loc and iloc functions. loc specifies by ['name of key"] for columns, or [[]] which can be chained. iloc does the same using indices of the location. ix is a method to mix loc and iloc. The different chaining results in different formats of output.

`slice` Calls to index include the first and exclude the last.

`lists` Are stored separately from their name. So x = [...] ; y = x declares y a pointer to the list that x also points to. So manipulating the list using y is the same as manipulating the list using x. Except that a slice or list() call creates a new list. Fuck you python. And deleting an element changes the indices of subsequent elements. This is under "explicit versus reference based copies".

`NumPy` is necessary to use any sums. It introduces arrays which MAKE PYTHON WORK LIKE R: elementwise mathematical operations; it's a little tidyverselike because you can use proximal assignments to make distal ones. Does this mean lazy evaluation? Arrays can contain any type but all the same type.

`attributes` In a NumPy object are accessed by '.' The same as '\$' accesses them in an R object.

## stata

`[by varlist]: command [varlist] [if exp] [in range] [using filename], options` describe summarize describe\
display Calculator Do Execute a do file lookup Online help save save. To overwrite :save, replace append concatenate new observations into existing columns codebook collapse "aggregate data" count drop delete duplicates tag duplicates return list list the output of the last command, maybe generate new variable egen new variable label define list list values of variables merge some kind of merge reshape maybe some kind of transpose "long to wide or wide to long" sort table two way tables tabulate one and two way frequencies stset strate sts stmh stsplit logit log scale logistic natural scale clogit, or conditional logistic, log scale unless or xtpoisson random effects incidence rate ration poisson regression

```         
outcome exposure##interactor,
exp(denominator follow up time) irr i(cluster) re
```

mixed mixed effects linear regression, output includes var(\_cons) which is the estimate of between cluster variance. \|\| is the \~ of stata, reml is restricted likelihood kscore arm if covariate==i\|\| cluster:, reml nolong xtlogit random effects, gives icc and clustering chisq. outcome treatment, or re i(cluster) quadchk probably not necessary any more because adaptive quadrature is used to approximate normal likelihood distribution

```{r bib, echo=FALSE, message=FALSE, include=FALSE}
knitr::write_bib(c('base', 'rmarkdown'), file = 'Epinotespackages.bib')
```

[EM algorithm] [differential treatment effect]
