---
title: "EpiNotes"
subtitle: "It's a dog."
author: "Lewis Campbell"
date: "`r Sys.Date()`"
output: 
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
    number_sections: true
  tufte::tufte_handout: 
    citation_package: natbib 
    latex_engine: xelatex
  bookdown::epub_book: default
bibliography: Epinotespackages.bib
csl: biomed-central.csl
biblio-style: plain
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
#library(bookdown) + render_book("EpiNotes.html", "Epinotes.epub", output_format = epub_book(pandoc_args = "--mathml", epub_version="epub3"))
#calibre(input="EpiNotes.epub", output="EpiNotes.mobi", options = "--mobi-file-type new")
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

\frontmatter
\tableofcontents

# Introduction

This is the notes from my _MSc Epidemiology_ from LSHTM, one of the two most respected Epidemiology institutions in the world.  It's a dog.

I wish I had done this twenty years ago, and spent the intervening time studying real statistics and computer science, instead of listening to people vomit party lines in my forties.

# Risk
`r newthought('The baseline odds is key to understanding')` the effect of a given odds ratio on the absolute risk difference.  While the odds ratio is the best way to state a constant effect of an exposure on the likelihood of an outcome, it needs to be applied to a baseline absolute risk to display itself.

![Absolute benefit as a function of risk of the event in a control subject and the relative effect (odds ratio) of the risk factor. The odds ratios are given for each curve.](C:/Users/lewis/Documents/Studying/Epidemiology/EpiNotes/Risk_magnification.jpeg)
```{r fig.margin=TRUE, cache=TRUE, message=FALSE, echo=FALSE, fig.cap='Absolute benefit as a function of risk of the event in a control subject and the relative effect (odds ratio) of the risk factor. The odds ratios are given for each curve.'}
knitr::include_graphics('C:/Users/lewis/Documents/Studying/Epidemiology/EpiNotes/Risk_magnification.jpeg')
```

# The contingency table

A table allows things to be seen, and calculations referred to.  "Expected values" for the cells in the table can be calculated under various hypotheses using the marginal totals: these E don't have to be all possible at once, so "expected" for rows and columns will differ.

\begin{tabular}{| c |c | c | c| }
\hline
& + & - & \\ \hline
D & a & b & $N_{D}$ \\ \hline
$D^c$ & c & d & $N_{D^c}$ \\ \hline
& $N_{+}$ &  $N_{-}$  & N \\ \hline
\end{tabular}


\begin{table}
\caption{Marginal calculations}
\begin{tabular}{l l l}
\hline
sensitivity & =		$P(+|D)$  	& =	a/(a+b) \\
specificity & = 	$P(-|DC)$	    & =	d/(c+d) \\
ppv         & = 	$P(D|+)$	    & =	a/(a+c) \\
npv         & = 	$P(DC|-)$	    & =	d/(d+b) \\
accuracy    & =		P(correct)  & =	(a+d)/(a+b+c+d) \\
variance    & =		$\frac{(a+b).(a+c).(b+d).(c+d)}{n^2(n-1)}$ \\
\end{tabular}
\end{table}


$DLR+ 	= \frac{P(+|D)}{P(+|D^C)}$
$= \frac{\frac{a}{(a+b)}}{\frac{c}{(c+d)}}$
$= \frac{(ac+ad)}{(ac+ab)}$

DLR+ = (diagnostic likelihood ratio of a positive test) = sensitivity/(1-specificity) 

DLR+ = Bayes Factor
		
Because Posterior Odds = Prior Odds times the DLR, a DLR of N means the H(D) is N times more supported by data than is its complement, 
$H(D^C)$

`r newthought('The standard error for log odds ratio is')` 

$$\sqrt{\frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}$$
and the 95% confidence interval for the log(OR) is +/- log(SE).

The exponential version of this it the Error Factor $e^{z_{\alpha} \times \sqrt{\frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}}}$, whereby the 95% CI is OR/EF to OR*EF or exp(log Estimate +/- log SE)

## Measures of outcome

Case definitions need 

- Method for detection or definition 

- Case boundary 

- Unit of analysis (a diagnosis, an individual, a cluster...?)

Population at risk is the entire realised, actual population to which the case definitions are applied. Above that somewhere is a sort of superpopulation, an ideal version of the realised population. This is usually just called the population in what I think is not even shorthand but a lack of concern over how ephemeral it proves to be under analysis. This _population_ is a sort of a holy quantity, those who could be cases: so case definition circularly influences the definition of a population, because the cases must be drawn from a population. The harder way to derive a case definition is to imagine the population first, and then create the definition of what would be important to call a case. In that case, the influences on the definition of the population are the more expected ones: geographic, demographic, perhaps genetic, and behavioural.



## Measures of association

Relative risk is $r_e / r_u$. Attributable risk is the risk difference $r_e - r_u$ under the assumption that the difference is causal. Again, here they come teaching the fruit before the tree. 

Attributable fraction AF is the proportion of the higher risk that is unique to the higher risk $AF = \frac{r_e - r_u}{r_e}$, whence, multiplying all by $r_u$ and substituting  $RR = \frac{r_e}{r_u}$ : $$AF = \frac{RR-1}{RR}$$

Population attributable risk PAR is the extra population risk due to exposure $PAR = r_t-r_u$, again assuming it's _all causal_. This is mathematically the same as attributable risk times the prevalence of exposure, and population attributable fraction is predictable from the definition.
		 $$PAR = p.AR = p(r_e-r_u)$$
		 $$PAF = p' AF_E =  \frac{p'(RR -1)}{RR}$$
		 
where p is the proportion of the population exposed, and p' is proportion of cases who are exposed.
because total cases if a fraction p of the population is exposed, 0<p<1, the risk in the total population is calculated for a single exposure, assuming all sorts of things (it's all causal, there are no effect modifiers, all other causes are orthogonal to both exposure and outcome)
        $$r_t = p.r_e + (1-p)r_u$$

So anyway PAF is like attributable fraction, and the whole calculation then goes a little something like this
        $$\frac{r_{t}-r_{u}}{r_{t}}	= \frac {PAR}{r_{t}} = \frac {p.(r_{e}-r{u})}{r_{t}}  = \frac {p.(RR-1)}{p.(RR-1)+1} $$

A confidence interval for PAF is given using 1-PAF. Honestly I'm not sure why but maybe it's emphasising that the observed value is usually the exposed and diseased, and so the uncertainty is in how much of the rest of the population there is: 
        $$1- (1 - \textrm{PAF} ^{\times}_{\div} e^{1.96 \sqrt{}\frac{a}{b n_1} + \frac{c}{d n_0}})$$

## Inference using certain sampling designs 

Call the parameter of interest theta; whether it's risk ratio, rate ratio, odds ratio (I know, sshh...). Then, 

- A cohort estimates theta directly but only estimates p and p' if it's a random sample, otherwise external estimates are used.  Then the above equations are used. 

- In cross sectional studies prevalence is estimated, incidence can't be. 

- In case control, "theta is estimated by the odds ratio"; as $\theta = ad/bc$ and $PAF = p'(theta – 1)/theta$ and $p' = a/n$ so $PAF = \frac{1- (b/n_1)}{(d/n_0)}$, the ratio of cases unexposed to controls unexposed. 

PAF for several levels of an exposure is given by $\sum \frac{p_k' (\theta_k -1)}{\theta_k}$ or the overall equation $\frac{p'1-\theta_{tot}}{\theta_{tot}}$, using as $\theta_{tot}$ a regression estimate with whatever covariates are desired to be included. The joint PAF for independent exposure is 
            $$1-PAF_T = 1-PAF_1 + 1-PAF_2 ... 1-PAF_i$$
$AF_E$ is often used to assign blame by transposing the conditional: as it is the proportion of cases that are associated with the exposure it is called the probability of causation or the assigned share of causation. This is clearly an error unless rare conditions are met, and the effects of the error are anywhere from very mild to moderate depending on the baseline probabilities.

\begin{tabular}{| c |c | c | c| }
\hline
 & Outcome & No Outcome & \\ \hline
Exposure & 30 & 20 & 50 \\ \hline
No Exposure & 10 & 40 & 50 \\ \hline
& 40 &  60 & 100 \\
\hline
\end{tabular}

AR = 30/50 – 10/50 = .4
AF = ((30/30+20) – (10/10+40))/(10/10+40) = 2/3
RR E+:E- = (30/30+20) / (10/10+40) = 3
Odds of O+ if E+ = 30/20 = 1.5
Odds of O+ if E- = 10/40 = .25
Odds ratio of E+ versus E- = (30*40)/(10*20) = 6

If the RR doubles to 6 by clockwise rotation to
>rbind(c(36, 14), c(6, 44))

the OR is 18.857

If the RR doubles to 6 because right shift to
>rbind(c(24, 26), c(4, 46)) 

then the OR is 10.615

PAR 	= 40/40+60 – 10/10+40 = .2					
PAF = 40/40+60 – 10/10+40 / 40/40+60 = 0.5
	= using RR (0.5).(3-1) / (0.5).(3-1)+1 = 0.5
	= using OR (0.5).(6-1) / (0.5).(6-1)+1 = 0.71

## Final caution 

Remember, kids: the assumption that PAF describes a causal relationship **might not be correct**: it might describe a confounder, or have an effect modification. 

# Probability and sampling

- Probability is always a _feature of populations_, not of data 

- the population is _connected to the data_ 
- by a probability  
- using assumptions, so a 
- _sample quantity is an estimator_ 
- the population value, that mythical value again, is the estimand 
## Kolmogorov's axioms 

1. Probability of something happening is 1 $P(\Omega) = 1$

2. Probability of nothing happening is 0 $P(E)\geq 0, P(E) \in R$

3. Probability of a thing is 1-its opposite 

4. Probability of at least one of some mutually exclusive things is the sum of their probabilities 

5. If event A implies event B then probability of A < probability of B 

6. The joint probability of any 2 events is the sum minus their intersection 
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
			
Probabilities in the same probability space are independent if $P(A \cap B) = P(A).P(B)$

The conditional probability is obtained by dividing a joint probability by a marginal probability, eg
$$P(x_3 | y_2) = \frac{p(x_3, y_2)}{p(y_2)}$$

## Bayes rule and diagnosis

Diagnostic likelihood ratio of a positive test (DLR+ve)
	$$\frac{P(+|D)}{P(+|D^{c})} = \frac{sensitivity}{(1-specificity)}$$

and DLR-ve
$$\frac{P(-|D)}{P(-|D^{c})} = \frac{(1-sensitivity)}{specificity}$$	

Because $P(D) = (1-P(D^{c}))$ the complementary Bayes rules for positive predictive value is
$$P(D|+) = \frac {P(+|D).P(D)} {P(+|D).P(D)  +  P(+|D^{c}).P(D^{c})}$$

		$$P(D^{c}|+) = \frac {P(+|D^{c}).P(D^{c})}{P(+|D).P(D)  +  P(+|D^{c}).P(D^{c})}$$

and dividing one into the other gives the posterior odds being equal to the diagnostic likelihood ratio times the prior odds:
$$\frac{P(D|+)}{P(D^{c}|+)} = \frac {P(+ | D)}{P(+ | D^{c})} \times \frac{P(D)}{P(D^{c})}$$

## Presneill

$P(B_{k}|A) = \frac{P(A|B_{k})}{\sum_{i=1}^n[P(A|B_{i}).P(B_{i})]}.P(B)$

## Chebyshev's inequality

The probability that a variable takes the value of k standard deviations from the mean is a maximum of 1/k squared, for any distribution where the mean has a single value:
$$P(abs{X - \mu} > k \sigma) = \frac{1}{k^2}$$

# Beyond subjectivity and objectivity

Gelman and Hennig at the RSS 2017 presented on the false dichotomy that hides bad behaviour by both sides.  They propose that Objectivity be replaced by transparency, consensus, impartiality and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. Together with stability, these make up a collection of virtues that they think is helpful in discussions of statistical foundations and practice.

# Exposure measurement

The true exposure is like True Grail, only seen by indirect evidence of its presence by an Instrument: this is the Measured Exposure.
Exposure measurement errors arises from the Instrument's design, its protocols, training or attention or malice or prejudice of those who use it; from the subjects in their memory, cyclic or random or circumstantial variability or recall; and from entering or coding or transforming data.
Bias can be caused by differential measurement exposure error.
If a parameter X is a proxy measure of the true parameter T, with a fixed bias b across any sample, and an additional random error E then $t_i = x_i + b + e_i$, where E(E)=0, E(X) = E(T)-b and so on. If the bias is not fixed, then the regression is not simple but multiple.  This can be hard to detect if not suspected on the basis of mechanistic understanding, which couples with unpredictable "test coverage" of regression models to reflect the true situation.
Non fixed bias is introduced by differential measurement error, among other things.
Precision is given by the correlation of T with X, the validity coefficient.  This all remains a bit theoretical unless the values of T can be known.
$\rho^2_{TX} = 1 - \frac{\sigma^2_E}{\sigma^2_X} = \frac{\sigma^2_T}{\sigma^2_X}$ , and assuming linear relationships for Y with both T and X,
if both $Y = \alpha_T + \beta_T T$ and $Y = \alpha_O + \beta_O T$ then $\beta_O = \rho^2_{TX} \beta_T$
but if X is a function of T rather than an addend, this doesn't hold.
	
This assumes a linear model, or rather two simultaneous models for T and X.   Logistic models for log(OR) can also be built, so that the observed odds ratio depends on the correlation of T and X, tending to 1 as the correlation tends to 0.
	$$OR_O = OR_T^{\rho^2_{TX}}$$
	
The correlation can be expressed separately as sensitivity and specificity of X for T.  The odds ratio or sign of the regression coefficient don't cross the null as long as (sens + spec) >= 1, or as long as an exposed case is classified as exposed with greater probability than an unexposed control is classified as exposed.

Scatter plots of data allow clues about linearity or remote or influential points.  

# Sampling

The study population is the population available for study.
A sampling frame is the entire extant list of possible units which could be sampled.

"Equal probability" selections (of each final stage unit): 

- population value is estimated by sample values (self-weighting)
eg Simple Random Sampling (and K&S alledges that Systematic Sampling is an EPS...?) 
- Probability proportional to size followed by numerical SRS (eg 4 at each stage 1) 
- SRS at stage 1 followed by proportional SRS (eg 20% of each stage 1)

Others need weighting at the analysis stage proportionate to their oversampling: 
- Stratified – sample within strata and add together 
- Hierarchical – Primary or First Stage (or Tier) or 1°, within which Second Stage (2°, etc) 
- "Probability Proportional to Size" then SRS with constant Tier 2 sample size 

If different sized 1° samples, gives equal probability of sampling each 2° unit. SRS then SRS with 2° being a constant sampling fraction of 1° if equally sized 1° samples
			
Balance covariates by

- stratification (usually logical) 
- blocking (see Fisher and Student's fight) 
- minimisation (re-read Senn 2004) 
- matching (by pairs, read Eldridge 2012).

Matching case-control by a variable allows effect modification by that variable to be estimated, and for ignoring the effect of the variable.  Can increase precision or efficiency; may add an unknown amount of control for unacknowledged confounders if those are correlated with the matching variable.

Matching eg by malaria rates last year, can cause problems. I intuit that if the matched variable is not the right one then no inferential efficiency is gained, eg if "rank of malaria incidence" is very different in the year of study then they're not matched by malaria incidence and random baseline variability may be considered to have been reduced when it actually hasn't.  So that's a problem like HTE or undermatched confounders. As it happens they're not very correlated at all, r=0.2 or so. Matching doesn't lose much power unless the matching and exposure variables are strongly correlated; but it can do it.  Also overmatching is logistically hard and increases concordant pairs.  And if the matched variable is on the causal pathway it underestimates the effect of exposure on outcome.

"Break-even values" for the correlation between matching and outcome variables for matching or stratification to be beneficial. 
Matched Odds ratios are derived from MH summary odds ratios where only discordant pairs count: if individually matched then each pair is a stratum, if the tabulation is at pair level then two boxes contain zeroes for each concordant pair.  Overall numbers of pairs are tabulated for each combination.
	
Randomisation is distinct from random sampling 
- "In a properly randomised trial any difference between the groups outside that imparted by the intervention are due to chance." 
- "Any difference between the trial groups should be due to the outcome." 
- True randomisation depends on randomness of sequence and allocation concealment. 

# Likelihood

A probability conditioned for a model; or a probability of a model given some data; the likelihood is numerically (and can be computationally) equal to a probability of the data given the model; eg a binomial and cauchy likelihood exist for the same data and the same parameter space.  The likelihood is calculated for each point in the parameter space and where it is maximal is the point best supported by the data, that is the point which makes the data most likely: the maximum likelihood estimate MLE.

Other points are divided by the MLE to yield the likelihood ratio, which rejects data but adds clarity and comparability.  If the likelihood and hence the likelihood ratio curves are normal, so log(likelihood ratio) is quadratic and so easily solved using the pattern $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$.  If it's not standard normal then it's not easy to do and iteration is probably as good as algebra.  The quadratic is chosen so that the curvature of the actual log likelihood and the quadratic fitted approximation are identical at the fit point, whether the MLE or the null, depending on whether the aim is to test or derive a confidence interval, or to determine the MLE.  Wald tests fit at the MLE.

Also above a LR of 0.1465 lies 95% of the likelihood curve and twice the log of the likelihood ratio is algebraically equivalent to the chi squared distribution, giving a chi squared statistic for every value of the parameter, given the data, under those assumptions.  The log(LR) at that point is -0.192, useful for the Gaussian assumption underlying the quadratic approximation: $log(LR) = -\frac{1}{2}\frac{\bar{x} - \mu}{\sigma \sqrt{n}}$.  The fit away from its fitted point is less good, so the choice is to fit at the MLE or at the null.
The quadratic is chosen to meet the log LR curve at the MLE and to have the same curvature:
	 $$\textrm{log(LR)} = - \frac{1}{2} (\frac{M - \pi_0}{S})^2$$ as for D failures in N trials D/N = M
	 
Fuckedly, London uses M not only for D/N but also for log(D/N-D).	$S^2 = M(1-M)$, $M = log(\frac{D}{N-D})$  and $S =\sqrt{\frac{1}{D} + \frac{1}{N-D}} = \sqrt{\frac{M(1-M)}{n}}$	$\frac{1}{S^2}$ is called the information of the data: the larger, the greater the curvature of log(LR), and the more precise the estimate and so on.	A supported range of parameter values with attendant likelihood over a certain cutoff likelihood can be calculated and as S is the standard error so the LR at the 95% CI limits is $e^{\frac{-1.96^2}{2}} = 0.1465$ if the likelihood curve is normal across the parameter space.
	
Use likelihood ratio statistic of the value of the log(LR) at the null value of the parameter (or for any 2 models where one is a restricted form of the other)

LRS = -2 log(LR) = -2(lognull – logMLE) = 2(logMLE - lognull) ~ χ2 df = (r-1).(c-1) 
or the Wald statistic of the fitted quadratic approximation to the log(LR) at the null
$LRS_{Wald} = (\frac{MLE}{S})^2$, and its square root if H0:log(LR)=0 is $z = \frac{MLE}{S}$
	
or the Score Test: alternative quadratic matching value, gradient and curvature at null, not MLE
Score test = $-2 \textrm{log(LR)}_{null_{quadfit}} = \frac{U^2}{V}$ where U=gradient and -V = curvature(fitted) at null from which $\chi^2_{MH} = \frac{U^2}{V}$ is derived for the Mantel Haenszel test, $\frac{score^2}{score \> variance}$

## Binomial likelihoods

are calculated so the most likely value of p is found for k successes in n trials
Likelihood = $\binom{n}{k} p^k (1 - p)^{n-k}$, approximated by $-\frac{1}{2}\frac{D/N - \pi}{S}$  with S set as $\sqrt{\frac{p(1-p)}{N}}$ by calculus to match the curvature of the actual likelihood at its maximum, being the sample standard error; the plot of which against p is identical to its ratio, whose log is approximated by the normal assumption:
		$\textrm{log(LR)} = - \frac{1}{2} (\frac{M - \pi_0}{S})^2$, making the Wald test $-2\textrm{log}(LR) = (\frac{M - \pi_0}{S})^2 \approx \chi^2, \textrm{df = 1}$

## Poisson likelihoods

are calculated so for best guesses at baseline λ0 and rate ratio θ for d0 and d1 events over times T0 and T1 
log Likelihood $L = (d_0 + d_1) log(\lambda_0) + d_1 log(\theta) - \lambda_0 T_0 - \theta \lambda_0 T_1 + \> constant$
which generates a surface maximal at $\lambda_0 = \frac{d_0 + d_1}{T_0 + T_1}$.  Substituting this into the above,
L is hence maximal at $λ0 = d_1 log (\frac{\theta T_1}{T_0}) - (d_0 + d_1) log (1 + \frac{\theta T_1}{T_0}) + \> constant$
which is the "profile log likelihood for θ";
its plot against θ is the same shape as its ratio-to-maximum against θ;
its plot against log(θ) is approximated by:
			$\textrm{log(LR)} = - \frac{1}{2} (\frac{MLE - \theta}{S})^2$ where S is the SE of θ
More complex likelihoods are fitted by iteration on the MLE, _eg_
taking nulls (L=MLE=0) for all *n* parameters, and working out, calculating gradient and curvature at each value to sketch the best approximation n-surface with its n-dimensional maximum being the improved estimate of MLE and repeating to convergence which doesn't work if the data are insufficient to estimate the number of parameters or if the profile log likelihoods are non-quadratic when eg Poisson, Logistic and Cox regression uses log transformations, 
or the similar Score Test of form $-2 \textrm{log}(LR) = \frac{U^2}{V}$ where U is the gradient (*aka* the Score) and V the negative of the curvature of a quadratic approximation to the likelihood fitted at the null (aka the Score Variance), not at the MLE.

The plausible values of all but the likelihood ratio test depend on the units of the fitted quadratic.  Also, the fitted values don't have a definite integral for anything above a quadratic equation and more complex likelihoods don't have such approximations.

In regression the LRS tests the joint null that all the variables equal their null values and tests any two models where one is a restricted form of the other.

# Robustness
Robust objects (estimators, statistics, models...) are ones which are changed little by perturbations in the data; good robust objects retain efficiency while being robust.  Efficient objects are ones which need few observations to attain a given performance.  Performance is the ability of an object to describe or predict reality, such as to describe variation reliably, reject a false claim or detect a true difference.

# Clusters

Used when there's a direct effect plus indirect (ecological, herd, cooperative, etc) effect on participants, or when it's difficult to allocate individuals. Indirect include, for infections, that which reduces Quantity (herd, reduced carriage, vector death, epidemic periodicity) or Quality (less or more virulent strains, resistance, non-reproducing strains) of infection or the Immunity of victim (immune recency, cross reaction, polyvalency, multi-hit, immune mediated damage). "Direct" + "indirect" = "total"; if all in a cluster participate the effect is "total", otherwise the "overall" effect is the weighted average of direct + indirect on participants and indirect on non-participants.  Individually allocated trials measure the Direct effect; cluster trials measure the Overall which depends on participation fraction and HTE combined with differential participation. 
- Second stage randomisation of individuals within clusters clarifies direct v indirect.
- Spin the bottle: random direction to walk from centre to border of cluster
- Used by the Expanded Programme on Immunisation, EPI of the WHO as 30 clusters of 7	randomly select units along the line of walk to be cluster centres
- clusters are "next nearest until quota of 7 children reached", no callbacks
- Not probability based ("not self weighting") and 
- In vaccine surveys mothers interviewed, Est is +- 10% at 95%CI assuming Design Effect 2
- Probability of choosing a child is awfully roughly $P_i = m \times \frac{M_i}{M} \times \frac{n}{N_i}$
- Where n is the very approximately equal number of children surveyed per cluster, Ni is total children in the ith cluster and others as below and $\hat Var (\hat R) = \frac{1}{m(m-1)} \sum_{i=1}^m{(y_i/n - \hat R)^2}$ so
- CI is $\hat R \pm t_{1-\alpha, m-1} \sqrt{\hat V(\hat R)}$ and the bias  is $\bar \rho - R = -\rho_{p, g}\sqrt{\frac{V(p)V(g)}{\bar g}}$ where
- g is the proportional change in population since the census, p is the true vaccinated proportion, rho is the correlation between p and g, C is the number of clusters in the whole population and R is the true ratio $R= \sum_{i=1}^C{\frac{g_i p_i}{C \bar g}}$.

Design effect for clusters all of the same size is $1 + ((b-1) \times ICC)$ where b is the number per cluster and ICC is the correlation coefficient within clusters, as an average across all the clusters.

## Compact segment sampling;

another cluster method.
Areas are initially chosen with PPS, eg from last census
The areas are divided into equal numbers of segments
Within each segment of a given area are an equal number of houses, sketched and recorded
A segment is randomly chosen from each area, and all houses in the segment are sampled
Probability of choosing a house in the ith segment and therefore a child is very roughly 
$P_i = \frac{m}{S_i} \times \frac{M_i}{M_{tot}}$ where
_m_ is the number of selected clusters, Si is the number of segments in the ith cluster, Mtot the census population of all clusters in the sampling frame, Mi census population in the ith cluster
IF population has changed uniformly and segments are exactly chosen, _eg_ vaccine coverage is estimated by a ratio of two random variables, number vaccinated and number chosen, each of which is weighted for the probability of being chosen, which therefore has to be known to estimate the true values.
$\hat{Var} (\hat R) = (\frac{m}{(m-1)\hat N ^2}) \sum_{i=1}^m {\frac{(y_i - n_i \hat R)^2}{P_i^2}}$

$\hat R = \frac{\sum_{i=1}^m{y_i / P_i}}{\sum_{i=1}^m{n_i / P_i}}$

IntraClass Correlation Coefficient (ICC or ρ) is Between Cluster Variance/Total Variance 
$\rho = \frac{\sigma_c^2}{\sigma_c^2 + \sigma_e^2}$, also $\rho = \frac{\sigma_c^2}{\pi (1-\pi)}$, or $\rho = \frac{\sigma^2_B}{\sigma^2_B + \sigma^2_W}$ 
where σc is the between cluster SD and  σe is everything else (here, within-cluster variation)
ICC can be used for means, proportions and counts but is not defined for rates.
k, the Between-Cluster Coefficient of Variation, is SD for cluster means/mean = $\frac{\sigma_c}{\bar x} = \frac{\sigma_c}{\pi}$ so
$$\rho = \frac{k^2 \pi}{1-\pi}$$

## Design Factor 

Design Factor is SE for design/SE under simple random, terminology analogous to Error Factor
Design Effect DEFF 
= Sample size / size by simple random sampling or Variance/Variance by SRS
= Design Factor squared if it's proportions (Also quoted as SE by design / SE by SRS!)
= 1 + (Number in each cluster-1) * rate of homogeneity = $1 + \rho(m - 1)$
DEFF on total N is $1+(\bar{m} - 1) \rho$ where m is number in a cluster or, if cluster sizes vary, $1+ \rho(\bar m(1 +(\frac{sd_m}{\bar m})^2) - 1)$ where $\frac{sd_m}{\bar m}$ is coefficient of variation of cluster size

Rate of homogeneity **roh** (here ρ) is the ICC for single stage clustering, and an equivalent ration of total variance for multi stage clustering.
		
k is estimated for power reasons from σc : see below or $m= \frac{N(1-\rho)}{(c - \rho N)}$ for cluster size given total N under simple random and number of clusters c. ICC can’t be used when the outcome is a rate per time; so k is used here. 
	
## Standard cluster sample size calculations

For two rates:	$n = \frac{(z_{1-\beta} + z_{1-\alpha/2})^2 . (\lambda_1 + \lambda_2)}{(\lambda_1 - \lambda_2)^2}$	
			$\textrm{As E}(s^2) = \lambda Av(1/y_j) +\sigma_c^2 = \lambda Av(1/y_j) + k^2\lambda^2\>\textrm{ so}$
			$\hat \sigma_c^2 = s^2 - r Av(1/y_j)$ and 
            $\hat k = \frac{\hat \sigma_c}{r}$
For two means:  		 
        $n = \frac{4(z_{1 - \alpha/2} + z_{1-\beta})^2}{d^2}, d = \frac{\delta}{\sigma}$
or
        $\frac{(u+v)^2(\sigma_1^2 + \sigma_2^2)}{\delta^2}$
        
or   

As $E(s^2) = \sigma^2 Av(1/n_{j}) + \sigma_c^2$, so $\hat \sigma_c^2 = s^2 - \hat \sigma^2 Av(1/n_j)$ across each jth cluster and	$\hat k=\frac{\hat \sigma_c}{\bar x}$	 where x-bar is the mean of all observations across all clusters combined.

For two proportions, $n = \frac{2[z_{1 - \alpha/2}\sqrt{2q(1-q)} + z_{1 - \beta}\sqrt{\pi_1(1-\pi_1) + \pi_2(1 - \pi_2)}]^2}{(\pi_1 - \pi_2)^2}$	, $q = \frac{\pi_1 + \pi_2}{2}$
            or 
            $n = \frac{[u\sqrt{\pi_1(1- \pi_1) + \pi_2(1 - \pi_2)} + v \sqrt {2\bar{\pi} (1-\bar{\pi})}]^2} {(\pi_1 - \pi_2)^2}$

As $E(s^2) = \pi (1 - \pi) Av(1/n_j) + \sigma_c^2$
so $\hat \sigma_c^2 = s^2 - p(1-p) Av(1/n_j)$
and $\hat k = \frac{\hat \sigma_c}{p}$
where p is the combined proportion across all clusters combined
These look simpler but sacrifice transparency if written for the number of clusters c
for rates $c=1 + f\frac{[\frac{\lambda_0 + \lambda_1}{y} + k^2(\lambda_0^2 + \lambda_1^2)]}{(\lambda_0 - \lambda_1)^2}$ or for proportions $c=1 + f\frac{[\frac{\pi_0(1-\pi_0) + \pi_1(1-\pi_1)}{m} + k^2(\pi_0^2 + \pi_1^2)]}{(\pi_0 - \pi_1)^2}$

where f is a factor combining zα and zβ, eg f=7.84 for power 0.8 and alpha 0.05, 10.5 for power 0.9 and alpha 0.05 and k is the assumed true coefficient of variation between clusters; y is the number of person-time follow up per cluster and m is the number of people per cluster, both of whcih are assumed identical.

If SRS but unequal randomisation ratios with size in each group n, then if the smaller group is n1, then alter the size of each group by $k= \frac{n_2}{n_1}$ and $n_1 = \frac{n(k+1)}{2k}$, which converges on $n_1 = \frac{n}{2}$ as the ratio k increases, at the cost of imprecision in estimates of the effect in group 1, and less information on rare events.

1 Was there an effect of treatment in this trial?
2 What was the average effect of treatment in this trial?
3 Was the treatment effect identical for all patients in the trial?
4 What was the effect of treatment for different subgroups of patients?
5 What will be the effect of treatment when used more generally (outside of the trial)?

Because few clusters tend to be randomised and the power calculations nastily fudged, baseline imbalance is common, with instability of the effect size estimates and hence increased false claims in both directions.  Balancing of baseline covariates is often thought to limit this.

Balance is achieved by matching (add 2 instead of 1 to the RHS of the simplifed equation, k is the average coefficient of variation only between matched pairs), stratifiction or restriction / constraint (reject all random allocations until one is generated with eg 10% difference in any covariate per arm). Among these only matching is random, depends on the matching variables being well chosen and doesn't permit analysis of the main effects of any matching variable.

The estimates are either calculated over the whole study, or combined from cluster-specific estimates.  This is not straighforward :
if equal weighting of cluster estimates is desired and the clusters were not randomly selected from a well defined target population then cluster estimates are better if they were randomly selected with PPS then the overall estimate is a consistent estimator of the true population value, and is of course easier but it doesn't allow simple t tests.

To derive the standard error the variance of the risk or, interchangeably, the rate ratio, for cluster specific estimates is estimated roughly by working in the log RR scale
$Var(\textrm{log} RR ) = Var(\textrm{log} R_1) + Var(\textrm{log} R_0) \approx \frac{Var(R_1)}{R_1^2} + \frac{Var(R_0)}{R_0^2}$, and $Var(R_1) \approx \frac{s_1^2}{c_1}$,
where s1 is the observed SD of the cluster specific estimates across the number c of clusters in arm 1 (the intervention, for example).
So the estimate accounting for clustering is $\textrm{log RR} \pm 1.96  \sqrt{Var(\textrm{log RR})}$.
A simple T or Wilcoxon rank sum test can be used for the summary estimates using these summary values.

## Cluster level analysis

Adjusted estimates of the effect of treatment allocation rely on the differences (residuals) between a model that accounts for all important-seeming covariates (that is, all except for cluster effects and the effect of treatment allocation).  These residuals will be randomly distributed across clusters under the null hypothesis, which is then tested by analysing them in place of the cluster level raw estimates.  For example, a poisson regression provides estimates; residuals are calculated as the ratios of rate in each cluster to the predicted rate for those individuals under the model; the mean and SD of residuals are calculated for each allocation group; these provide a T test (or equivalent) and the ratio of mean residuals is the estimate of effect size, with Var(RR) calculated analogously to the above, this time for the ratio of residuals.

## Individual level analysis
With linear models, assume the data are all iid but that there is a cluster effect (data are still independent within a cluster but are all drawn from the same independent distribution).  Use poisson for rates and a gamma error, giving a negative binomial likelihood.  Normal for risk and a normal error, giving a normal likelihood.  Binomial for logit and  normal error, giving a non analytic likelihood which is converted to a log which is approximated by a log normal using the minimum of a quadratic equation to pass through a certain number of points representing the data.  This last might be unstable on changing the number of points to fit, if the approximation likelihood is not a good reflection of the whole data.  This is revealed by a quadrature check with >1% difference in likelihood on changing the number of quadrature points; this is actually probably not necessary now with adaptive quadrature being used at least in Stata.  If that's the case:

## Generalised estimating equations

GEE assume that any two points from separate clusters are uncorrelated, but within a cluster all points are identically correlated with each other, represented by an exchangeable correlation matrix derived from the data.  This estimate of rho is the primary output; then the regression coefficients and their standard errrors are estimated from it in turn.  The regression coefficient is a population average odds ratio, assuming no cluster level random effect but only correlation... I thiink.  Overall the buzz words are exchangeable correlation matrices and robust standard errors, with a Wald test $z = \frac{\beta}{\textrm{SE}(\beta)}$.

## Cluster level t test

Cluster level analysis with a t test is robust and has good coverage even with low cluster numbers (15 or fewer) but can't adjust well.  Linear models are efficient, GEE robust.  With binary data they differ: random effects models estimate cluster specific - averaged odds ratios and GEE estimate population averaged odds ratios.
With 15 clusters in total you can estimate continuous or rate coefficients, with 30 their standard error; or 30 and 50 respectively for binary outcomes.

GEE inflate Type 1 error if few clusters; and vague hand waving about linear modelling's distributional assumptions not being easy to support if there are few clusters.

Moving field: Bayesian, restricted maximum likelihood.

# Distributions

Probability Mass Function is >0 everywhere and the sum of the individual probabilities of possible values adds up to 1
	
Probability Density Function is >0 everywhere and has area =1 under it

The area under the pdf corresponds to the probability for those values of that random variable
The probability of a single specific number is 0
Integrating the CDF yields the CDF
Cumulative Distribution Function is the probability that a random variable takes the value ≤x
	$F(x) = P(X \leqslant x)$
Integrating the CDF at its limits yields an area of probability
Survivor function is S(x) = 1-F(x)
Quantiles: the αth quantile of a distribution function F(x) is the point such that $F(x_{\alpha}) = \alpha$

# Variance and error terms

For categorical data, variance is p(1-p) and sd therefore $\sqrt{p(1-p)}$ and standard error $\sqrt{\frac{p(1-p)}{n}}$
standard error for the difference in two proportions is $\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$
		
s.e.m., eg for the representativeness of a sample mean of its normally distributed population, is
	$$\frac{s}{\sqrt{n}} = \sqrt{\frac{s^2}{n}}$$
			
and for 2 independent samples is
			$$\textrm{sem} = \sqrt{\frac{s_{1}^2}{n_{1}} + \frac{s_{0}^2}{n_{0}}}$$
	If it's reasonable to assume alike variance, such as for example randomised samples,
			$$\textrm{sem} = s_{p}\sqrt{\frac{1}{n_{1}}+\frac{1}{n_{0}}}$$
	for which the sample size-weighted common estimate of σ assuming alike variance is
	$$s_{p}= \sqrt{\frac{(n_{1}-1).s_{1}^2 + (n_{0}-1)s_{0}^2}{(n_{1} + n_{0} - 2)}}$$
			
paired samples are modified if they are alike by rho
			$$s = \sqrt{(s_{1}^2 + s_{2}^2 - 2 \rho . s_{1} . s_{2} )}$$
which simplifies to $\sqrt{(s_{1}^2+s_{2}^2)}$ if the "pairs" are nothing of the kind, and not correlated at all so rho = 0
	$$t = \frac{\bar X_{1} - \bar X_{0}}{sem}$$

and if variances are unequal the distribution of the difference of means is _NOT t_! It's something close to t that is well approximated by changing the degrees of freedom by
	$df = \frac{(\frac{S_{x}^2}{n_{x}} + \frac{S_{y}^2}{n_{y}})^2}	{(\frac{(\frac{S_{x}^2}{n_{x}})^2}{(n_{x}-1)}) + (\frac{(\frac{S_{y}^2}{n_{y}})^2}{(n_{y}-1)})}$
which may be non integer
			  
This altered t distribution is used to create the quantiles for confidence intervals etc.
	$eg \bar Y - \bar X \pm t_{df} \times \sqrt{\frac{s_{x}^2}{n_{x}} + \frac{s_{y}^2}{n_{y}}}$

and for proportions

standard error for a confidence interval:
	$\sqrt{\frac{p_{1}.(p_{1}-1)}{n_{1}} + \frac{p_{2}.(p_{2}-1)}{n_{2}}}$
standard error for a hypothesis test assumes both are from the same π so uses the common proportion (all +ve / all n) before repeating the above formula:
	$\sqrt{p(1-p).(\frac{1}{n_{1}} + \frac{1}{n_{2}})}$

## Bernoulli mass function
$p(X=x) = p^x.(1-p)^{1-x}$
$p(X=k) = \binom {n}{k} p^k(1-p)^{n-k}$ where $\binom {n}{k} = \frac {n!}{k!(n-k)!}$
(remembering that $\binom {n}{0} = \binom {n}{n} = 1$)

## Normal distribution

Any normal distribution is a multiple of σ and an addend of μ from Z, the
Standard Normal Distribution: 
	$$Z = \frac{x - \mu}{\sigma},  N \sim (0,1)$$
So if X is a normally distributed variable with mean μ and sd σ then $X = \mu + \sigma Z, N \sim (\mu,\sigma^2)$
	
65%, 95% and 99% of the SND lie within 1, 2 and 3 SD of μ
	
## Multinomial distribution is

$$P(x_{1},\dots x_{n};k) = \frac{k!}{x_{1}!x_{2}!\dots {x_{n}!}}.p_{1}^{x_{1}} \dots p_{n}^{x_{n}}$$

of which a special case is the binomial distribution
		$P(x, k) = \frac{k!}{x!(k-x)!} \times p^{x}.q^{k-x}$
		
T distribution is descended from the standard normal, and assumed to be symmetric and centred on 0 so with only 1 parameter, the degrees of freedom.  Skew can be exponentially transformed or another distribution used.


## Poisson has MEAN AND VARIANCE EQUAL to λ

which is events / time, $\lambda = \frac{D}{t}$
and noting $1-x \approx e^{-x}$ so $(1 - x)^n \approx e^{-nx}$ its probability mass function is:
	$P(X=x;\lambda) = \frac{\lambda^x.e^{-\lambda}}{x!}$
where x is a non negative integer and $\textrm{Risk} = 1 - e^{-\lambda t}$

Standard Error of a rate is SE(number of events)/person-time at risk
	$SE_{events} = \sqrt{D}, SE_{risk} = \frac{\sqrt D}{t}$
and $\lambda = \frac{D}{t}$ so $SE = \sqrt{\frac{\lambda}{t}}$
SE for log rate is $\frac{1}{\sqrt{D}}$, CI is $\lambda^{\times}_{\div} EF$, EF=$e^{\frac{1.96}{\sqrt D}}$ ; ratio or difference = ${\lambda_0-\lambda_1}^{\times}_{\div} e^{1.96 \sqrt{\frac{1}{D_0} + \frac{1}{D_1}}}$

Used for example in modelling especially unbounded count data,
$X \sim Poisson (\lambda t)$ where $\lambda = E[X/t]$
approximating the binomial for large n and small p; let λ=np 
modelling contingency tables. In censoring survival models (as an "unbounded" problem).
NHST is the Wald against H0: λ0 = λ1, $z = \frac{\textrm{log}(rate \> ratio)}{s.e. \textrm{log}(rate \> ratio)}$


## Poisson regression 

This is very similarly shaped to other regressions: rate = constant x timeband x exposure.  Under a poisson distribution the lambda is equal across strata; if it's not and especially if there's a linear trend, there is overdispersion.  If overdispersion is suspected by qualitative examination of rates, or by the ratio of LRS:df being greater than 1, then ~~an adult must be asked for help~~ a value (constant or distributed) can be added to the regression model to account for the departure from poisson.  The value is called the frailty and the resulting model is a negative binomial model.

## Cox regression

limits each timeband to that containing one event (call it a timeclick).  So rate = Changing baseline * exposure: the model assumes the exposure effect is constant (the proportional hazard assumption).  Test this graphically or by taking logs of the cumulative hazard so that the "Nelson-Aalen plot" of
$\sum \lambda_{ti} = \sum \lambda_{t0} \times \theta_i$ is parallel against time, and test by fitting interaction terms of time (by arbitrary epochs or as a superimposed linear trend) and using LRT or something.

## Multivariate distributions

MV norm has mean as a vector, sigma is the variance / covariance matrix

These have parameters of location, scale and skew. Normal location is the mean, t location is the non centrality parameter which are a scalar for univariate and an n-element vector for n-dimensional distributions (eg an n-variate distribution of densities).  Scale for a univariate is the variance for univariate normal and t distributions, but is a dispersion matrix sigma of n by n elements for for n-variate distributions (it's the variance / covariance matrix). The skew parameter is a vector of length n for n-variate distributions.

MV t has degrees of freedom usually presumed identical across al variates, a parameter for centrality delta, and a variance / covariance matrix sigma

density by `dmvt(x, delta=c(i,j)`, sigma= `as.matrix(r, t, y, u), df=df, log=T)`

Bivariate has delta=0 if standard and sigma=diag(2), ie rbind(c(2, 0, 0, 2))

#Asymptotics

Really very useful topic: the behaviour of functions as the size nears infinity, in this case the behaviour of estimators as the sample size approaches infinity; eg
>Strong Law of Large Numbers: 

sample averages converge on the population average as n increases.

>Central Limit Theorem: 

"the distribution of averages of i.i.d variables (properly normalised)	becomes that of a standard normal as the sample size increases"

So the average of samples, and their variance, converge on population values: a sample is Consistent if it does this, with its estimate converging on the estimated value.

For consistent samples, the mean, sd and the like are all consistent.
- The distribution also becomes more normal-like as n rises.
- 95% of 95% confidence intervals contain μ; except see below...

The CLT doesn't guarantee that a given n is large enough
- sample proportions don't converge very fast on the population proportion, so
- 95% confidence intervals don't always contain the true value 95% of the time, but
- somewhat less.

"Exact" estimators eg binom.test in R, don't use the central limit theorem and are	computationally intensive and while they guarantee 95% "coverage" of the true value are a little conservative. Poisson also don't converge well especially for low p: again, poisson.test is exact.

Sample size increases precision rather than the probability that its coverage includes the true value.

The estimator is $\hat{p} \pm z_{(1- \frac{\alpha}{2})} \sqrt{\frac{p(1-p)}{n}}$
Wald or Agresti-Coull estimators (preferred) work best at p=0.5

- Wald :	$95\% CI = \hat{p} \pm \frac{1}{\sqrt{n}}$, more generally an estimate $\pm$ its standard error:
$$\hat p \pm z_{\alpha / 2} . \sqrt{{\hat p \hat q} {n}}$$
				
- Agresti-Coull adjusts n by z^2^ so if X successes in n is the proportion p, and simplifying z of 1.96 as 2, adds 2 successes and 2 failures: so 
			$\tilde{n} = n + z^2, \> \tilde{p} = \frac{1}{\tilde{n}}(X + \frac{1}{2} z^2)$
			and CI for z = $\tilde{p} \pm z_{(1- \frac{\alpha}{2})} \sqrt{\frac{\tilde{p}(1-\tilde{p})}{\tilde{n}}}$

# Correlation

categorical variance is by tabulation, eg mean pair agreement or Observed Agreement OA:
					
or Kappa statistic for excess agreement over max chance agreement $\kappa = \frac{OA-CA}{1-CA}$,	

(where CA is $\frac {E[a]+E[d]}{a+b+c+d}$ and $E[a] = \frac{\textrm{Row total} \times \textrm{Column total}}{\textrm{Grand total}}$  as in the χ2 )

continuous is partitioned into systematic and random error eg by ANOVA;
their ratio is the Intraclass Correlation Coefficient, aka reliability coefficient
_eg_ by Pearson's correlation of product-moment where for a population
	$\rho (X, Y) = \frac{cov (X, Y)}{\sigma_{X}.\sigma_{Y}}$, where 
	$cov (X, Y) = E[(X-\mu_{X})(Y-\mu_{Y})]$
and using the identities
	$\sigma_{X}^2 = E[(X- E[X])^2] = E[X]^2 - E[X^2]$ and 
					  
	$E[(X-\mu_{X}).(Y-\mu_{Y})] = E[(X-E[X]).(Y-E[Y])]$, so
	$\rho_{(X, Y)} = \frac{E[XY]-E[X]E[Y]} {\sqrt{E[X]^2 - E[X^2]} \sqrt{E[Y]^2-E[Y^2]} }$
					

then the sample correlation r, assuming
    - a normal distribution of y for each xi 
    - a uniform variance for y across all x
    - y is monotonic on x

- is the value that describes the closeness of all points to a linear relationship
- is useful for a description of the scatter about the least squares linear relationship
- is the number of standard deviations that y changes for a 1 SD change in x.

Given by Pearson's Correlation Coefficient:

					$r = r_{xy} = \frac{\sum_{i=1}^n (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{\sum_{i=1}^n (x_{i} - \bar{x})^2}   \sqrt{\sum_{i=1}^n (y_{i} - \bar{y})^2}}$ ,	and

can also be quoted as the mean of the standard scores of x and y:
$r = r_{xy} = \frac{1}{n-1} \sum_{i=1}^n (\frac{x_{i} - \bar{x}}{s_{x}})(\frac{y_{i} - \bar{y}}{s_{y}})$	 ,	and	$\frac{\sum x_i y_i - n \bar x \bar y}{ns_x s_y}$  
					= $\frac{n \sum x_i y_i - \sum x_i \sum y_i}{\sqrt{n \sum x_i^2 - (\sum x_i)^2} \sqrt{n \sum y_i^2 - (\sum y_i)^2}}$, or	
					$r_{xy} = \frac{\sum x_i y_i - \frac{1}{n} \sum x_i \sum y_i}{\sqrt{\sum x_i^2 - \frac{1}{n}(\sum x_i)^2} \sqrt{\sum y_i^2 - \frac{1}{n}(\sum y_i)^2}}$
	
r squared explains the proportion of variance that is explained by the straight line,
For that straight line the regression coefficient for the straight line $y = \beta_0 x + \beta_1 x +\gamma x + \epsilon$ is
$\beta_1 = \frac{\sum(x-\bar x)(y-\bar y)}{\sum(x- \bar x)^2}$
					 or $\hat \beta_1 = Cor(Y, X) \frac{sd_y}{sd_x}$
					 
And Spearman's is Pearson's rho of the *ranks* of the values, rather than the values themselves.

# Hypothesis Testing

The confidence interval is the set of observations for which H0 is rejected, that is an inversion of the alpha level under H0 (eg for the Binomial rather than use an asymptotic approximation to the CLT for 0.95 as the Wald interval (too narrow), this can be directly calculated as an exact interval, the Clopper-Pearson interval (too wide) or another procedure the Wilson Score method).
Often overlooked is that evidence for a hypothesis is always relative to that for another hypothesis; often it's the same evidence.

Correlation of parameters within a subgroup implies that, when viewed at the level of the population, there is some component of variation that is attributable to membership of that subgroup.  A neater way to say this is that within cluster correlation measures the same phenomenon as between cluster variation.  The information is therefore reduced in comparison to i.i.d observations.
Robust standard errors, GEE and multilevel modelling are ways to address this.

## Robust standard errors

Robust variance is proportional to the sum of the squared residuals (residuals being the difference between the observed values and those predicted by the model).  They can be calculated individually if the data are i.i.d or calculated for each cluster then summed, where there is correlation.  This still assumes independence between clusters.

Robust standard errors do not affect the calculation of the maximum likelihood and so a LRS and test are not valid and the MLE is not altered.  A quadratic approximation is not used so the standard errors are not excessively narrow; the cluster level residuals are used to generate the variance instead.  "Robust SE are correct provided the model is correct" and there are >30 clusters.

## Generalised Estimating Equations

GEE take account of correlation in the calculation of the effect estimate as well as the standard error.  The correlation matrix structure is assumed: independent (iid), exchangeable (observations within a cluster are equally correlated and are not correlated with those outside the cluster) or autocorrelated (the same observation at different times, for example).

Exchangeable correlation is the usual GEE setting, where the effect estimate is a weighted combination of the effect estimates in each cluster and the standard error is derived from the residuals outside the model.  A population average effect is estimated: the average odds of an outcome among those exposed, divided by the average odds among those not exposed.

# Multilevel Modelling (Random Effects models)

The effect of cluster, whether innate, unmeasured confounder or a true mechanistic effect, is represented in the regression model by a separate term for each cluster: this is rendered tractable by assuming a stochastic distribution of the size of these terms rather than attempting to measure or specify the cluster effect for each cluster (hence "random effects"):

The log odds of the jth person in the ith cluster is then given by, where for example ui ~ N(0, σu2) and σu2 is estimated as part of the model building.  The same information is imparted by estimating rho, the within cluster (also called "intra-class") correlation coefficient.  A cluster specific effect is estimated: the odds of an individual having the outcome if exposed, divided by the odds if not exposed. 

The intention is to derive a model that provides a fully specified likelihood: assuming that it has done so the LRT is appropriate.  That likelihood becomes complicated easily: a bivariate normal distribution for the likelihood is produced for a normal error plus normally distributed cluster effect, which has an algebraic solution, as does the combination of poisson error and gamma cluster effects to produce a negative binomial; but the combination of binomial error and normal cluster effects, or poisson error and normal cluster effects, produce mixture distributions without roots.  The reliability of estimates needs to be checked.

Power
- is 1-beta and depends on the assumptions of the population structure
- is a function that depends on the value of mu0
- the distributions of the observation under H0 versus Ha hypotheses are calculated
- then a line is drawn across the null for alpha;
- at that line the extreme tail under Ha is 1-β = P	
	
If testing Ha: μ > μ0, $1 - \beta = P (\bar X > \mu_{0} +z_{1-\alpha} \frac{\sigma}{\sqrt{n}}; \mu = \mu_{a})$ , where $\bar X \sim N(\mu_a, \sigma^2/n)$ and μ0, α are known.  If 3 of the 4 unknowns μa, σ, n, β are specified the last can be calculated
	
If assuming only the noncentral t distribution then power is $P = (\frac {\bar X - \mu_{0}} {s / \sqrt{n}} > t_{1-\alpha, n-1}; \mu = \mu_{a})$ ; note P=α at μa=μ0 ;
On the other hand power depends only on the difference in means divided by S.E.M $\frac {\sqrt{n}(\mu_{a} - \mu_{0})}{\sigma}$, and the effect size is dimensionless and can be used across contexts a bit.

# Survival and logrank test

Kaplan Meier survival probability is the cumulative probability of an entity that is initially at risk surviving subsequent epochs until the one in question, each epoch being "data-defined" and usually as containing one event:
as $$P_{event} = \frac{n_{events}}{n_{at-risk}} – (0.5n_{censored})$$
if the censoring time is not defined for the interval
P(Si) = (1-Pevent1).(1-Pevent2)...(1-Peventi)
		$P(S_x) = \prod_{i=1}^x (1-\frac{\textrm{events}}{\textrm{at risk} - 0.5 \times \textrm{censored}})_i$ , or $\widehat {S}(t)=\prod \limits_{i:\>t_{i}\leq t}\left(1-{\frac {d_{i}}{n_{i}}}\right),$
A good KM curve has ticks on the line for censoring occurrences and nat-risk below the line
Log Rank test is a Chi Squared, with Observed totals and Expected E = (row total x column total)/(grand total). Summed across all intervals for each group, and added, giving the statistic:
		$\chi^2_{logrank} = \frac{(O_1 - E_1)^2}{E_1} + \frac{(O_2 - E_2)^2}{E_2}$ , df=1 or $Z=\frac  {\sum _{{j=1}}^{J}(O_{1j}-E_{1j})}{\sqrt{\sum _{j=1}^J V_j}}$
		Not sure whether you could substitute P(Sx) times number at risk at t=0 for E.

The hazard function h(t) is the failure rate summed over ever smaller time intervals, which may be more than 1, hence is not a probability.  This depends on a failure function, F(t), which is a probability, being a cumulative distribution function.
The P(Si) above is equivalent to the reliability function R(t), also called the survival function S(t), the probability of "no failure" as a function of time.  R(t) = 1 – F(t).
Cumulative failure as a function of time, F(t) is the integral over time of the probability density of failure as a function of time, f(t).

$\tilde{H} (t) = \sum_{{t_{i}\leq t}}\frac{d_{i}}{n_{i}}$

h(t) = f(t) / 1-F(t) = f(t) / R(t).
$h(t) = \lim _{\Delta t\to 0}{\frac {R(t)-R(t+\Delta t)}{\Delta t\cdot R(t)}}$, which does not have to be parametric as long as there is a definite cumulative distribution.

If the failure density is modelled as exponential (ie Poisson(1), that is the time until first failure is Poisson distributed and other failures are presumed to be independent in every case including those already failed) then the familiar result is that $f(t) = \lambda^1 e^{-\lambda t}$ so $F(t) = \int_0^t \lambda e^{-\lambda t} dt$ then the hazard function h(t) is
	$h(t) = \frac{f(t)}{R(t)} = \frac{\lambda e^{- \lambda t}}{e^{- \lambda t}} = \lambda$, which is time-blind / memoryless / constant with respect to time
Assuming, then, that the hazard function is constant and the cumulative hazard rises linearly:
- Risk  = events/number at risk; 
- hazard = (events/time at risk in the limit as t goes to 0) = lambda; 
- survival as a function of time S(t) = product of 1-risk across times; cumulative hazard = sum of risks for all times (Nelson-Aalen estimate of the cumulative hazard).
    
The log of the hazard ratios for two exposures is log(HR1) – log(HR2) = log(constant)
H(t) = -log(S(T)) or S(t) = e^(-H(t)) 
S(t) = e^(-lambda t)
H(t) = lambda t
Risk up to time t = 1- e^(-lambda t)
Average survival time = 1/lambda

ARIMA
The error term $\epsilon$ in the usual linear model $Y_i = \alpha + \beta X_i + \epsilon_i$ is assumed to be composed of errors which are independent, identically distributed (usually normally distributed with common variance). This is the definition of white noise.

If the value of a variable at time t is either static or at least predicted mainly by its value at time t-1, then it's a time series. Some function of time $\phi X_t$, added to an error term, gives the Autoregression formula:
$X_t = \phi X_{t-1} + \epsilon_t$

These assume errors are white noise; but usually errors are correlated with the previous error, just as the underlying true value is autocorrelated.  So now the model describes an autoreggressive process with autocorrelated errors:
$\epsilon_t = W_t + \theta W_{t-1}$

The autocorrelation magnitude can be estimated by comparing values that are a constant distance apart; lag 1 correlation is contiguous pairs, lag 2 is pairs separated by one intervening variable and so on. A series is stationary over a time, if the mean over that time is always constant. Some fiddling can make it stationary over a trend (so if there is an underlying spurious rise in price due to inflation). Examples of stationary processes are linear predictors where $X_t$ is stationary or a random walk, where W is white noise added to the last value of X to produce the current value of X:
$X_t = X_{t-1} + W_i$
This is demonstrated by differencing in R, eg for the default lag 1
```
diff(x, lag = 1)
```
	
# Multiple testing

\begin{tabular}{| l | l | l | l | }
\hline
& $\beta=0$ & $\beta \neq 0$ & Hypotheses \\ \hline
Claim $\beta = 0$ & U & T & m-R \\ \hline
Claim $\beta \neq 0$ & V & S & R \\ \hline
Claims & $m_{0}$ &  $m-m_{0}$ & m \\
\hline
\end{tabular}

Control of false positives involves an error measure and a correction for it.  There are many  such systems, their use depending on the needs of the end user of the data.

False positive rate $E[\frac{V}{m_{0}}]$ is the rate at which false results are called positive.
Family wise error rate $P(V \geq 1)$ which converges to 1 for multiple tests even as alpha is fixed, controlled by 
- Bonferroni: $\alpha_{fwer} = \frac{\alpha} {m}$ (very conservative) or calculate p-values, order them as P1 ... Pm with their null hypotheses H1 ... Hm and either
- (Holm, stable) let *R* be the smallest *k* such that $P_{(k)} > \frac{\alpha}{m + 1 - k}$, reject H1 to HR-1 or
- (Hochberg, powerful, assumes positive dependence eg in reusing data) let *R* be the largest *k* such that $P_{(k)} \leq \frac{\alpha}{m + 1 - k}$, reject H1 to HR 

False discovery rate is the rate at which claims of significance are false. Controlled so that $\textrm{E}(\frac{V}{R})$ is below a chosen threshold q:
Benjamini-Hochberg order P1 ... Pm with their null hypotheses H1 ... Hm and for the largest *k* such that $P_{k} \leq \alpha. \frac{k}{m}$, reject Hi where i= 1 ... k which is useful because $E[Q] \leq \frac{m_0}{m} \alpha \leq \alpha$
Or you could report "adjusted p-values" which are no longer p-values
eg to control FWER for P1 ... Pm take $P_{i}^{fwer} = \max{m \times P_{i}, 1}$ and call each Pi<α significant

Expected values
	- are properties of distributions, as are variances of those distributions
	- the population value is estimated by the relevant sample value
	
E(x) is linear, so E(cx) = E.c(x) and E(aX+bY) = a.E(X)+b.E(Y)
The centre of mass from a probability mass function of discrete data gives that expected value
	$E[X] = \sum_{x} {x.p(x)}$, extended to a sample mean where $\bar{X} = \sum_{i=0}^{n} x_{i}.p(x_{i})$, 
	where p is independent and identically distributed and is equal to $p = \frac{1}{n}$ for every x.
	
For continuous variables it's an area under the function t.f(t) where f(t) is the variable's PDF
    $E[X] = \int t.f(t)$
	
Given a normally distributed continuous variable X the population variance is 
$Var[X] = E([X- \mu]^2)  = E[X^2] - E[X]^2$
	
And because binary categorical random variables have expected value
	$E[\bar{X}] = (1-p).x^{c} + p.x$
then if x is coded as 0 if a variable is not present, eg tails, and 1 if present, eg heads, so
		$E[\bar{X}] = (1-p) \times 0 + p \times 1 = p$	and
		$E[X^2] = (1-p) \times 0^2 + p \times 1^2 = p$ then
		$E[Var(X)] = p - p^2 = p(1-p)$	and also from the definition of Var[X]
		$Var[aX] = a^2.Var[X]$
	 

The sample variance is sort of analogous as $s^2 = \frac{ \sum_{i=1} (X_{i} - \bar{X})^2}{n-1}$ and $\frac {\sum x_i^2 - \frac{(\sum x_i)^2}{n}}{n-1}$

`r newthought('THE SAMPLE VARIANCE ALSO HAS A DISTRIBUTION')` relating to the population of sample variances from which it is drawn: it is an unbiased estimator, meaning the mean of sample variances is an estimate of the population variance; and the variance of sample means is an estimate of the population variance, scaled for the sample size.  I think this is incredibly important.
The variance of the sample mean is the population variance divided by n
		 $Var[\bar{X}] = \frac{\sigma^2}{n}$, so s.e.m. = $\sigma \sqrt \frac{1}{n}$, because
		 $Var[\bar X]= Var[\frac{1}{n}.\sum X_{i}] = (\frac{1}{n^2}).Var[\sum{X_{i}}] = (\frac{1}{n^2}).\sum \sigma^2 = \frac{\sigma^2}{n}$

The SE for the mean of a log transformed variable, because
		 $log(X) \simeq log(\mu) + (X - \mu)(log'(\mu))$ and $log'(\mu) = \frac{1}{\mu}$, is
		 $SE(log(X)) \simeq SE(X) log'(\mu) = \frac{SE(X)}{\mu}$
		 
For log proportions, using $\frac{SE(X)}{\mu}$, $SE(log (p)) \simeq \frac{\sqrt{\frac{(p(1-p)}{n}})}{d/n}  = \sqrt{\frac{1}{d} - \frac{1}{n}}$ so 
	for log risk ratio, $RR = \frac{p_1}{p_2}$ so $\textrm{log(RR) = log}(p_1) - \textrm{log}(p_2)$ and
	$\textrm{SE(log(RR))} = \sqrt{Var(\textrm{log}(p_1)) + Var(\textrm{log}(p_2))} = \sqrt{\frac{1}{d_1} - \frac{1}{n_1} + \frac{1}{d_2} - \frac{1}{n_2}}$
	
For log rates $\textrm{SE(log}(\lambda)) = \frac{1}{\sqrt{D}}$
 - The log of a product is the sum of the logs
 - The sum of the logs is the log of the products
 - The log of a quotient is the difference of the logs
 - The difference of the logs is the log of the quotient
 - The exponent on the argument is the coefficient of the log
 - The coefficient of the log is the exponent on the argument 

# Bootstraps 

Efron and Tibshinari.
Permits confidence intervals without very complex maths.
Approximate the sampling distribution of a statistic using the distribution implied by the data.
Use the data you have and sample within it, with replacement, B times.
Calculate the statistic you need to estimate for each new sample you have made.
The resampling distribution non-parametrically approximates the population distribution, so the standard deviation of it implies the standard error of the median, confidence intervals can be generated etc.
Bias Corrected and Accelerated (BCA) interval performs MUCH better than the raw CI; use bootstrap package to construct it.

## Permutation tests
Powerful and manifold: eg Rank sum test permutes ranks for the rank sum, Fisher's Exact test permutes binary groups for a hypergeometric probability, an ordinary permutation test permutes the raw data.
Randomisation tests eg if matched data have differences with signs randomly reassigned the signed rank test is the result; or a regressor of interest might be permuted for regression testing.
When grouped / stratified data are compared the labels are irrelevant for forming H0. 
So take grouped data, calculate some comparative statistic.
Reassort the group labels randomly across the data; calculate a statistic for each of these random bins.
Repeat many times; this results in a distribution of statistics 
Observe number of times these differences are more extreme than the initially observed data.

# Paired (matched) data

If method A and method B are used on the same sample the discordance (r, s) is informative:

\begin{tabular}{| c |c | c | c| }
\hline
& $+_{B}$ & $-_{B}$ & \\ \hline
$+_{A}$ &k & r & k+r \\ \hline
$-_{A}$ & s & m & s+m \\ \hline
& &  & N \\
\hline
\end{tabular}

This is also used in matched rather than paired data, where one can imagine ρ being quite important.

Odds ratio is r/s, the ratio of discordance between the tests, and error factor is $e^{z_{0.05} \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$ so OR with 95% CI is $\frac{r}{s} \times e^{1.96 \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$ 	to	 $\frac{r}{s} / e^{1.96 \times \sqrt{\frac{1}{r} + \frac{1}{s}}}$

The discordant proportion is $\frac{r+s}{n}$ and its standard error is  $\frac{\sqrt{r+s}}{n}$

So 95% CI for an estimate of the proportional discordance is $\frac{r+s}{n} \pm 1.96 \frac{\sqrt{r+s}}{n}$ 

Paired categorical data from the 2x2 agreement table tested by McNemar's test:
$$\chi^2_{pair} = \frac{(r-s)^2}{r+s}$$
or with the continuity correction $\frac{(|r-s|-1)^2}{r+s}, \nu = 1$, or by MHχ2 with pairs as strata or an exact binomial test if discordant cells <20.

## Rate data
The rate, call it λ, is estimated by $\hat \lambda$, usually Events / Total time at risk $\hat \lambda = \frac{D}{t}$
S.E. for the estimate is $\sqrt{\frac{\hat \lambda}{t}}$ so 95% CI is $\lambda = \hat \lambda \pm 1.96 \sqrt{\frac{\hat \lambda}{t}}$
Rate differences are $(\lambda_1 - \lambda_2) \pm 1.96 \sqrt{\frac{\lambda_1}{t_1} - \frac{\lambda_2}{t_2}}$
Expected number of events e1 is $E(D_1) = \frac{D t_1}{t} = \lambda t$
		
Variance for e is $V(e) = \frac{Dt t_{2}}{t^2}$
		
And χ2 is $\chi ^2 = \frac{(|D_1 - E(D_1)|-0.5)^2}{V(D_1)} \textrm{, df=1}$, $H_0 = \lambda_1 - \lambda_2 = 0$ or equivalently that $\frac{\lambda_1}{\lambda_2} = 1$

χ2 for trend, where x  is the group score or level, is 	 $$\frac{(\sum D_i x_i - \lambda \sum t_i x_i)^2}{\lambda^2 (t \sum t_i x_i^2 - (\sum t_i x_i)^2)}$$

# Non parametric equivalents

## Wilcoxon Signed Rank

for paired samples or one sample from a hypothesised median. Assumes symmetrical distribution calculate the differences (of each value from the median, or across each pair): N is total nonzero differences.

Assign ranks to the absolute values of differences. Sum the ranks of positive differences, sum the ranks of negative differences.

The null hypothesis is that T=sum of all ranks divided by 2. Call the smaller sum T and look it up on a table of N against p values. There are $\frac{n(n + 1)}{2}$ possible pairs of differences, each of which has an average difference.

The real values of these averages are ordered $D_1 \dots D_{T5} \dots D_i$ where $T_5$ is the T with probability 0.025 (the 2-sided p value of 0.05 on the tables). DT5 to Di-T5 are the outer bounds of the 95% confidence interval for the difference.

## Wilcoxon Rank Sum
for the difference of medians in two samples (Mann Whitney U / MWW). Assumes data are i.i.d. and takes the strong H0; distributions of two samples differ only in location for continuous data.
Less strong but valid for ordinal data H0; P(X>Y) = P(Y>X) for ordinal responses.

Take two samples of size N1 and N2, with a total size N. Order all of the outcome values, rank the values in each sample as R1 and R2. The total rank sum is $R_1 + R_2 = \frac{N.(N+1)}{2}$, equal values take the mean of the ranks they span. Sum the ranks in each group. The null hypothesis is that the groups are identically located. T is the rank sum in the smaller group. 
	
Calculate the number of possible choices of size 1 ... T from groups of size N1 and N2. Sum the possible choices and invert for the probability or look it up for tables of N1, N2.

Ui is $U_i = R_i - \frac{N_1 . (N_1 + 1)}{2}$, and the total possible is $U_1 + U_2 = N_1 . N_2$. 
$AUC_{ROC_1} = \frac{U_1}{N_1.N.2}$ for ROC curves
95% CI by bootstrap or Hodges-Lehmann estimates of location shift.


## Spearman Rank Correlation Coefficient
Ranks for each observation are calculated (ties given average rank). Pearson's correlation coefficient calculated for the ranks as if they were x, y coordinates.

# Data types and features

This is partly covered by the [sampling] section, but it's integral to demonstrating epidemiological understanding.

## Questionnaires

Questionnaires are able to ask about 
    attitudes
    beliefs
    behaviours
    experiences
    attributes

Methods of applying the question are by 
    Interview
    Questionnaire
    Diary
    Observation
    Routine Record Reaching
    Measurements
    Environmental Measurements

Standard introductions are important.
If appropriate to use a questionnaire, develop one that is 
- easy to answer, process and analyse, 
- minimises measurement error, 
- preserves dignity and privacy of respondent, and 
- limits burden on respondents.
    "The minimum amount of an individual's total experience that will provide sufficient information concerning the problem under study" (including exposure and covariates).

Burden is added by 
- length 
- old information 
- low salience (and high sensitivity) 
- frequent events 
- greater depth of detail, 
- being a proxy respondent 

Too much burden causes low response, failure to complete, low quality or random responses, alienation from questionnaires.

Responses are maximised by 
- low burden 
- trained interviewers 
- personalisation 
- storyboarded not scripted acknowledgment and introductions 
- facilitation (SSAEs for return,face-to-face, diligent attempts to contact, incentives) 

Allow responses by softening or appearing to understand sensitive questions, giving an option to give a socially acceptable answer before reporting true exposure.  Start with a relevant question, leave demographics to the end.  Go by concept group, go from general to particular, go from bland to sensitive.  Give an introduction, instructions (including the logic of the questions), link parts with language and conclude with thanks, ask for comments and tell what comes next.  Make it nice to look at: clear, not cluttered or congested.  If offering a closed list, cover all possible options once.  End with a nice thank you and a request for comments or suggestions.

Pre-Test by expert initial review, interview, interviews or cognitive testing around different versions of a complex question (think-aloud, paraphrasis of the question, explanation for why a choice or answer was made).

Then pilot with a debrief of respondents, observation while responding, permutation of questions, item response distribution analysis (proportion missing / don't know / valid), validation (not always possible).  At least expert review and 20 pilot runs.

Are all the words understood, does the logic of order always work, are there questions that makes no sense for some people, do the questions mean the same to everyone?  Give aids to memory if possible: eg lead-in questions, possible answers to choose, a life calendar leading up to the time.  If given by interview give space for times for starting, finishing and any notes.

Electronic data capture (questionnaires straight into electronic format) have advantages in filling or calculating routine information, checks on format or consistency, easy skipping or contingency questioning, easy alteration or updating, complex identification and linkage, and the preservation of the data.

Disadvantages are in the up front work, need for networking in some implementations, desirability of the hardware, and some points that are only important if you don't have IT support.

Translation needs 

- Preliminary translation (I would put Simple Clarity or Translatable before that one) 
- Evaluation of the first translation (eg by blind back translation) 
- cross-language equivalence eg by giving perfectly bilingual people both versions (?!) 
- validity assessment 

## Tidy Data

- Each observation is a row 
- Each variable is a column 
- Each "kind" of variable is a table 
- A column allows linkage between tables 

Also have a row with variable names, which are human readable, and save in one file per table.
A fully coherent data set is the raw data, tidy data, a codebook and a recipe to go from one to the other.
The codebook is the information about the variable which is not contained in the tidy dataset, including information about units, the choices you've made in selecting or presenting data and about the design of the research.  Best as a text file with sections "codebook", "study design".
The script or set of instructions to process raw -> tidy data should not need parameters: having parameters introduces analyst degrees of freedom.

# Wrong conclusions

## Effect modification

"the association of an exposure with an outcome varies by a third factor".

## Confounding

is the unequal distribution of a further risk factor between those exposed and unexposed to the exposure of interest. Confounders are
	1 associated with the exposure in the source population 
	2 associated with the outcome in the nonexposed 
	3 not a causal intermediate from exposure to outcome 

## Bias 

is any systematic error in design or conduct of a study leading to incorrect conclusions.

### Selection bias

Responder bias is a type of selection bias, the responder odds ratio being biased unpredictably by the ratio of probabilities of selection from a contingency table:
		$$OR_R = \frac{P_a P_d}{P_b P_c}OR$$
Knowing this, a non-response of 0.8 could bias the odds ratio by a factor of 0.8 to 1.4, for example.
- Compliance 
- Surveillance 
- Differential Follow-up / missing data
- healthy worker effect 


### Information bias

- Reporting, Recall, Interviewer 
- Observer: Recording, Measurement, Classification, Ascertainment
- Reporting bias in Systematic reviews 
- Errors in Sequence generation, allocation concealment, blinding, incomplete outcome data, and selective outcome reporting

(Cochrane list)

## Control of threats to inference:

Discarding information based on potential comparators having parameters that fall outside acceptable ranges: 
- Restriction is absolute and simple but reduces potential comparisons, hence inferences 
- Matching 
	- by individual (search for control with Value = case Value +/- acceptable range) 
	- by group aka frequency (distribution in case group matched by that in control group) 
	- re-note Case Definitions: clear method, clear boundaries, defined source population 

Or not discarding but instead randomising.
- Simple random "needs a whole population list" although of course there are limits. 
- Stratified random, multiple stage random, single randomisation of the fixed point of a sample system, called "systematic random". Eg start at a random place in a pseudorandom ordered list and take regular steps through to sample. 

Bias is any systematic effect (London:"error") in the design or conduct of a study which results in an incorrect estimate of the association between exposure and outcome. 
- Information bias is present when there is a difference in the accuracy of information	collected for exposure or outcome.
- Reporting bias is due to subjects reporting one with different accuracy, depending on their status in the other (for example reporting exposure differently if they have the outcome, or reporting outcomes differently if they were exposed). 
- Observer bias is due to measurements giving different answers on one depending on the status of the other (for example, the outcome is reported with greater deviation or variation in the exposed, or vice versa) 
- Selection bias is present when there is a systematic difference in the exposure or outcome between those who are observed and 	those who are not. AKA Collider Stratification bias. 
- The sampling frame may not be representative of the target population 
- The groups may not be comparable (their sampling frames may differ) 

## To avoid. 
- Blinding 
- Objective measures 
- Randomisation 
- Complete follow up 
- Timely measurements 
- Calibration under the desired circumstances ( _eg_ values in true controls by _eg_ questionnaire or BP cuff calibration) 

# Reliability
(~= Repeatability): intra and interobserver reliability.
Variability: continuous is partitioned into systematic and random error eg by ANOVA; 
their ratio is the Intraclass Correlation Coefficient, *aka* reliability coefficient
continuous is by some correlation coefficient eg Pearson's method.
This gives Pearson's correlation of product-moment where for a population
	$\rho (X, Y) = \frac{cov (X, Y)}{\sigma_{X}.\sigma_{Y}}$	,  $cov (X, Y) = E[(X-\mu_{X})(Y-\mu_{Y})]$
and using the identities
		 $\sigma_{X}^2 = E[(X- E[\bar{X}]^2)] = E[X] - E[X^2]$ and
		 $E[(X-\mu_{X}).(Y-\mu_{Y})] = E[(X-E[X]).(Y-E[Y])]$, so
		
	$\rho_{(X, Y)} = \frac{E[XY]-E[X]E[Y]}{\sqrt{E[X^2] - E[X]^2} \sqrt{E[Y^2]-E[Y]^2} }$	

then the sample correlation r is
    $r = r_{xy} = \frac{\sum_{i=1}^n (x_{i} - \bar {x})(y_{i} - \bar {y})}{\sqrt{\sum_{i=1}^n (x_{i} - \bar {x})^2}   \sqrt{\sum_{i=1}^n (y_{i} - \bar {y})^2}}$
			

and can also be quoted as the mean of the standard scores of x and y:
		$r = r_{xy} = \frac{1}{n-1} \sum_{i=1}^n (\frac{x_{i} - \bar {x}}{s_{x}})(\frac{y_{i} - \bar {y}}{s_{y}})$
					
categorical is by tabulation, eg mean pair agreement or Observed Agreement OA:
		$\frac {a+d}{a+b+c+d}$
or Kappa statistic for excess agreement over max chance agreement  $\frac{OA-CA}{1-CA}$,	

(where CA is $\frac {E[a]+E[d]}{a+b+c+d}$ and $E[a] = \frac{Row total \times Column total}{Grand total}$ as in the χ2 )


Regression is by $y = \beta_0 + \beta_1 x + \epsilon$ where ε is normal with SD estimated by s, below

β1 from $\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar X)(y_i - \bar Y)}{\sum_{i=1}^n (x_i - \bar X)^2}$, then $\hat \beta_0 = \bar Y - \hat \beta_1 \bar X$ or 
		$\hat \beta_1 = Cor (Y, X) \frac{SD_y}{SD_x}$
		

Confidence intervals for the slope are $\beta_1 \pm t_{2 \times 0.975, \nu} \times s.e.$, with t statistics used to test H0: coefficient = 0
Standard error differs for prediction of a line at x0 and for the probability of a new point at x0
$s.e._{line \beta_0} = \hat \sigma \sqrt{\frac{1}{n} + \frac {(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}$, 
$s.e._{point \beta_0} = \hat \sigma \sqrt{1 + {\frac{1}{n} + \frac {(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}}$
		
,
$s.e._{\beta_1} = s{\frac{1}{\sqrt{\sum(x - \bar x)^2}}}$
$s = \sqrt{\frac{\sum(y - \bar y)^2 - \beta_1^2 \sum(x - \bar x)^2}{n – 2}} \textrm{, df = } n - n_{coefficients}$
	
So if the null hypothesis is that exposure is not predictive of outcome, and exposure of interest is at the head of columns, then the expected frequency of outcomes is (row total/table total) for the proportion of entries that exist in the row, times (column total) for the available number of exposed observations.

\begin{tabular}{| c |c | c | c| }
\hline
& + & - & \\ \hline
D & a & b & $N_{D}$ \\ \hline
$D^c$ & c & d & $N_{D^c}$ \\ \hline
& $N_{+}$ &  $N_{-}$  & N \\ \hline
\end{tabular}

The distribution of the squared differences in observed and expected values generates a χ2 distribution scaled for the expected values, with degrees of freedom $\nu = (rows-1) \times (columns-1)$. That is, the χ2 statistic is given by $\sum {\frac{(O-E)^{2}}{E}}$, 
H0 = "the distribution of observations among the categories of one variable is independent of their distribution among the categories of the other" or "there is no association between row and column variables"

To repeat:
	$$E(a) = \frac{N_D N_+}{N}$$
    $$Var (a) = \frac{N_D N_+ N_{D^c} N_-}{N^2(N-1)}$$

which is only from Smith et al *"Field Trials"* but the same shape as in the C-M-H test for trend below

Chi squared is distributed as $\frac{1}{{2^\frac{k}{2} \Gamma(\frac{k}{2})}}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}$, but that's not important right now.

In the special case of a 2x2 table the χ2 statistic is
		$\chi^2_{Yates} = \frac{N.(|ad-bc|-\frac{N}{2})^2}{(a+b).(c+d).(a+c).(b+d)}$

including Yates's continuity correction of O-E-0.5 for each cell because of a tail of the underlying binomial continuous distribution being better estimated by a point midway between that cell count and one fewer.  This sums to N/2 for a 2x2 table with N=20-40 and no value <5.
More generally χ2 can be used in a 2xC table if no value <1 and 80% values are >=5

Otherwise Fisher's Exact:  
$$p_{table} = \frac{N_+ ! N_+! N_D! N_{D^c}!}{N! a! b! c! d!}$$
and either sum p_table across all the possible tables with lower probabilities
or double the sum of probabilities of lower p_table (either acceptable, both differ!)

and for the test of a "trend" across scored levels of a variable which differs between groups Xi, hence having one degree of freedom as it's comparing two means-of-scores X̄1 and X̄2 is a t test squared, which is also the form of the Pearson Chi Squared:
		 $$\chi^2_\rho = \frac{(X _{1} - X _{2})^2}
{s^{2}.(\frac{1}{n_{1}} + \frac{1}{n_{2}})}$$

or if assessing against H0: increase in log(OR) per ration of x==0, which is a trend in ordinally scored variables where xi is score of a group i, di is those with an outcome in that group; ni is the total in the exposure group; O and N are the totals with the outcome and grand totals.

$U = \sum d_i x_i - \frac{O}{N} \sum n_i x_i$
$V = \frac{O(N-O)}{N^2(N-1)} N \sum (n_i x_i^2) - (\sum n_i x_i)^2$
$S^2 = \frac{V}{QR}$ and error factor $EF = e^{1.96 S}$
$\chi^2_{trend}=\frac{U^2}{V} \textrm{, df=1}$
Increase in log odds for an increase in x is U/V; the variance of this estimate is $\sqrt{1/V}$.
The difference between the standard Chi squared test for this table and the Chi squared test for trend is the Chi squared test for departure from linear trend with df = (number of exposure groups – 2).
Also $V = \sum V_i$ where $V_i = \frac{D_i H_i N_{0i} N_{1i}}{N_i^2 (N_1 -1)}$ which is called the Mantel-Haenszel Chi Squared in SM04 of EPM202.
	
# ANOVA
Related to linear regression.   A table of n observations y, in k groups 
The sum of squares of deviations from the mean, $\sum (y - \bar y)^2$, is partitioned into
	between group $\sum n_i (\bar y_i - \bar y)^2$ with *k*-1 degrees of freedom
	within group $\sum (n_i - 1) s_i^2$ with *n-k* degrees of freedom
		for a line the total variability is from the 0-gradient line (null) at the mean of y,
		the y-distance from the null line to the regression line at each xi = SSregression,
		the y-distance from regression to point at xi = SSresidual and
		each SS is divided by its degrees of freedom giving the mean square, $MS = \frac{SS}{df}$
For a line, the residuals are $\sum (y - \hat{y})^2$ and the model SS is of deviations from the mean.
The F statistic is $\frac {\textrm{Between Group} MS} {\textrm{Within Group} MS} , df = k-1, n-k$ or $\frac{regression}{residual}, df= 1, n-2$ for a line.
		For the F test, under the null hypothesis the ratio of two variances follows the
		F distribution and for a line, F = t^2^ .
		This can also be used for regression diagnostics where $F = \frac{\textrm{Model MS}}{\textrm {Residual MS}}$;
A partial F test excludes the contribution of one covariate after adjustment for others, as a LRT equivalent.

The R^2^ is $\frac {Regression SS}{Total SS}$, giving a proportion of variance explained by the regression or grouping.

	As $Var(X) = \frac{1}{n -1} \sum_{i=1}^n (X_i - \bar X)^2$, so 
	$Cov(X, Y) = \frac{1}{n -1} \sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y) = \frac{1}{n-1}  \sum_{i=1}^n (X_i Y_i - n \bar X \bar Y)$ and
	$Cor (X, Y) = \frac{Cov(X, Y)}{S_x S_y}$
	

Mantel-Haenszel estimate of the common Odds Ratio is weighted for the non-supporting information as a proportion of stratum size
		 $\frac{\sum ({\frac{a_i d_i} {n_i}})}{\sum ({\frac{b_i c_i} {n_i}})}$, that is $\frac{(a_{1}.d_{1} / N_{1}) + ... (a_{j}.d_{j} / N_{j})}{(b_{1}.c_{1} / N_{1}) + ... (b_{j} c_{j} / N_{j}})$
		 or if $w_i = \frac{b_i c_i}{n_i}$ 
		 then $OR_{MH} = \frac{\sum w_i OR_i}{\sum w_i}$

and there's a corresponding Cochrane-Mantel-Haenszel Chi^2^ test of the same shape as the simple single table single stratum situation, 
	$\chi^2_{MH} = \frac{[\sum a_{i} - \sum E (a_{i})]^2}{\sum Var[a_{i}]}$, equivalent to 
	$\chi^2_{MH} = \frac {(|\sum{\frac{a-(a+b).(a+c)}{n}}|-0.5)^2} {\frac{\sum(a+b).(a+c).(b+d).(c+d)}{(n^3-n^2)}}$

$\chi^2_{trend} = \frac{[\sum^k_{i = 1}r_i v_i – R \mu]^2}{p(1-p)[\sum^k_{i = 1} n_iv_i^2 – N \mu^2]}$

$mu = \sum^k_{i = 1} \frac{n_i v_i}{N}$

where each of k groups of observations are denoted as ri successes out of ni total with score vi assigned. R is the sum of all ri, N is the sum of all ni and p = R/N.


# Regression

Linear regression is almost the canonical version of this, with Logistic, Cox, Poisson and even Mantel_Haenszel methods being special or extended cases. 

A regression line is the line to which values "regress" in repeated observation of the parameter values.  The remaining variation in those sampled values is smaller than the variation in the whole dataset, because some of it is already modelled by the line.  Errors are unobservable true errors from known coefficients, whereas residuals are the observable errors from the estimated coefficient: so in a sense residuals are estimates of the error.

Regression seeks to minimise the residuals which because they add to 0, are squared:

For $Y_i = \beta_0 + \beta_1 X_i$ the sum of squares = $\sum_{i=1}^n (Y_i - \beta_0 + \beta_1 X_i)^2$ is minimised
	when $\beta_1 = 0 , \hat \beta_0 = \bar Y$ and when $\beta_0 = 0 , \hat \beta_1 = \frac{\sum_{i=1}^n X_i Y_i}{\sum_{i=1}^n X_i^2}$

the estimator that minimises the sum of squares for a regression line is $\beta_1 = \frac{\sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X)}{\sum_{i=1}^n (X_i - \bar X)^2}$
	 , or $Cor(X,Y) \frac{sd(Y)}{sd(X)}$
	 

Model Yi = β0 + β1 Xi + ϵi where ϵi ~ N(0, σ2).  Observed outcome i is Yi at predictor Xi;
	Predicted outcome at predictor Xi is $\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i$ and residual is $e_i = Y_i - \hat Y_i$
	Least squares minimises $\sum_{i=1}^n e_i^2$, ei being estimates of ϵi .

The errors are the product of all the unmodelled confounders or modifiers acting outside the model, on the observed data, so E(ϵi )=0 ; if an intercept is included then their sum=0 and if a linear regressor is included then $\sum_{i=1}^n e_i X_i = 0$ so the residuals are very useful for investigating poor model fit such as by residual plots.

ML estimate of the variance σ^2^ around the line is $\frac{1}{n} \sum_{i=1}^n e_i^2$, and $\hat \sigma^2 = \frac{1}{n-2} \sum_{i=1}^n e_i^2$ is an unbiased estimate for the population sigma based on the independent residuals.
The numerator for total variability in response variable is $\sum_{i=1}^n (Y_i - \bar Y)^2$ and is made up of the variability explained by the line and the residual variability.
	$\sum_{i=1}^n (Y_i - \bar Y)^2 = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n (\hat Y_i - \bar Y)^2$ and

the proportion of the total variability explained by the regression relationship is R squared:
	
$$R^2 = \frac {\sum_{i=1}^n (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}$$

	Which is literally the sample correlation squared.
	Deleting data or adding terms to a regression model always increase R^2

Variance around the regression line is usually replaced by its estimate.
	$\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar X)^2}$

## Mantel-Haenszel 
Mantel-Haenszel in general uses an association measure in stratum i weighted for total exposure in the stratum. So for a cohort describing relative risk:

	$RR_{MH} = \frac{\sum{\frac{d_{1i} \times T_{0i}}{T_i}}}{\sum{\frac{d_{0i} \times T_{1i}}{T_i}}} = \frac{Q}{R}$
	or if $w_i = \frac{d_{0i}T_{1i}}{T_i}$ then 
	$RR_{MH} = \frac{\sum(w_i \times RR_i)}{\sum w_i}$

	 $se_{MH} = \sqrt{\frac{V}{QR}}$ where $V = \sum V_i$ and $V_i = \frac{d_i T_{1i} T_{0i}}{T_i^2}$ and NHST is the MH chi squared


	 $\chi^2_{MH} = \frac{(\sum d_{1i} - \sum E_{1i})^2}{\sum V_i}$ or $\frac{(O-E)^2}{V}$, df = 1

	where Exi is overall stratumi rate times observation time for exposure x, $E_{1i} = \frac{d_i \times T_{1i}}{T_i}$

To detect effect modification the Chi Square test for heterogeneity $H_0: RR_i = RR_{MH}$

	$\chi^2_{het} = \sum \frac{(d_{1i} T_{0i} - RR_{MH} d_{01} T_{1i})^2}{RR_{MH} V_i T_i^2} \textrm{, df = c-1}$

Cohort E(D1) = Y1 *(D/T) and U = D1 – E(D1)

Var(U) = $D \frac{t_1}{t}(1 - \frac{t_1}{t})$ and test statistic  $z= \frac{U}{SE(U)} = \frac{U}{\sqrt{Var(U)}} or Z^2 = \frac{U^2}{V} \textrm{, Chi square df=1}$

## The MH odds ratio across strata

is the weighted mean sum of odds ratios across strata
	generally $OR_{MH} = \frac{\sum(w_i OR_i)}{w_i}$
		weights $w_i = \frac{d_{0i} h_{1i}}{n_i}$ denominator of OR divided by the total
	or ORMH = Q/R where Q = $\sum \frac{d_{1i} h_{0i}}{n_i}$ and R = $\sum \frac{d_{0i} h_{1i}}{n_i}$
		then se(log(ORMH)) = $\sqrt{\frac{V}{QR}}$, V = $\sum V_i = \sum \frac{d_i h_i n_{0i} n_{1i}}{n_i^2 (n_i - 1)}$,
		V calculated from the marginal totals because each stratum has equal variance
Chi squared is $\chi^2_{MH} = \frac{(\sum d_{1i} - \sum E_{1i})^2}{\sum V_i} = \frac{(O-E)^2}{V} = \frac{U^2}{V} \textrm{, df=1}$,

	$E_{1i}= \frac{d_i n_{1i}}{n_iO} = \sum d_{1i}, \> E = \sum E_{1i} \textrm{, U = O-E}$
	MH chi squared is (n-1)/n times the size of the non MH chi squared in the simple 2x2 table.
Rule of 5 to check validity:
	subtrahends of `c(sum(min(c(di, n1i)))` , `sum(max(c(0, (n1i – hi))))) – sum(Ei) > 5`
Chi squared for heterogeneity tests H0: for all i, ORi = ORMH , so $\chi^2_{het} = \sum \frac{(d_{1i} h_{0i} - OR_{MH} d_{0i} h_{1i})^2}{OR_{MH} V_i n^2_i}$

## Multivariable regression
Extending the line to a plane, or later to an n-dimensional plane.  Linear in the coefficients.  Least squares used again, fixing at zero (or removing ) coefficients and calculating regression through the new origin for the remainder, repeatedly.
For a model with sum of squares $\sum (Y_i - X_{1i} \beta_1 - X_{2i} \beta_2)^2$ to estimate B2
	define Y centred, $\~ Y_i = Y_i - X_{1i} \beta_1$ then sum of squares = $\sum (\~Y_i - X_{2i} \beta_2)^2$
	and so $\beta_2 = \frac{\sum \~Y_i X_{2i}}{\sum X_{2i}^2}$, and so on for other coefficients.
	and if $e_{i, a | b} = a_i -  \frac{\sum_{j=1}^n a_j b_j }{\sum_{i=1}^n b_j^2} b_i$
So we get the minimiser $\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2} X_1}$ or $\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e^2_{i, X_1 | X_2} }$

A multivariable regression coefficient is hence the expected change in response for a 1 unit change in the associated predictor, holding all the other values constant (at zero, say, as if regressing through the origin for each predictor).

The model is then $Y_i = \sum_{k=1}^p x_{ik} \beta_k + \epsilon_i$ with fitted responses $\hat Y_i = \sum_{k=1}^p x_{ik} \hat \beta_k$
	residuals are $e_i = Y_i - \hat Y_i$ and variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i^2$
	coefficients have standard errors $\hat \sigma_{\hat \beta_k}$ and H0:coeff=0 tested by the t-test $\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$

Outliers can be real or spurious, conform to the regression line or not.  Leverage is distance along the regression, influence is orthogonal distance from the line.  Influence measure diagnostics amount to leaving out a point and recalculating and all are a bit "in context".  Poor model fit includes heteroskedasticity, missing model terms, temporal patterns (find out by residuals versus collection order).  Residual Q-Q plots investigate normality.

Adding spurious variables inflates the variance: as the true sigma is unknown the variance inflation factor of a predictor is the inflation of variance due to correlation of it with other variables, over the ideal situation in which it's orthogonal to all the other variables.
Missing helpful variables out introduces bias in the estimate.

# Missing (from Josse)
Missing for various reasons 
MCAR – P(missing) does not depend on either the missing value itself or any other value.
MAR – P(missing) does not depend on the the value itself; but may depend on other variables (ie it's Missing At Random Within Strata)
Multiple correspondence analysis MCA graph on a "missingness matrix" shows neighbours in missingness; but doesn't imply a missingness definition (eg broken machine probably MCAR but if broken by a high value, then MNAR).

EM algorithm, os supplementary EM algorithm, allows a model that uses the observed data, then updates with the last estimate and so on.  Needs a new process for each assumed underlying model.

Imputation using the mean shrinks the correlation to zero; by the regression line shrinks to 1; can impute by the regression line plus noise (stochastic regression) and preserve both the joint and marginal distribution of the data, so estimation has no bias added to the missingness bias itself.  It's an extension of this last to the multivariate case that results in the mice procedure.

The impact of missingness is related to the difficulty of imputation and for the same reasons: both proportion and structure of the missingness, and of the data itself, are important in determining these. Get signposts to this by 

Cross validation removes some values and repeats, checks the resulting inferences.

Principal Components minimises the distance between an observation and its projection. Estimates the single best projection matrix that does so.  With missing values there is no such single estimated matrix. Use instead one of the many algorithms, eg Iteraive PCA. Iterative PCA starts with values imputed somehow, eg the mean.  Constructs a vector using the left and right singular value matrices (check) then updates the imputation; then repeats. Always converges but it's not a convex method so there are local maxima; so Josse in missMDA use a "regularized PCA".
PCA uses the observed linear relationships between variables and so is most efficient and predictive of reality when the true relationships are linear.  So you need subject knowledge, to interpret or even to reparameterise (eg by making interactions explicit). 
Nevertheless PCA can work when variables are collinear or n < p, while linear-based methods (mice, Amelia, joint imputations) work less well. "You inherit from the techniques of your imputation". This is because it preserves both joint and conditional distributions, as above, and so all inference based on these has no additional bias.

Multiple correspondence analysis MCA is like PCA for categorical data, using an indicator matrix

Low rank structures imply that you can use clusters to infer with greatest efficiency.

# Logistic regression
The quantity estimated from the model is the log(Odds), from the log(OR)(="coefficient") and log(Other Odds). SE for log odds is as usual $\sqrt{\frac{1}{x_1}+\dots}$ so SE for odds is $e^{\textrm{SE(log odds)}}$
	log(odds) = a + b1A1 + b2A2 ...
Hypothesis testing by Wald tests (with SE for the null hypothesis, so the SE chosen depends on th null hypothesis chosen)
	H0: log(Odds) = 0 for each level of a potential variable for the model, $z=\frac{\textrm{coefficient}}{\textrm{SE}}$
	Or LRS, H0: the likelihood is not significantly increased by addition of the variable
Adjustment for other variables uses a proportional odds assumption.
Because it's built from a square table the absolute numbers of cases and non cases determine the baseline odds (="constant"): therefore the sample fraction ratio is added to the model.
	log(odds) = log(constant) + log(sample fraction ratio)+ log(exposure)
	sample fraction ratio  = proportion of cases sampled / proportion of non cases sampled
	log(constant) + log(sample fraction ratio) = constant*

Logistic regression for trend across levels of a variable:
	doesn't have to assume proportional odds (although this simplifies and gives more precision)
		this is parameter reduction eg from 4 parameters for different levels to 1
		this is critically altered by the choice of grouping for levels of exposure
			eg choose the midpoint of arbitrary age bins to 
Hypothesis tests for trend are
	"test for linear trend": model is better with linear effect than without, then
	"test for departure from trend": for improved prediction with many versus one parameter
		both use LRS with df = difference in number of parameters.

Interaction in the model is referred to as eg param1.param2 and is a single new parameter
	tests for interaction are against H0: the interaction is not greater than chance, using LRS.

Interaction is not detected automatically and must be added to a model then tested, so there is no way to guarantee that all meaningful interactions are detected or that interactions detected are meaningful beyond the dataset.

The assumptions of a linear model imply that the residuals are normally distributed, with a constant variance across x and y.  Plots of standardised residuals (rescaled to mean = 0, sd = 1) help: density of Std Res for normality, Std Res versus corresponding fitted value for deviation from linear.

# Conditional Logistic Regression
Conditional on the membership of some stratum, such as a matched pair, the effect of a covariate is estimated.  The effect of the matching variable is not estimated (because the procedure would be different) nor is the constant term (because that would need the effect of the matching variable to be included in it).  Instead of logit(p) = log(odds) = beta + stratum + constant, we have log(OR) = beta.

Confounding is assessed by excluding a covariate from the model to create nested models, which are then tested by the likelihood ratio test; interaction by stratifying and using LRT with or without strata; linear trend by a score test or by adding a linear predictor and LRT.

Variables can, confusingly, be fixed or changing; if changing they can have a deterministic or stochastic relationship with the determinant of that change, such as time.  Rates that vary systematically over time are accounted by splitting into epochs and calculating rates conditional on the occurence of an event, ignoring non event containing epochs (Lexis expansion or "episode splitting").  This looks like a sort of limit case for Cox regression, allowing estimates of the rate, not only rate ratio, during each epoch as long as there are sufficient events.




# Error
Measurement error: Validity and Reliability....!
For continuous variables: Correlation coefficients.  Mathematical coupling.  Agreement versus correlation.
For categorical: Classification f agreement. 
See also Cronbach alpha for a composite rating X as a sum of K items $Y_1 + Y_2 + Y_i \dots Y_K$

$\alpha = \frac{K}{K-1}\left(1-\frac{\sum^K_{i=1}\sigma^2_{Y_i}}{\sigma^2_X}\right)$
Misclassification has variable effects but mostly:
	Differential may be decisively erroneous.
	Non-differential exposure misclassification biases estimates to null
	Non-differential outcome underestimates alter ratios less, and bias differences to 0
	Non-differential outcome overestimates bias ratios and differences to null

# Analyse
Stratification
Needs accurate identification of strata: if too broad will replicate the confounding, if misclassified could unpredictably increase or decrease the confounding: "residual confounding".  If matched by strata, those strata are reported as if unmatched then results pooled somehow.  If perfectly matched for the confounder then the groups can't study the effect of the confounder unless they're also perfectly matched for another cause (resulting in tiny fractured groups, for which a solution is Bayesian Borrowing).  Stratum specific probabilities are either randomly distributed about the true total population value if there is partially controlled confounding, or are real, and display the effect modification.
A special case is Standardisation, applying weights to observations in categories according to a standard distribution of observations expected in those categories.
	In eg direct standardisation for age, a standard demograph weights crude population mortality for age standardised mortality: the number "deaths per thousand person years of standard population" is the "deaths per thousand person years of actual population" (crude death rate) times comparative mortality figure (CMF, ratio of rates).  I think the weight here is the area of a histogram where x=age range and y=observations, each of the steps of which are multiplied by a death rate to give a new histogram.  The area under this new histogram is the population standardised mortality rate and the ratio of its area to the original is the CMF; clearly the standard matters, especially as they insist on probabilities not odds.
	Indirect standardisation for age uses not size but death rates as the standard.  It's the basis for the Standard Mortality Rates and hence ratio, SMR.  "Direct standardisation is subject to less bias"; why?  Because true rates are needed and so there's more finely grained data?  Maybe, because "indirect may be the only possible" if rates aren't available at each stratum or numbers are small hence standard error high.
	So Direct uses observed rates if applied to a standard population, ("Directly standardised rates")
	Indirect uses standard rates if applied to the observed population
	If a confounder is misclassified its effect may remain: residual confounding

Or use a model to borrow from neighbourign areas, using both a structured and unstructured geographical component, eg:
$Y_i|P(x_i) = Binomial(N_i, P(x_i))$
$logit(P(x_i)) = z_i \beta + S(i) + u_i$


Where S(i) is according to some coordinate reference system CRS (UTS in Eastings + Northings; or lat/long in unstructured ellipsoid projection)
Study designs
Case Control sampling is either one or more tier: "internal controls" are unexposed ones from the initial sample.  If a rare exposure the initial sample must be very large; or "similar" unexposed can be selected from the same or a similar population in another sample as "external controls".  The similarity is discussed in mechanistic terms needing experts, not advice on sufficient stochastic descriptions of the similarity.
	"If the groups are comparable they have internal validity"; "if the conclusions are applicable to other populations it has external validity"
	"In case studies the controls are only a subset of the NonCases in the population"
	The OR is "Odds of exposure in those with outcome"; the logical flips assumes that the sampling probability of controls is the same whether exposed or unexposed.

Case definition
	The method used to identify
	The boundaries of cases – eg cutoff on a continuous measure, ins and outs for categorical
	The unit of analysis – eg community, person, organ, events themselves

Cohort E(D1) = Y1 *(D/Y) 
	Exposure status is not biased by knowledge of outcome
	Exposure status is known for all (because it's the definition of who is in)
	Time sequence and course of events can be described	
	More than one outcome or type of outcome can be reported
	Consider
		Source of controls
		How many types of controls
		How many controls per case (power gains fall from 4 per case)
		Should they be matched?
		Loss to follow up reduces power and certainty
		Differential loss to follow up (by exposure or outcome) biases irretrievably
		Information / classification bias if outcomes are not equally accurate by exposure

Ecological studies use population values rather than individual values so n is small
	Multi-group / Time-trend, descriptive / analytic
	Cheap, easy, quick, maybe all that's possible, allow population inference
		But don't allow individual inference and don't control confounders
	"Ecological Fallacy" is extending ecological inferences to individuals.
		Population average exposures would also have to vary by individual to account for
			variability in outcomes per individual
	Contextual effects of exposures: when grouped, may have effects contingent on others
	Outcomes can become exposures for other or the same outcomes (also a contextual effect)
	Summary / environmental / integral measures
	Confounding is easily hidden by complexity and size and there are no opportunities to adjust
	Associations, so confounders, may exist at population level that don't at individual level
		Details of the distribution of confounders is often not available
Bias: Information bias is common, often undetectable and severe
	often due to the use of proxy, secondary or routine data at high risk of misclassification
		"non-differential misclassification within groups in ecological studies",
			unlike in individual level studies, "usually leads to bias away from the null"
	Misclassification is very common and may be severe.
	A sort of selection bias by examining exposure and (time dependent) outcome in the
			same slice, eg measuring candidate risk factors and the cancers at
			time t (not t+latent period).
Correlations and regressions are the usual numbers generated after exploratory analysis.
Having the Idea from Crombie has a holy feeling about it, for all that it was written humbly by a sharp witted amateur.
	Review existing practice
	Challenge accepted ideas
	Look for conflicting ideas, ask around
	Investigate geographical variation or interindividual variation
	Identify Cinderella topics
	Let loose the imagination / be kind to ideas / reject little
Dissect the questions
	Write
	Discuss with colleagues over pints
	Think in the bath
	Read around the subject until you have read enough
	Look upstream
Choose the best question									
	Will this method answer my question?
	Evidence there is a problem							
	Is it feasible?
	Burden of disease									
	Do I have the resources?
	Big possible changes to patient outcome				
Is method appropriate for the design?
	Cheap solution or expensive problem
	Question is a question other people can understand
	An answer would be done by people
	Urgency of good data / chance of bad practice to entrench
	Feasibility
Refine the question with a sharp tool and a high threshold for acceptance
	Review as if newly seeing it
	Is it as important as it looked?
	Don't rush in: are you still sure that's the question?
	Review and redo the analysis, simulate, simulate, simulate
	Criticise mercilessly
		Where could it go wrong?
		Why might people not participate?
		What necessary data might not be feasible to collect?
		Do we have enough blood and treasure to spend?
		Can we simplify it and still get an answer / will it be even better?
	Be realistic
		Ask the best version of the best question as well as you can, if you can do it well
		Don't waste effort on getting incrementally closer to perfection
		Don't start what you will never finish
Qualitative
	Seeks to interpret phenomena through the meanings people bring to them
	When the topic is not familiar, the context is needed, rich views or the social world in which
	others live is the goal.
	Common features are flexibility, triangulation, contextualisation, iteration and reflexivity.
	Techniques include interviews, focus groups, observation, participation.

# Utility
This is something that needs to be expanded here. For now it's just a dumping ground for some interesting things. Such as:2E: Discrete Choice Experiment (DCE) to determine weighting
Data Collection
We will use a best-worst discrete choice experiment (BWS DCE; BWS Case 3) to determine the value
placed on each dimension of the tool’s descriptive system. We will use best practice methods for the
design and analysis of choice experiments13 and follow approaches taken previously by CI Ratcliffe14
and CI Viney15, 16. The optimal sample size for a BWS DCE task is dependent upon the final number
of dimensions and levels to be included in the tool’s descriptive system, as determined by 2B and 2D
of our study17. Sociodemographic information, eg age, sex, ill health experience, income, remoteness
and family structure will also be collected.
AnalysisHREC Application Form Version 13, March 2017 Page 26
The primary analysis will use a mixed multinomial logit (MMNL) model with a panel specification8.
The MMNL can also identify heterogeneity in responses9; we will also examine alternative model
specifications18, as different respondents may trade-off in different ways, reflecting cultural or social
norms. From these analyses, we will develop a scoring algorithm for the instrument, based on the
ratio of marginal utilities (RMU) approach, below.
In valuing health gain, considering quantity of life (survival) separately from characteristics
(QOL/wellbeing) of those extra years, and including simple interactions between them, as would
usually be done for choice experiment analyses, may not be adequate in capturing this interrelatedness. An additional condition is also required for QALYs: that value of a health profile with
zero life expectancy is zero irrespective of the quality of life in that period. The utility function for
all models will, initially, be assumed to be linear with respect to time, and levels of QOL/wellbeing
enter the utility function as an interaction with life expectancy (the TIME variable). Thus, the utility
(U) of health state j in scenario s for individual i is in Equation 1. Thus, if TIME is set at zero, the
systematic component of the utility function is also zero.
$U_{isj} = αTIME_{isj} + βX_{isj}' TIME_{isj} + ε_{isj} $
Generating a scoring system for QALYs
To generate QALY weights for the health states to create the scoring algorithm of the tool, the initial
analysis will use a ratio of marginal utilities (RMU) approach, as used by AI Viney15, 16. In a QALY
model, ten years in full health provides 10 QALYs. If another health state provides 10 QALYs over
a 15 year period, the QALY model values that health state at 10/15 = 0.667. Using Eq1, the systematic
component of the utility function can be differentiated with respect to TIME,
$\frac{\delta U}{\delta TIME} = α + βX′$
The RMU between two health state profiles j and j* can therefore be estimated as:
$RMU_{j,j*} = \frac{\alpha + \beta X_{isj}'}{\alpha + \beta X_{isj*}'}$
This gives a value for a health state with characteristics defined by the βX isj ' term, relative to another
health state defined by characteristics βX isj ' * . If βX isj ' * is set to full health (ie the best level of each
dimension), the value generated using Eq 3 is a QALY weight15, 19.

# Ethics
	Beneficence, non-Maleficence, Autonomy, Justice
	Goal-, duty- or rights-based justifications.
	Validity, welfare, dignity (where Dignity = Confidentiality, Consent, Coercion)
	Consequentialism, Deontology, Communitarian schemata
	Reseach is not valueless and ethics are not universal: it's a process to ensure the dilemma is continuously resolved.
ICH GCP principles
	1. Do it with GCP				2. Benefit > Risk
	3. Rights of subjects > Society		4. Info supports the trial
	5. Protocol						6. Approved by Ethics
	7. Supervised by Dr				8. Qualified team 
	9. Informed consent				10. Data protected
	11. Privacy / confidentiality AND 	GMP manufactured products
12. Quality control in trial

NIH has the...
The Eight Ethical Principles
    1. Collaborative Partnership 
    2. Social Value 
    3. Scientific Validity 
    4. Fair subject selection 
    5. Favorable risk-benefit ratio 
    6. Independent review 
    7. Informed Consent 
    8. Respect for human subjects 
Protocols for the NIH contain
Précis 
    • Introduction 
    • Objectives 
    • Inclusion and Exclusion Criteria 
    • Plan for Monitoring Subjects and Criteria for Withdrawal of Subjects from the Study 
    • Analysis of the Study 
    • Human Subject Protections 
        ◦ Rationale for Subject Selection 
        ◦ Recruitment Plan and Procedures 
        ◦ Justify the Exclusion of Women, Minorities and Children (if applicable) 
        ◦ Evaluation of Benefits and Risks/Discomforts of Participation 
        ◦ Description of the Consent/Assent Process 
        ◦ Plan for Maintaining Privacy and Confidentiality of Subject Records and Data 
    • Data and Safety Monitoring Plan 
    • Protocol Monitoring Plan 
    • Data Management Plan 
    • Plan for Research Use and Storage of Human Samples, Specimens or Data 
    • Remuneration/Compensation 
    • Scientific References 



# Community Engagement
From Smith Morrow Ross: "community engagement will be defined as the process of the trial team working collaboratively with the community on all aspects of the study which aﬀect the community and its well-being. Overall, engagement should typically involve continuous mutual learning and communication between researchers and a range of community members before and during a trial and after a trial ends."
I thin there's a little too much of the White Mans Burden in this, where the inherent "good" of engagement is because of the researchers displaying cultural humility.  I'm sure it's nice to see but the studied community would get along just as well with nobody coming in to satisfy themselves about their cultural humility, but the conduct of a trial does change things and the focus should be on whether those changes are beneficial enough to justify doing the study there.
Should never be prefabricated, always negotiated and always actually strengthen participants' ability to control important aspects of their own lives or "empower" them.
Find out, acknowledge and reconcile complexities in community attitudes and internal power structures, empower the unempowered.  Pilot or engagement activities may reveal that much more work needs done just to understand the views of the community, before touching the research design.
Locoregional administrators or governments: Smith adds special interest and religious groups to this, explicitly recommending power games and assessing factions as if it's in fucking Helmand.  Local health providers for advice and "synergy".  This is all motherhood shite.
Frameworks Methodologies Theories Participatory
Apparently visual aids are Participatory.
Result feedback: considered and mediated feedback of overall results; but they also recommend using simple tests as a sop to the simple folk while sending off all the other tests.
Surveillance
"the ongoing systematic collection, analysis and interpretation of health data essential to the planning, implementation, and evaluation of public health practice, closely integrated with the timely dissemination of these data to those who need to know. The final link in the surveillance chain is the application of these data to prevention and control. A surveillance system includes a functional capacity for data collection, analysis and dissemination linked to public health programmes." (CDC, 1986)
Surveillance (tracking a disease or risks) vs monitoring (tracking process and outcomes)
	Practical, uniform, rapid; unlike epidemiological research which is one-off and specialised
	Many data collectors with less interest or time
	Passive (cheap, routine, patchy and dodgy data) or active (expensive, higher quality data)
	Lab surveillance only works for some diseases and depends on referrals; or
		serology surveillance of samples sent for other purposes
	Save some money and approach good performance by two stage systems or sentinel systems
Establish the objectives of the surveillance system 
    1. Develop a case definition 
    2. Develop a data collection mechanism 
    3. Field test methods 
    4. Data analysis 
    5. Interpretation 
    6. Dissemination of information 
    7. Evaluation of the system 

# Infectious epidemiology
"Generation time and serial interval are the same"
	Generation time is latent plus infectious and serial interval is time between identical clinical moments in successive cases in the same chain of transmission.
	The corresponding – or not – clinical times are incubation and symptomatic.
	Virulence is proportion symptomatic / having
	SAR (Secondary Attack Rate) is proportion Exposed and Infected / Exposed and as usual is entirely contingent on things not measured for its calculation and hence not included in it: eg usually assumes **identical duration** and **intensity** of a **unique** exposure among all who are considered, such as a class or village.  SAR and transmission probability are conditional parameters and either require contact details or see below.
	R0, (basic reproduction number) the average number of new infections per source, is an idealised value that could have been observed pragmatically if perfect knowledge of all exposed individuals were available. Generally it's most useful as a threshold: if R0<1 in a population the disease can't expand.  Compartments in populations behave differently and most diseases are not "simple" so modelling derives the R0.  Models rarely derive the same R0 if given the same data.  The CAL calls it: R0 = contacts * duration of infectiousness * probability of infection per contact.  If x is the proportion susceptible and "herd immunity" is 1-x and  then when R is 1:
	$HI = 1-\frac{R}{R_0} = 1-\frac{1}{R_0} = \frac{R_0}{R_0}-\frac{1}{R_0}$, so HIT (herd immunity threshold) is $HIT = \frac{R_0-1}{R_0}$
Vaccine efficacy is foolish, "usually" 1-(Risk in vaccinated/Risk in Unvaccinated).
	If due to reducing infectiousness it's VEi, if susceptibility Ves.
		If VEs=0, VEi=1 and f are vaccinated then Rf=(1-f)R0 
		If f=1-(1/R0) then HI happens.  For more general VEi and VEs this f is multiplied by $\frac{1}{VE_s + VE_i - VE_s VE_i}$
			
	Estimation depends on being able to see direct and indirect effects; with some assumptions VEs and VEi can be jointly estimated from only the disease rate in vacc v unvacc if the vacc proportion is varied in strata, clusters or times.  This joint estimation has a coverage estimate trading VEi against VEs
	and if a 2nd stage cluster, infection probability β, VEs =0, proportion infected at time t It, contact probability c, duration of contact d then in population of n individuals the hazard rate for infection if vaccinated is $\lambda_{0i}(t) = {c \beta I_{i}t}{n_i}$ and for unvaccinated is that times 1-VEi. 

# Causality – Bradford Hill – 9 dogme
"An active agent or a static condition"
Temporality, Strength (size), Consistency (across types of information), Dose:Response (Biological gradient), Specificity, Biological plausibility, coherence (sort of biological plausibility but probably intended to mean borrowing from other situations supports the idea), "Experiment" (protection by removal, in the examples), analogy (the example is HepB / HIV).
	Predictive performance
	No alternative theories after rigorous testing

X and Y are associated in the population if 
	X causes Y,
	Y causes X, or
	both X and Y are caused by another variable, call it Z.  In this case X and Y are not
		associated in strata defined by Z
X and Y may not be associated in the population, but are associated within strata of Z, if
	Z is caused by both X and Y
Z-conditioning can therefore create or remove X:Y associations depending on the causal structure.

# Causal Complements are those things which together are "necessary and sufficient"
Multi-causality demonstrated by Directed Acyclic Graphs
	Nodes, Edges, Directed or not; Parents, Children, Ancestors, Descendants.
	directed path is all forward arrows eg AC or AEDF,
	backdoor path is one starting with a block eg EACD
	collider: C is a collider on ACB but not ECD
	Blocked path has a collider along it; otherwise open
Remove all arrows from the exposure; in this case investigating effect of E on D.
	If there are unblocked paths there is confounding.
	Draw dotted lines connecting any nodes that share a child that is either in S or descends to S
	A set S of candidate confounders will be sufficient to control confounding if
		none is a descendant of the exposure
		there is no open path from the exposure to the outcome that avoids a member of S,
			including the new, dotted lines that denote 1st-gen sharing of S ancestry
S can induce open paths by causing the dotted lines: this demonstrates the creation of conditional association by stratum creation.
Selection or some other process of bias can be modelled with DAGs as well,
	another name for selection bias being Collider Stratification bias.

# Machine Learning
	A Perceptron is the smallest form of neural network, with weighted inputs and a bias which feed an activation function to produce a single output.  The output is calibrated repeatedly against appropriate values and the weights altered to improve the closeness of prediction until either accuracy or number of repetition limits are crossed.

The ideal type of data depends on the intended output.  eg 
Description: whole population
Exploratory: many variables on a random sample, up to the whole population
Inferential: The right variables in the right population, randomly sampled
Prediction: split training and test sets from same population
Causal: data from a randomised trial
Mechanistic: rich data about all available responses and predictors

# Model fit
Parameterisation is giving values to parameters in a model by whatever means.
Fitting = calibration and is parameterisation by selecting models with a "good fit" to data.
Validation is checking the model results by some standard (statistical or "face validity").
Parametric sensitivity analysis is altering parameters in the model to check the change in results.

Goodness of fit metric is used to decide whether the model prediction fits the data.  Given a set of observations and their accompanying expected values at positions i under a certain model that takes parameter x, a goodness of fit function describes how well the model fits the data for a given value of x.  This function might be written g(E1, ... En, O1, On).
A saturated model exactly fits all the data points.
When an objective function describes how the goodness of fit function varies by the choice of x, it can be minimised for some value of x.  Formally,
	$\textrm{Maximise } f(x)  \textrm{ subject to } x \in X$	 where $f(x) = g(E_1(x), \dots , E_n(x), O_1, \dots , O_n)$
		
Some options are linear residual distance (may not fit at all, positive and negative cancel),
		absolute linear residual distance (may not fit at all, odd behaviour at zero)
		sum of squared residuals (SSR or SSQ) is the least squares method
		$g(x) = \sum_i w_i (E_i(x) - O_i)^2$
			
If the deviances from model are uncorrelated, have equal variance expectation (and so wi = 1) and are normally distributed then uncertainty is not a problem for inferences on whether a model is superior to another model.  Under these preconditions the least squares estimate equals the maximum likelihood estimate.  On the other hand, there's no absolute statement possible about "how good" a fit is using this method, only that it's better than another one.  If variances are not equal they can be weighted, by the reciprocal of the variance at each observation if known, or by the reciprocal of the model value at each observation if not (the latter is Pearson's chi squared technique).
Maximum likelihood is another option: choose a likelihood function that expresses the probability of a certain parameter conditional on the data observed and conditional on the likelihood function.  For example, the likelihood that the probability of an event is p, given 5 observed successes out of 10, is given by the binomial function and is maximal at p=0.5. 

The deviance allows model fit to be compared across models, and gives a sort of "absolute goodness" of fit by comparing the likelihood of a test model with the likelihood of the saturated model.  D clearly gives a number less than one, tending to 1 as the model approaches perfection.

$D = -2 log \frac{L(\lambda)}{L(\hat \lambda)}$

Then the difference in deviances between to models is chi squared distributed with degrees of freedom equal to the difference in number of parameters between the models.  This can be used to construct parametric confidence intervals around the best fitting parameters, form a test statistic and calculate the AIC.
The parameters that lead to the best fit are chosen by a fitting algorithm.

# Meta Analysis
	- Provide a data display and objective review
	- Give a summary interpretation
	- Test an overall hypothesis
	- Estimate an average exposure effect
	- Assess whether data compatible with the exposure effect being the same in all studies

Fixed Effect assumes the effect really is the same and any difference is due to sampling variation: provides a summary using a weighted average of the individual study estimates.  Weights are eg 1/variance of the log odds ratio, so the summary OR is 
$\psi_F = \frac{\sum_{i=1}^k w_i \psi_i}{\sum_{i=1}^k w_i} , \pm \frac{1}{\sum_{i=1}^k w_i}$,
which is the variance regenerated from the averaged weights.
A Forest Plot represents each study (box size = weight, bars for CI, diamond width = CI of summary estimate) and the heterogeneity between them uses a Chi Squared test $Q = \sum_{i=1}^k w_i (\psi_i - \psi_F)$.  If heterogenous then use Random Effect.

Random Effect assumes the true effects vary around an average: the between study variance of the estimates (or alternatively, the residuals around the average of the estimates) is used to modify weights of each study.  A summary OR is the same shape as the fixed model summary OR but with between-study-variance-adjusted weights, w*i. These weights are far less dispersed than the Fixed Effect weights so smaller studies are weighted more heavily and the confidence intervals tend to be wider.  So Random Effects models are more conservative.

# Git work
pwd print working directory
ls
clear
touch newfilename for new file
cp file filetocopyto -r for recursive(=copy all within)
mkdir
cd
	cd .. up one
	cd home
rm thing to remove -r with caution! No undo!
mv object dir , also changes name if with file name
echo 
date

git init
git clone
git add . Adds all new files to the index
	git add -u updates
	git add -A does both
git commit -m "message" to local repository
git push to remote directory
git checkout -b branchname another version of the same directory
git branch see what branch you're on
pull request is not a git feature but on github


# markdown: 
SQL
CREATE TABLE tablename (fieldname FIELDTYPE); etc
INSERT INTO table VALUES (valueforrow1col1, valueforrow1col2); etc


# stata
[by varlist]: command [varlist] [if exp] [in range] [using filename], options 
describe
summarize
describe		
display		Calculator
Do			Execute a do file
lookup		Online help
save			save. To overwrite :save, replace
append		concatenate new observations into existing columns
codebook
collapse		"aggregate data"
count
drop			delete
duplicates		tag duplicates
return list		list the output of the last command, maybe
generate		new variable
egen			new variable
label define
list			list values of variables
merge			some kind of merge
reshape		maybe some kind of transpose "long to wide or wide to long"
sort
table			two way tables
tabulate		one and two way frequencies
stset
strate
sts
stmh
stsplit
logit			log scale
logistic		natural scale
clogit, or		conditional logistic, log scale unless or
xtpoisson		random effects incidence rate ration poisson regression

	outcome exposure##interactor,
	exp(denominator follow up time) irr i(cluster) re
mixed			mixed effects linear regression, output includes var(_cons)
			which is the estimate of between cluster variance.
			|| is the ~ of stata, reml is restricted likelihood
	kscore arm if covariate==i|| cluster:, reml nolong
xtlogit		random effects, gives icc and clustering chisq.
	outcome treatment, or re i(cluster) 
quadchk		probably not necessary any more because adaptive quadrature is			used to approximate normal likelihood distribution


# R
download.file
file.exists
dir.create
list.files
dir
ls
read.table(sep, quote, na.strings, nrows, skip)	read.csv	read.csv2
seq(from, to, by along )
cut(data, breaks)
factor()		relevel(data, ref)
intersect
split-apply-combine
sample

apply(data, dim, fun)
tapply(apply along an index, a function to data) ie tapply(data, index, function)
sapply(simplify the output of a function applied to data)
	eg	perms <- sapply(1:10000, function(i) testStat(DEcounts,
			sample(group)))
lapply(apply a function across a list; unlist to simplify)

Hmisc::cut2(data, g)
reshape2::melt(data, id, measure.vars)
		dcast(melteddata, formula, Aggregation value)
transformations
plot
	coplot
	hist
abs
sqrt
ceiling
floor
cos
sin
log
log2
log10
exp
round(data, digits=n)
signif(data, digits=n)	
p.adjust(method = c("bonferroni", "BH", "BY")


## xlsx
xlsx::read.xlsx(sheetIndex, rowIndex, colIndex)	read.xlsx2	write.xlsx

## XML and HTML
xml::xmlTreeParse(fileURL, useInternal)	xmlRoot	xmlName	xmlSApply(doc, path fun, ...)
	TreeParse returns a structured list which can be subset eg rootNode[[1]][[1]], or have components
		programmatically extracted eg xmlSApply(rootNode, xmlValue)
	xPAth language	/Node	//Node Node['@value'='thing']
Read the short intro at least.
httr::GET, PATCH, POST, HEAD, PUT, DELETE 

## JSON
Javascript Object Notation
jsonlite::fromJSON		toJSON
multiple nesting of data from the data frame resulting from fromJSON
data.table
subsets by expression, not column
:= adds a variable
.N is an integer length 1 containing the number of observations
does not copy data tables on subsetting, use copy function
plyr-like operations, for example on .N by some value
data.tables have keys to allow joins.
setkey and merge
fread for fast reading (substitute for read.table for tab separated files)
tibble eg with list-columns so a data frame need not hold numbers or other low level entities in its cells but whole lists of interest such eg as a training resample from a dataset, a test resample, a model fitted to that test dataset and the resulting RMSE.  Tibbles are lazy and surly, don't require a list of vectors as the underlying structure and give a little more information in printing than data.frames.

## subsetting in base
split-apply-combine
	with(tapply(X, factor, function)) type of structure or
	with(by(X, factor, function)) or
	aggregate(data ~ factor, source or environment, function)
by position
by logical statements
sort
order eg X[order(X$var1, X$var3),]
add columns by X$newvar or by Y <- cbind(X, newvar)
sum	colSums	rowSums	any	all	is.na	%in%
head	tail
table
xtabs(var1 ~ var2 + ., data=DF) where . is all the variables
ftable(


## plyr
plyr::arrange(X, var1)
		mutate(oldata, newvar = function(oldvar)
		`.` or as.quoted allow a list of variables to me assembled for later evaluation in plyr
		ddply(.data, .variable, .fun , ... others)
		dlply
## dplyr
arrange 	 reorders rows 
filter	 returns subset of rows based on a condition 
select	 returns subset of columns (use starts_with, contains, matches, ends_with to specify), in the order specified
mutate	 transforms data and adds or replaces them in the data.frame 
rename	 rename without otherwise changing
summarise	 summarises.  Also spelled summarize
group_by	 reorders a data.frame "by group" 
merge		 (x, y, by.x, by.y, all = T/F) where x is one dataset and y another, and "all" is whether to keep duplicate variables and add a subscript to the added variables to identify it .  plyr::join is faster but only works by a common variable. plyr::join_all takes a list (so need to form a list) of data frames and joins all by a common ID variable.
%>%		 chains operators in sequence 
*_join	left_joinincludes all observations in the left data frame; right_join analogous, inner_join only observations in both, full_join all observations no matter the match
dbi package for using many other backends to dplyr

## tidyr
unite	brings together values, like paste but with type specs.

lm(outcome~predictor + predictor2 - 1)	 With intercept removed by -1; relevel() to set intercept. Counts modelled straightforwardy and adjusted for lower bound or heteroskedasticity.  Indicator variables add an optional intercept for binary factors.
Influence.measures .  rstandard is residuals, hatvalues is for influential points (esp data entry errors). dffits is how much the line at that point is influenced by the absence of the data point.  dfbeta is one for each point for each regressor; it's a matrix.  cook's distance is a summary of the dfbeta by giving an overall change in them.  resid returns ordinary residuals, hatvalues gives the predicted Y value with the point excluded vs included.  A cross validated leave-one out residual, PRESS residuals are resid(model) / (1- hatvalues(model)).

## Hadley Wickham on the Tidyverse:

Import-Tidy-{Transform Visualise Model} – Communicate
The eye is drawn to the verbs and that's important because good data science can't be done without programming and the verbs are activities that can be programmed.  But for good workflow it's the interactions between each of those that's important.  Modelling is ultimately a computational approach that scales well but, because it uses assumptions at some level, and can't question those assumptions, can therefore not really surprise.  Visualisation on the other hand is a human activity and it's much harder to throw more brains at the problem than to throw more computers at the problem

Share data structures, compose simple pieces, embrace functional programming and write for humans.

# Python. 'sake

object Objects have attributes, which have associated methods (functions specific to object types). Methods are often called with an object as prefix: object.method() .
method  Methods work differently depending on the type on which they are called.  Fucks again python.
packages  Packages are collections of scripts (.py files); they are called modules in this context.  Packages imported, then modules called from the packages by package.module (eg numpy.array and the irritating renaming of the package is where np.array comes from); or modules are imported from packages then called bare; in which case the "import as" makes more sense because you can quote it bare.
index  Indexing starts at 0 and starts if descending at -1.  Indexing in NumPy is by recursive [] calls or by R-style [dim1, dim2, ...] coordinates.
dictionary  Key:value pairs initiated with {"key":"value}, accessed by [""].  Stored as hashes.  Lists can't, therefore, be keys because they are mutable. 
Pandas Produces DataFrame which is a cbind of "series" .  All to make an array that can contain different data types. Accessed by [] for the series and [[]] for a new DataFrame containing the same information as the [] does.  Extended by the loc and iloc functions.  loc specifies by ['name of key"] for columns, or [[]] which can be chained. iloc does the same using indices of the location.  ix is a method to mix loc and iloc.  The different chaining results in different formats of output.
 slice  Calls to index include the first and exclude the last.
lists  Are stored separately from their name.  So x = [...] ; y = x declares y a pointer to the list that x also points to.  So manipulating the list using y is the same as manipulating the list using x.  Except that a slice or list() call creates a new list.  Fuck you python.  And deleting an element changes the indices of subsequent elements.  This is under "explicit versus reference based copies".
NumPy  Is necessary to use any sums.  It introduces arrays which MAKE PYTHON WORK LIKE R: elementwise mathematical operations; it's a little tidyverselike because you can use proximal assignments to make distal ones.  Does this mean lazy evaluation? Arrays can contain any typep but all the same type.
attributes  In a NumPy object are accessed by . The same as $ accesses them in an R object.


```{r bib, include=FALSE}
# create a bib file for the R packages used in this document, previous yaml included these for tufte-book but not necessary if using just pandoc-citeproc

knitr::write_bib(c('base', 'rmarkdown'), file = 'Epinotespackages.bib')
```
